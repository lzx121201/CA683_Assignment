{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reddit Sarcasm Neural Networks.ipynb","provenance":[],"collapsed_sections":["fzniQ5uBr1Rp","satmN4XNvx9S"],"mount_file_id":"1H9NRTIvIH6c_XEUwkqyjzlq0wxoYNVQm","authorship_tag":"ABX9TyNpSb1PWyEvjDhzKCbuCIWY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fzniQ5uBr1Rp"},"source":["# Setting up the environment"]},{"cell_type":"code","metadata":{"id":"Mz37RXqjsOC7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618094630130,"user_tz":-60,"elapsed":4155,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"d5f87d80-159e-48f4-9b87-1b71acd6e74b"},"source":["pip install contractions"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ltXZrlNsbMX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618094630702,"user_tz":-60,"elapsed":1374,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"53be1083-4929-4bc0-be9e-41f21cd52c61"},"source":["nltk.download('stopwords')"],"execution_count":54,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"a7wKhtU8r74g","executionInfo":{"status":"ok","timestamp":1618094630701,"user_tz":-60,"elapsed":3051,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","\n","# For preprocessing\n","import unicodedata   ## Removing Accented Characters\n","import contractions #from contractions.py ##Expanding Contractions\n","import re ## Removing Special Character\n","import spacy  ## Lemmatization\n","nlp = spacy.load('en_core_web_sm')\n","import string ## Removing Punctuation\n","import nltk ##Stemming\n","from nltk.tokenize import ToktokTokenizer ## Removing Stopwords\n","tokenizer = ToktokTokenizer() ## Removing Stopwords\n","stopword_list = nltk.corpus.stopwords.words('english') ## Removing Stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer ##TfIdf"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQyuQJw0s1M6","executionInfo":{"status":"ok","timestamp":1618094639674,"user_tz":-60,"elapsed":6450,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"a367e4b0-6e5c-4e45-a056-8bb8a415ae40"},"source":["#Reading Data\n","train_data = pd.read_csv(\"/content/drive/MyDrive/Data_Mining_Assignment/data/train-balanced.csv\", sep='\\t')\n","keys = pd.read_csv(\"/content/drive/MyDrive/Data_Mining_Assignment/data/key.csv\", sep='\\t')\n","\n","\n","#Adding column names to train and test dataframe\n","train_data.columns = keys.columns\n","train_data.dropna(subset=['comment'], inplace=True) ## Dropping rows with no comments\n","train_data.info()"],"execution_count":55,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 1010772 entries, 0 to 1010824\n","Data columns (total 10 columns):\n"," #   Column          Non-Null Count    Dtype \n","---  ------          --------------    ----- \n"," 0   label           1010772 non-null  int64 \n"," 1   comment         1010772 non-null  object\n"," 2   author          1010772 non-null  object\n"," 3   subreddit       1010772 non-null  object\n"," 4   score           1010772 non-null  int64 \n"," 5   ups             1010772 non-null  int64 \n"," 6   downs           1010772 non-null  int64 \n"," 7   date            1010772 non-null  object\n"," 8   created_utc     1010772 non-null  int64 \n"," 9   parent_comment  1010772 non-null  object\n","dtypes: int64(5), object(5)\n","memory usage: 84.8+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"L23tXYrdtIGN","executionInfo":{"status":"ok","timestamp":1618094639679,"user_tz":-60,"elapsed":3229,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"0399544a-b69f-485f-860e-11d3328d4ab9"},"source":["train_data"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>comment</th>\n","      <th>author</th>\n","      <th>subreddit</th>\n","      <th>score</th>\n","      <th>ups</th>\n","      <th>downs</th>\n","      <th>date</th>\n","      <th>created_utc</th>\n","      <th>parent_comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>You do know west teams play against west teams...</td>\n","      <td>Shbshb906</td>\n","      <td>nba</td>\n","      <td>-4</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-11</td>\n","      <td>1477959850</td>\n","      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>They were underdogs earlier today, but since G...</td>\n","      <td>Creepeth</td>\n","      <td>nfl</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>1474580737</td>\n","      <td>They're favored to win.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>This meme isn't funny none of the \"new york ni...</td>\n","      <td>icebrotha</td>\n","      <td>BlackPeopleTwitter</td>\n","      <td>-8</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>1476824627</td>\n","      <td>deadass don't kill my buzz</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>I could use one of those tools.</td>\n","      <td>cush2push</td>\n","      <td>MaddenUltimateTeam</td>\n","      <td>6</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-12</td>\n","      <td>1483117213</td>\n","      <td>Yep can confirm I saw the tool they use for th...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>I don't pay attention to her, but as long as s...</td>\n","      <td>only7inches</td>\n","      <td>AskReddit</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>1472812508</td>\n","      <td>do you find ariana grande sexy ?</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1010820</th>\n","      <td>1</td>\n","      <td>I'm sure that Iran and N. Korea have the techn...</td>\n","      <td>TwarkMain</td>\n","      <td>reddit.com</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2009-04</td>\n","      <td>1240620472</td>\n","      <td>No one is calling this an engineered pathogen,...</td>\n","    </tr>\n","    <tr>\n","      <th>1010821</th>\n","      <td>1</td>\n","      <td>whatever you do, don't vote green!</td>\n","      <td>BCHarvey</td>\n","      <td>climate</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2009-05</td>\n","      <td>1242340060</td>\n","      <td>In a move typical of their recent do-nothing a...</td>\n","    </tr>\n","    <tr>\n","      <th>1010822</th>\n","      <td>1</td>\n","      <td>Perhaps this is an atheist conspiracy to make ...</td>\n","      <td>rebelcommander</td>\n","      <td>atheism</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2009-01</td>\n","      <td>1231633377</td>\n","      <td>Screw the Disabled--I've got to get to Church ...</td>\n","    </tr>\n","    <tr>\n","      <th>1010823</th>\n","      <td>1</td>\n","      <td>The Slavs got their own country - it is called...</td>\n","      <td>catsi</td>\n","      <td>worldnews</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2009-01</td>\n","      <td>1232745169</td>\n","      <td>I've always been unsettled by that. I hear a l...</td>\n","    </tr>\n","    <tr>\n","      <th>1010824</th>\n","      <td>1</td>\n","      <td>values, as in capitalism .. there is good mone...</td>\n","      <td>frogking</td>\n","      <td>politics</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2009-01</td>\n","      <td>1232778014</td>\n","      <td>Why do the people who make our laws seem unabl...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1010772 rows × 10 columns</p>\n","</div>"],"text/plain":["         label  ...                                     parent_comment\n","0            0  ...  The blazers and Mavericks (The wests 5 and 6 s...\n","1            0  ...                            They're favored to win.\n","2            0  ...                         deadass don't kill my buzz\n","3            0  ...  Yep can confirm I saw the tool they use for th...\n","4            0  ...                   do you find ariana grande sexy ?\n","...        ...  ...                                                ...\n","1010820      1  ...  No one is calling this an engineered pathogen,...\n","1010821      1  ...  In a move typical of their recent do-nothing a...\n","1010822      1  ...  Screw the Disabled--I've got to get to Church ...\n","1010823      1  ...  I've always been unsettled by that. I hear a l...\n","1010824      1  ...  Why do the people who make our laws seem unabl...\n","\n","[1010772 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"satmN4XNvx9S"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"pCAv0BOXwxrO","executionInfo":{"status":"ok","timestamp":1618094639680,"user_tz":-60,"elapsed":534,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["## Defining Data cleaning Functions\n","\n","# Removing Accented Characters\n","def remove_accented_chars(text):\n","    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return new_text\n","\n","# Removing Special Characters except \"\", '', and !\n","def remove_special_characters(text):\n","    # define the pattern to keep\n","    pat = r'[^a-zA-z0-9!\\\"\\',?/:;\\s]' \n","    return re.sub(pat, '', text)\n","\n","# Removing StopWords\n","def remove_stopwords(text):\n","    # convert sentence into token of words\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    # check in lowercase \n","    t = [token for token in tokens if token.lower() not in stopword_list]\n","    text = ' '.join(t)    \n","    return text\n","\n","# Removing extra whitespaces and tabs\n","def remove_extra_whitespace_tabs(text):\n","    pattern = r'^\\s*|\\s\\s*'\n","    return re.sub(pattern, ' ', text).strip()"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuUuiE_Nv1Hq","executionInfo":{"status":"ok","timestamp":1618094780879,"user_tz":-60,"elapsed":141728,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["## Calling the Functions\n","train_data['cleancomment']=train_data['comment'].map(lambda s:remove_accented_chars(s))\n","train_data['cleancomment']=train_data['cleancomment'].map(lambda s:remove_special_characters(s))\n","train_data['cleancomment']=train_data['cleancomment'].map(lambda s:remove_stopwords(s))\n","train_data['cleancomment']=train_data['cleancomment'].map(lambda s:remove_extra_whitespace_tabs(s))\n","\n","## Combining \"Comment\", \"Subreddit\" and \"Author\" to get better results\n","train_data['all'] = train_data.agg('{0[cleancomment]} {0[subreddit]} {0[author]}'.format, axis=1)"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"Xzb7uoSwzcWs","executionInfo":{"status":"ok","timestamp":1618094780881,"user_tz":-60,"elapsed":141724,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"8e071906-22b2-4980-edd0-5f41a682ea43"},"source":["train_data = train_data.drop(['comment', 'author', 'subreddit', 'score', 'downs', 'ups', 'date', 'created_utc', 'parent_comment', 'cleancomment'], axis=1)\n","train_data.head()"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>all</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>know west teams play west teams east teams rig...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>underdogs earlier today , since Gronk ' announ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>meme ' funny none \" new york nigga \" ones Blac...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>could use one tools MaddenUltimateTeam cush2push</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>' pay attention , long ' legal ' kick bed took...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                                all\n","0      0  know west teams play west teams east teams rig...\n","1      0  underdogs earlier today , since Gronk ' announ...\n","2      0  meme ' funny none \" new york nigga \" ones Blac...\n","3      0   could use one tools MaddenUltimateTeam cush2push\n","4      0  ' pay attention , long ' legal ' kick bed took..."]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwE5U51n2-TE","executionInfo":{"status":"ok","timestamp":1618094780882,"user_tz":-60,"elapsed":141720,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"0cdb19d1-ed63-4c5e-9052-91e476d5b555"},"source":["train_data.rename(columns = {'label':'target'}, inplace = True)\n","train_data.rename(columns = {'all':'text'}, inplace = True)\n","train_data.info()"],"execution_count":60,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 1010772 entries, 0 to 1010824\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count    Dtype \n","---  ------  --------------    ----- \n"," 0   target  1010772 non-null  int64 \n"," 1   text    1010772 non-null  object\n","dtypes: int64(1), object(1)\n","memory usage: 23.1+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5UDa2rZCeGY6","executionInfo":{"status":"ok","timestamp":1618094781371,"user_tz":-60,"elapsed":142204,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["sar_train_data = train_data[train_data['target'] == 1]\n","neu_train_data = train_data[train_data['target'] == 0]\n","sar_train_data = sar_train_data[:25000]\n","neu_train_data = neu_train_data[:25000]\n","del train_data\n","frames = [sar_train_data, neu_train_data]\n","train_data = pd.concat(frames, sort=False)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"CuvutE2T4CUJ","executionInfo":{"status":"ok","timestamp":1618094781372,"user_tz":-60,"elapsed":142200,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"fe093488-b563-4785-e632-380efba66ac5"},"source":["train_data"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>32</th>\n","      <td>1</td>\n","      <td>' reviews ! ProductTesting RoguishPoppet</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>1</td>\n","      <td>wow totally unreasonable assume agency covered...</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>1</td>\n","      <td>Ho ho ho Melania said way could happened ' kno...</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>1</td>\n","      <td>' wait potus starts twitter war Morning Joe po...</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>1</td>\n","      <td>gotta love teachers give exams day halloween C...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>42332</th>\n","      <td>0</td>\n","      <td>AYY OH AYY kpop plasmicPyrotechnic</td>\n","    </tr>\n","    <tr>\n","      <th>42333</th>\n","      <td>0</td>\n","      <td>15 kegs 2 epics : 400 scraps thoooo gwent Klydex4</td>\n","    </tr>\n","    <tr>\n","      <th>42336</th>\n","      <td>0</td>\n","      <td>' even want think inside looks like pcmasterra...</td>\n","    </tr>\n","    <tr>\n","      <th>42337</th>\n","      <td>0</td>\n","      <td>Quite lot working Overwatch ElysiumProject Cry...</td>\n","    </tr>\n","    <tr>\n","      <th>42338</th>\n","      <td>0</td>\n","      <td>\" Yeah , fast travel losers \" skyrim soundguy82</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 2 columns</p>\n","</div>"],"text/plain":["       target                                               text\n","32          1           ' reviews ! ProductTesting RoguishPoppet\n","43          1  wow totally unreasonable assume agency covered...\n","44          1  Ho ho ho Melania said way could happened ' kno...\n","65          1  ' wait potus starts twitter war Morning Joe po...\n","68          1  gotta love teachers give exams day halloween C...\n","...       ...                                                ...\n","42332       0                 AYY OH AYY kpop plasmicPyrotechnic\n","42333       0  15 kegs 2 epics : 400 scraps thoooo gwent Klydex4\n","42336       0  ' even want think inside looks like pcmasterra...\n","42337       0  Quite lot working Overwatch ElysiumProject Cry...\n","42338       0    \" Yeah , fast travel losers \" skyrim soundguy82\n","\n","[50000 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"mnbkHnZo6y-2","executionInfo":{"status":"ok","timestamp":1618094781373,"user_tz":-60,"elapsed":142195,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["x = train_data.text\n","y = train_data.target"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOr0TK9r660X","executionInfo":{"status":"ok","timestamp":1618094781373,"user_tz":-60,"elapsed":142191,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["from sklearn.model_selection import train_test_split\n","SEED = 2000\n","x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n","x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SnvPRTfg_Z3G","executionInfo":{"status":"ok","timestamp":1618094781374,"user_tz":-60,"elapsed":142188,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"abcf4975-6d90-46fb-e6c3-05c6774b7193"},"source":["print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n","                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n","                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n","print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n","                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n","                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n","print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n","                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n","                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["Train set has total 49000 entries with 49.98% negative, 50.02% positive\n","Validation set has total 500 entries with 49.40% negative, 50.60% positive\n","Test set has total 500 entries with 53.00% negative, 47.00% positive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4mb89OWV3kA7"},"source":["# Training ANN model with TFIDF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-TLyyCL3qQ3","executionInfo":{"status":"ok","timestamp":1618094781868,"user_tz":-60,"elapsed":139701,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"e647ce9d-3b70-4f17-f29e-fd8d54ee7d48"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tvec1 = TfidfVectorizer(max_features=100000)\n","tvec1.fit(x_train)"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.float64'>, encoding='utf-8',\n","                input='content', lowercase=True, max_df=1.0,\n","                max_features=100000, min_df=1, ngram_range=(1, 1), norm='l2',\n","                preprocessor=None, smooth_idf=True, stop_words=None,\n","                strip_accents=None, sublinear_tf=False,\n","                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n","                vocabulary=None)"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAKCthF15bls","executionInfo":{"status":"ok","timestamp":1618086776807,"user_tz":-60,"elapsed":141151,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"aba36b7f-4764-4f29-9b8e-dcf9955a5ccc"},"source":[" len(tvec1.get_feature_names())"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["74469"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"5UGs_89pAKuK","executionInfo":{"status":"ok","timestamp":1618094782381,"user_tz":-60,"elapsed":135770,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["x_train_tfidf = tvec1.transform(x_train)"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLVptlzF5qrj","executionInfo":{"status":"ok","timestamp":1618086777337,"user_tz":-60,"elapsed":139324,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"a0b4077b-35ad-494e-99d9-61c04c39207d"},"source":["x_train_tfidf"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<49000x74469 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 356255 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"qZYjBGQyANHM","executionInfo":{"status":"ok","timestamp":1618094782871,"user_tz":-60,"elapsed":133432,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["x_validation_tfidf = tvec1.transform(x_validation).toarray()"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMer0GDM7bbW","executionInfo":{"status":"ok","timestamp":1617741865710,"user_tz":-60,"elapsed":667,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"e286dc20-3e17-45c0-dd69-e17d0fb0ab94"},"source":["x_validation_tfidf.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(500, 73114)"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"yy6YYWbvH5eH","executionInfo":{"status":"ok","timestamp":1618097497813,"user_tz":-60,"elapsed":795,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["x_test_tfidf = tvec1.transform(x_test).toarray()"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdmMoBSzAVES","executionInfo":{"status":"ok","timestamp":1618086781828,"user_tz":-60,"elapsed":138558,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"2020158b-e7a9-4574-86a1-af8f3e79be25"},"source":["%%time\n","from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression()\n","clf.fit(x_train_tfidf, y_train)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["CPU times: user 3.22 s, sys: 4.33 s, total: 7.55 s\n","Wall time: 3.98 s\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NsH3U2gEAW2M","executionInfo":{"status":"ok","timestamp":1618098861907,"user_tz":-60,"elapsed":527,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"762f077e-3ce1-4164-91f4-bafa1d84beab"},"source":["clf.score(x_validation_tfidf, y_validation)"],"execution_count":105,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.678"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HHOjmSLHAYUY","executionInfo":{"status":"ok","timestamp":1618086781829,"user_tz":-60,"elapsed":135190,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"f5b82b65-d4db-4f5a-ccb8-a5bbf31decf6"},"source":["clf.score(x_train_tfidf, y_train)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8226326530612245"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBXtEQPaNx5m","executionInfo":{"status":"ok","timestamp":1618099004870,"user_tz":-60,"elapsed":519,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"45955725-76a6-40b4-b802-005fd0116a3e"},"source":["clf.score(x_test_tfidf, y_test)"],"execution_count":106,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.67"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"code","metadata":{"id":"aUjIFBYkAnAi","executionInfo":{"status":"ok","timestamp":1618094782872,"user_tz":-60,"elapsed":126461,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["seed = 7\n","np.random.seed(seed)\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ah-ot1gBk8Q","executionInfo":{"status":"ok","timestamp":1618094782872,"user_tz":-60,"elapsed":123669,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["def batch_generator(X_data, y_data, batch_size):\n","    samples_per_epoch = X_data.shape[0]\n","    number_of_batches = samples_per_epoch/batch_size\n","    counter=0\n","    index = np.arange(np.shape(y_data)[0])\n","    while 1:\n","        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n","        X_batch = X_data[index_batch,:].toarray()\n","        y_batch = y_data[y_data.index[index_batch]]\n","        counter += 1\n","        yield np.array(X_batch),np.array(y_batch)\n","        if (counter > number_of_batches):\n","            counter=0"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_MZGPNKBqBg","executionInfo":{"status":"ok","timestamp":1618087129857,"user_tz":-60,"elapsed":478556,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"e5a641b1-654f-4a8c-d71c-b7588b739a45"},"source":["%%time\n","model = Sequential()\n","model.add(Dense(64, activation='relu', input_dim=74469))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit_generator(generator=batch_generator(x_train_tfidf, y_train, 32),\n","                    epochs=5, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/5\n","1531/1531 [==============================] - 71s 46ms/step - loss: 0.6609 - accuracy: 0.6027 - val_loss: 0.6056 - val_accuracy: 0.6720\n","Epoch 2/5\n","1531/1531 [==============================] - 70s 45ms/step - loss: 0.4672 - accuracy: 0.7824 - val_loss: 0.7274 - val_accuracy: 0.6320\n","Epoch 3/5\n","1531/1531 [==============================] - 69s 45ms/step - loss: 0.2563 - accuracy: 0.8953 - val_loss: 0.9769 - val_accuracy: 0.5880\n","Epoch 4/5\n","1531/1531 [==============================] - 69s 45ms/step - loss: 0.1188 - accuracy: 0.9574 - val_loss: 1.2451 - val_accuracy: 0.5780\n","Epoch 5/5\n","1531/1531 [==============================] - 67s 44ms/step - loss: 0.0559 - accuracy: 0.9821 - val_loss: 1.5215 - val_accuracy: 0.5780\n","CPU times: user 10min 2s, sys: 11.1 s, total: 10min 13s\n","Wall time: 5min 46s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iXcNs9iR7zqS"},"source":["# Normalizing the inputs to improve the performance of the ANN"]},{"cell_type":"code","metadata":{"id":"PWjLBJ2l761D","executionInfo":{"status":"ok","timestamp":1618087130316,"user_tz":-60,"elapsed":474528,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["from sklearn.preprocessing import Normalizer\n","norm = Normalizer().fit(x_train_tfidf)\n","x_train_tfidf_norm = norm.transform(x_train_tfidf)\n","x_validation_tfidf_norm = norm.transform(x_validation_tfidf)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PR0JDnOD8Ae4","executionInfo":{"status":"ok","timestamp":1618087482102,"user_tz":-60,"elapsed":824693,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"118f73cf-8886-4e0f-809e-813677828a9d"},"source":["%%time\n","model_n = Sequential()\n","model_n.add(Dense(64, activation='relu', input_dim=74469))\n","model_n.add(Dense(1, activation='sigmoid'))\n","model_n.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_n.fit_generator(generator=batch_generator(x_train_tfidf_norm, y_train, 32),\n","                    epochs=5, validation_data=(x_validation_tfidf_norm, y_validation),\n","                    steps_per_epoch=x_train_tfidf_norm.shape[0]/32)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/5\n","1531/1531 [==============================] - 71s 46ms/step - loss: 0.6600 - accuracy: 0.6033 - val_loss: 0.6062 - val_accuracy: 0.6700\n","Epoch 2/5\n","1531/1531 [==============================] - 70s 46ms/step - loss: 0.4666 - accuracy: 0.7848 - val_loss: 0.7238 - val_accuracy: 0.6260\n","Epoch 3/5\n","1531/1531 [==============================] - 71s 46ms/step - loss: 0.2527 - accuracy: 0.8991 - val_loss: 0.9664 - val_accuracy: 0.5760\n","Epoch 4/5\n","1531/1531 [==============================] - 70s 46ms/step - loss: 0.1156 - accuracy: 0.9595 - val_loss: 1.2336 - val_accuracy: 0.5640\n","Epoch 5/5\n","1531/1531 [==============================] - 70s 46ms/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 1.5120 - val_accuracy: 0.5780\n","CPU times: user 10min 14s, sys: 11.5 s, total: 10min 25s\n","Wall time: 5min 52s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kh8XcUST9EzO"},"source":["# Using Dropout"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"id":"1P0dupww9Gp9","executionInfo":{"status":"error","timestamp":1618087645579,"user_tz":-60,"elapsed":985556,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"cea1de97-d86a-403b-9cfa-fe78fa434fc5"},"source":["model1 = Sequential()\n","model1.add(Dense(64, activation='relu', input_dim=74469))\n","model1.add(Dropout(0.2))\n","model1.add(Dense(1, activation='sigmoid'))\n","model1.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model1.fit_generator(generator=batch_generator(x_train_tfidf, y_train, 32),\n","                    epochs=5, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1531/1531 [==============================] - 67s 44ms/step - loss: 0.6617 - accuracy: 0.6050 - val_loss: 0.6036 - val_accuracy: 0.6780\n","Epoch 2/5\n","1531/1531 [==============================] - 66s 43ms/step - loss: 0.4848 - accuracy: 0.7721 - val_loss: 0.6924 - val_accuracy: 0.6440\n","Epoch 3/5\n"," 673/1531 [============>.................] - ETA: 37s - loss: 0.3234 - accuracy: 0.8641"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-6ae15673b0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m model1.fit_generator(generator=batch_generator(x_train_tfidf, y_train, 32),\n\u001b[1;32m     10\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_validation_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     steps_per_epoch=x_train_tfidf.shape[0]/32)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"ASdnDl709guV"},"source":["There is another method I can try to prevent overfitting. By presenting the data in the same order for every epoch, there's a possibility that the model learns the parameters which also includes the noise of the training data, which eventually leads to overfitting. This can be improved by shuffling the order of the data we feed the model. Below I added shuffling to the batch generator function and tried with the same model structure and compared the result."]},{"cell_type":"markdown","metadata":{"id":"BtOYE_dokMmU"},"source":["# Using CustomeCallback Function to understand the best accuracy"]},{"cell_type":"code","metadata":{"id":"t5wS0QtNjVto","executionInfo":{"status":"ok","timestamp":1618096942720,"user_tz":-60,"elapsed":587,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["import keras\n","class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n","    def on_test_batch_end(self, batch, logs=None):\n","       print(\"For batch {}, accuracy is {:7.2f}.\".format(batch, logs[\"accuracy\"]))\n","\n","    def on_train_batch_end(self, batch, logs=None):\n","      print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n","\n","    #def on_predict_end(self, logs=None):\n","    #    keys = list(logs.keys())\n","    #    print(\"Stop predicting; got log keys: {}\".format(keys))\n","\n","    # def on_predict_batch_end(self, batch, logs=None):\n","    #    print(\"...Predicting: end of batch {}; got log keys (loss): {}\".format(batch, logs[\"loss\"]))\n","\n","    # def on_epoch_end(self, epoch, logs=None):\n","    #     print(\n","    #         \"3. The average loss for epoch {} is {:7.2f} \"\n","    #         \"and accuracyr is {:7.2f}.\".format(\n","    #             epoch, logs[\"loss\"], logs[\"accuracy\"]\n","    #         )\n","    #     )"],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPbjCV-CkiIC","executionInfo":{"status":"ok","timestamp":1618097031441,"user_tz":-60,"elapsed":87263,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"e51f7f67-e9cb-41aa-8d2f-f702a04c84ff"},"source":["model1 = Sequential()\n","model1.add(Dense(64, activation='relu', input_dim=74469))\n","model1.add(Dropout(0.2))\n","model1.add(Dense(1, activation='sigmoid'))\n","model1.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model1.fit_generator(generator=batch_generator(x_train_tfidf, y_train, 32),\n","                    epochs=1, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32,\n","                     callbacks=[LossAndErrorPrintingCallback()])"],"execution_count":89,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["   1/1531 [..............................] - ETA: 13:15 - loss: 0.6930 - accuracy: 0.4688For batch 0, loss is    0.69.\n","For batch 1, loss is    0.69.\n","   3/1531 [..............................] - ETA: 1:07 - loss: 0.6932 - accuracy: 0.4410 For batch 2, loss is    0.69.\n","For batch 3, loss is    0.69.\n","   5/1531 [..............................] - ETA: 1:06 - loss: 0.6932 - accuracy: 0.4546For batch 4, loss is    0.69.\n","For batch 5, loss is    0.69.\n","   7/1531 [..............................] - ETA: 1:06 - loss: 0.6931 - accuracy: 0.4654For batch 6, loss is    0.69.\n","For batch 7, loss is    0.69.\n","   9/1531 [..............................] - ETA: 1:07 - loss: 0.6931 - accuracy: 0.4711For batch 8, loss is    0.69.\n","For batch 9, loss is    0.69.\n","  11/1531 [..............................] - ETA: 1:08 - loss: 0.6931 - accuracy: 0.4747For batch 10, loss is    0.69.\n","  12/1531 [..............................] - ETA: 1:09 - loss: 0.6931 - accuracy: 0.4766For batch 11, loss is    0.69.\n","  13/1531 [..............................] - ETA: 1:10 - loss: 0.6931 - accuracy: 0.4780For batch 12, loss is    0.69.\n","  14/1531 [..............................] - ETA: 1:10 - loss: 0.6931 - accuracy: 0.4793For batch 13, loss is    0.69.\n","For batch 14, loss is    0.69.\n","  16/1531 [..............................] - ETA: 1:10 - loss: 0.6930 - accuracy: 0.4820For batch 15, loss is    0.69.\n","For batch 16, loss is    0.69.\n","  18/1531 [..............................] - ETA: 1:10 - loss: 0.6930 - accuracy: 0.4845For batch 17, loss is    0.69.\n","For batch 18, loss is    0.69.\n","  20/1531 [..............................] - ETA: 1:10 - loss: 0.6930 - accuracy: 0.4872For batch 19, loss is    0.69.\n","For batch 20, loss is    0.69.\n","  22/1531 [..............................] - ETA: 1:09 - loss: 0.6930 - accuracy: 0.4900For batch 21, loss is    0.69.\n","For batch 22, loss is    0.69.\n","  24/1531 [..............................] - ETA: 1:09 - loss: 0.6929 - accuracy: 0.4924For batch 23, loss is    0.69.\n","For batch 24, loss is    0.69.\n","  26/1531 [..............................] - ETA: 1:09 - loss: 0.6929 - accuracy: 0.4943For batch 25, loss is    0.69.\n","For batch 26, loss is    0.69.\n","  28/1531 [..............................] - ETA: 1:09 - loss: 0.6929 - accuracy: 0.4956For batch 27, loss is    0.69.\n","For batch 28, loss is    0.69.\n","  30/1531 [..............................] - ETA: 1:09 - loss: 0.6929 - accuracy: 0.4966For batch 29, loss is    0.69.\n","For batch 30, loss is    0.69.\n","  32/1531 [..............................] - ETA: 1:09 - loss: 0.6928 - accuracy: 0.4976For batch 31, loss is    0.69.\n","  33/1531 [..............................] - ETA: 1:09 - loss: 0.6928 - accuracy: 0.4980For batch 32, loss is    0.69.\n","  34/1531 [..............................] - ETA: 1:09 - loss: 0.6928 - accuracy: 0.4985For batch 33, loss is    0.69.\n","For batch 34, loss is    0.69.\n","  36/1531 [..............................] - ETA: 1:09 - loss: 0.6928 - accuracy: 0.4988For batch 35, loss is    0.69.\n","For batch 36, loss is    0.69.\n","  38/1531 [..............................] - ETA: 1:09 - loss: 0.6928 - accuracy: 0.4990For batch 37, loss is    0.69.\n","For batch 38, loss is    0.69.\n","  40/1531 [..............................] - ETA: 1:08 - loss: 0.6928 - accuracy: 0.4993For batch 39, loss is    0.69.\n","  41/1531 [..............................] - ETA: 1:09 - loss: 0.6928 - accuracy: 0.4996For batch 40, loss is    0.69.\n","For batch 41, loss is    0.69.\n","  43/1531 [..............................] - ETA: 1:08 - loss: 0.6928 - accuracy: 0.5001For batch 42, loss is    0.69.\n","For batch 43, loss is    0.69.\n","  45/1531 [..............................] - ETA: 1:08 - loss: 0.6928 - accuracy: 0.5008For batch 44, loss is    0.69.\n","For batch 45, loss is    0.69.\n","  47/1531 [..............................] - ETA: 1:08 - loss: 0.6928 - accuracy: 0.5014For batch 46, loss is    0.69.\n","For batch 47, loss is    0.69.\n","  49/1531 [..............................] - ETA: 1:08 - loss: 0.6928 - accuracy: 0.5020For batch 48, loss is    0.69.\n","For batch 49, loss is    0.69.\n","  51/1531 [..............................] - ETA: 1:08 - loss: 0.6928 - accuracy: 0.5025For batch 50, loss is    0.69.\n","For batch 51, loss is    0.69.\n","  53/1531 [>.............................] - ETA: 1:08 - loss: 0.6928 - accuracy: 0.5032For batch 52, loss is    0.69.\n","For batch 53, loss is    0.69.\n","  55/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5039For batch 54, loss is    0.69.\n","  56/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5043For batch 55, loss is    0.69.\n","  57/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5048For batch 56, loss is    0.69.\n","  58/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5052For batch 57, loss is    0.69.\n","For batch 58, loss is    0.69.\n","  60/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5060For batch 59, loss is    0.69.\n","  61/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5063For batch 60, loss is    0.69.\n","For batch 61, loss is    0.69.\n","  63/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5069For batch 62, loss is    0.69.\n","For batch 63, loss is    0.69.\n","  65/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5074For batch 64, loss is    0.69.\n","For batch 65, loss is    0.69.\n","  67/1531 [>.............................] - ETA: 1:08 - loss: 0.6927 - accuracy: 0.5080For batch 66, loss is    0.69.\n","For batch 67, loss is    0.69.\n","  69/1531 [>.............................] - ETA: 1:08 - loss: 0.6926 - accuracy: 0.5085For batch 68, loss is    0.69.\n","For batch 69, loss is    0.69.\n","  71/1531 [>.............................] - ETA: 1:08 - loss: 0.6926 - accuracy: 0.5089For batch 70, loss is    0.69.\n","For batch 71, loss is    0.69.\n","  73/1531 [>.............................] - ETA: 1:08 - loss: 0.6926 - accuracy: 0.5093For batch 72, loss is    0.69.\n","For batch 73, loss is    0.69.\n","  75/1531 [>.............................] - ETA: 1:07 - loss: 0.6926 - accuracy: 0.5098For batch 74, loss is    0.69.\n","For batch 75, loss is    0.69.\n","  77/1531 [>.............................] - ETA: 1:07 - loss: 0.6926 - accuracy: 0.5102For batch 76, loss is    0.69.\n","For batch 77, loss is    0.69.\n","  79/1531 [>.............................] - ETA: 1:07 - loss: 0.6926 - accuracy: 0.5106For batch 78, loss is    0.69.\n","For batch 79, loss is    0.69.\n","  81/1531 [>.............................] - ETA: 1:07 - loss: 0.6925 - accuracy: 0.5111For batch 80, loss is    0.69.\n","For batch 81, loss is    0.69.\n","  83/1531 [>.............................] - ETA: 1:07 - loss: 0.6925 - accuracy: 0.5116For batch 82, loss is    0.69.\n","For batch 83, loss is    0.69.\n","  85/1531 [>.............................] - ETA: 1:07 - loss: 0.6925 - accuracy: 0.5121For batch 84, loss is    0.69.\n","For batch 85, loss is    0.69.\n","  87/1531 [>.............................] - ETA: 1:07 - loss: 0.6925 - accuracy: 0.5126For batch 86, loss is    0.69.\n","For batch 87, loss is    0.69.\n","  89/1531 [>.............................] - ETA: 1:07 - loss: 0.6925 - accuracy: 0.5132For batch 88, loss is    0.69.\n","For batch 89, loss is    0.69.\n","  91/1531 [>.............................] - ETA: 1:07 - loss: 0.6924 - accuracy: 0.5137For batch 90, loss is    0.69.\n","  92/1531 [>.............................] - ETA: 1:07 - loss: 0.6924 - accuracy: 0.5139For batch 91, loss is    0.69.\n","For batch 92, loss is    0.69.\n","  94/1531 [>.............................] - ETA: 1:07 - loss: 0.6924 - accuracy: 0.5144For batch 93, loss is    0.69.\n","For batch 94, loss is    0.69.\n","  96/1531 [>.............................] - ETA: 1:06 - loss: 0.6924 - accuracy: 0.5149For batch 95, loss is    0.69.\n","For batch 96, loss is    0.69.\n","  98/1531 [>.............................] - ETA: 1:06 - loss: 0.6923 - accuracy: 0.5154For batch 97, loss is    0.69.\n","  99/1531 [>.............................] - ETA: 1:06 - loss: 0.6923 - accuracy: 0.5156For batch 98, loss is    0.69.\n","For batch 99, loss is    0.69.\n"," 101/1531 [>.............................] - ETA: 1:06 - loss: 0.6923 - accuracy: 0.5161For batch 100, loss is    0.69.\n","For batch 101, loss is    0.69.\n"," 103/1531 [=>............................] - ETA: 1:06 - loss: 0.6923 - accuracy: 0.5166For batch 102, loss is    0.69.\n","For batch 103, loss is    0.69.\n"," 105/1531 [=>............................] - ETA: 1:06 - loss: 0.6923 - accuracy: 0.5171For batch 104, loss is    0.69.\n","For batch 105, loss is    0.69.\n"," 107/1531 [=>............................] - ETA: 1:06 - loss: 0.6922 - accuracy: 0.5176For batch 106, loss is    0.69.\n","For batch 107, loss is    0.69.\n"," 109/1531 [=>............................] - ETA: 1:06 - loss: 0.6922 - accuracy: 0.5181For batch 108, loss is    0.69.\n"," 110/1531 [=>............................] - ETA: 1:06 - loss: 0.6922 - accuracy: 0.5184For batch 109, loss is    0.69.\n","For batch 110, loss is    0.69.\n"," 112/1531 [=>............................] - ETA: 1:06 - loss: 0.6922 - accuracy: 0.5189For batch 111, loss is    0.69.\n","For batch 112, loss is    0.69.\n"," 114/1531 [=>............................] - ETA: 1:06 - loss: 0.6921 - accuracy: 0.5195For batch 113, loss is    0.69.\n","For batch 114, loss is    0.69.\n"," 116/1531 [=>............................] - ETA: 1:06 - loss: 0.6921 - accuracy: 0.5200For batch 115, loss is    0.69.\n","For batch 116, loss is    0.69.\n"," 118/1531 [=>............................] - ETA: 1:05 - loss: 0.6921 - accuracy: 0.5206For batch 117, loss is    0.69.\n","For batch 118, loss is    0.69.\n"," 120/1531 [=>............................] - ETA: 1:05 - loss: 0.6920 - accuracy: 0.5211For batch 119, loss is    0.69.\n"," 121/1531 [=>............................] - ETA: 1:06 - loss: 0.6920 - accuracy: 0.5214For batch 120, loss is    0.69.\n"," 122/1531 [=>............................] - ETA: 1:06 - loss: 0.6920 - accuracy: 0.5217For batch 121, loss is    0.69.\n","For batch 122, loss is    0.69.\n"," 124/1531 [=>............................] - ETA: 1:05 - loss: 0.6920 - accuracy: 0.5223For batch 123, loss is    0.69.\n","For batch 124, loss is    0.69.\n"," 126/1531 [=>............................] - ETA: 1:05 - loss: 0.6919 - accuracy: 0.5228For batch 125, loss is    0.69.\n","For batch 126, loss is    0.69.\n"," 128/1531 [=>............................] - ETA: 1:05 - loss: 0.6919 - accuracy: 0.5234For batch 127, loss is    0.69.\n","For batch 128, loss is    0.69.\n"," 130/1531 [=>............................] - ETA: 1:05 - loss: 0.6919 - accuracy: 0.5239For batch 129, loss is    0.69.\n"," 131/1531 [=>............................] - ETA: 1:05 - loss: 0.6919 - accuracy: 0.5242For batch 130, loss is    0.69.\n"," 132/1531 [=>............................] - ETA: 1:05 - loss: 0.6918 - accuracy: 0.5244For batch 131, loss is    0.69.\n","For batch 132, loss is    0.69.\n"," 134/1531 [=>............................] - ETA: 1:05 - loss: 0.6918 - accuracy: 0.5250For batch 133, loss is    0.69.\n","For batch 134, loss is    0.69.\n"," 136/1531 [=>............................] - ETA: 1:05 - loss: 0.6918 - accuracy: 0.5255For batch 135, loss is    0.69.\n"," 137/1531 [=>............................] - ETA: 1:05 - loss: 0.6917 - accuracy: 0.5258For batch 136, loss is    0.69.\n","For batch 137, loss is    0.69.\n"," 139/1531 [=>............................] - ETA: 1:05 - loss: 0.6917 - accuracy: 0.5263For batch 138, loss is    0.69.\n","For batch 139, loss is    0.69.\n"," 141/1531 [=>............................] - ETA: 1:05 - loss: 0.6917 - accuracy: 0.5268For batch 140, loss is    0.69.\n"," 142/1531 [=>............................] - ETA: 1:05 - loss: 0.6917 - accuracy: 0.5271For batch 141, loss is    0.69.\n","For batch 142, loss is    0.69.\n"," 144/1531 [=>............................] - ETA: 1:05 - loss: 0.6916 - accuracy: 0.5276For batch 143, loss is    0.69.\n","For batch 144, loss is    0.69.\n"," 146/1531 [=>............................] - ETA: 1:05 - loss: 0.6916 - accuracy: 0.5281For batch 145, loss is    0.69.\n","For batch 146, loss is    0.69.\n"," 148/1531 [=>............................] - ETA: 1:05 - loss: 0.6915 - accuracy: 0.5286For batch 147, loss is    0.69.\n","For batch 148, loss is    0.69.\n"," 150/1531 [=>............................] - ETA: 1:04 - loss: 0.6915 - accuracy: 0.5291For batch 149, loss is    0.69.\n","For batch 150, loss is    0.69.\n"," 152/1531 [=>............................] - ETA: 1:04 - loss: 0.6915 - accuracy: 0.5297For batch 151, loss is    0.69.\n"," 153/1531 [=>............................] - ETA: 1:04 - loss: 0.6914 - accuracy: 0.5299For batch 152, loss is    0.69.\n"," 154/1531 [==>...........................] - ETA: 1:04 - loss: 0.6914 - accuracy: 0.5302For batch 153, loss is    0.69.\n"," 155/1531 [==>...........................] - ETA: 1:04 - loss: 0.6914 - accuracy: 0.5304For batch 154, loss is    0.69.\n","For batch 155, loss is    0.69.\n"," 157/1531 [==>...........................] - ETA: 1:04 - loss: 0.6913 - accuracy: 0.5310For batch 156, loss is    0.69.\n","For batch 157, loss is    0.69.\n"," 159/1531 [==>...........................] - ETA: 1:04 - loss: 0.6913 - accuracy: 0.5315For batch 158, loss is    0.69.\n"," 160/1531 [==>...........................] - ETA: 1:04 - loss: 0.6913 - accuracy: 0.5317For batch 159, loss is    0.69.\n"," 161/1531 [==>...........................] - ETA: 1:04 - loss: 0.6913 - accuracy: 0.5320For batch 160, loss is    0.69.\n"," 162/1531 [==>...........................] - ETA: 1:04 - loss: 0.6912 - accuracy: 0.5322For batch 161, loss is    0.69.\n"," 163/1531 [==>...........................] - ETA: 1:04 - loss: 0.6912 - accuracy: 0.5325For batch 162, loss is    0.69.\n","For batch 163, loss is    0.69.\n"," 165/1531 [==>...........................] - ETA: 1:04 - loss: 0.6912 - accuracy: 0.5329For batch 164, loss is    0.69.\n","For batch 165, loss is    0.69.\n"," 167/1531 [==>...........................] - ETA: 1:04 - loss: 0.6911 - accuracy: 0.5334For batch 166, loss is    0.69.\n","For batch 167, loss is    0.69.\n"," 169/1531 [==>...........................] - ETA: 1:04 - loss: 0.6911 - accuracy: 0.5339For batch 168, loss is    0.69.\n","For batch 169, loss is    0.69.\n"," 171/1531 [==>...........................] - ETA: 1:04 - loss: 0.6910 - accuracy: 0.5343For batch 170, loss is    0.69.\n"," 172/1531 [==>...........................] - ETA: 1:04 - loss: 0.6910 - accuracy: 0.5345For batch 171, loss is    0.69.\n","For batch 172, loss is    0.69.\n"," 174/1531 [==>...........................] - ETA: 1:04 - loss: 0.6910 - accuracy: 0.5349For batch 173, loss is    0.69.\n"," 175/1531 [==>...........................] - ETA: 1:03 - loss: 0.6909 - accuracy: 0.5351For batch 174, loss is    0.69.\n"," 176/1531 [==>...........................] - ETA: 1:04 - loss: 0.6909 - accuracy: 0.5353For batch 175, loss is    0.69.\n","For batch 176, loss is    0.69.\n"," 178/1531 [==>...........................] - ETA: 1:03 - loss: 0.6909 - accuracy: 0.5358For batch 177, loss is    0.69.\n","For batch 178, loss is    0.69.\n"," 180/1531 [==>...........................] - ETA: 1:03 - loss: 0.6908 - accuracy: 0.5362For batch 179, loss is    0.69.\n"," 181/1531 [==>...........................] - ETA: 1:03 - loss: 0.6908 - accuracy: 0.5364For batch 180, loss is    0.69.\n"," 182/1531 [==>...........................] - ETA: 1:03 - loss: 0.6908 - accuracy: 0.5366For batch 181, loss is    0.69.\n","For batch 182, loss is    0.69.\n"," 184/1531 [==>...........................] - ETA: 1:03 - loss: 0.6907 - accuracy: 0.5369For batch 183, loss is    0.69.\n","For batch 184, loss is    0.69.\n"," 186/1531 [==>...........................] - ETA: 1:03 - loss: 0.6907 - accuracy: 0.5373For batch 185, loss is    0.69.\n"," 187/1531 [==>...........................] - ETA: 1:03 - loss: 0.6906 - accuracy: 0.5375For batch 186, loss is    0.69.\n","For batch 187, loss is    0.69.\n"," 189/1531 [==>...........................] - ETA: 1:03 - loss: 0.6906 - accuracy: 0.5379For batch 188, loss is    0.69.\n","For batch 189, loss is    0.69.\n"," 191/1531 [==>...........................] - ETA: 1:03 - loss: 0.6905 - accuracy: 0.5383For batch 190, loss is    0.69.\n"," 192/1531 [==>...........................] - ETA: 1:03 - loss: 0.6905 - accuracy: 0.5385For batch 191, loss is    0.69.\n","For batch 192, loss is    0.69.\n"," 194/1531 [==>...........................] - ETA: 1:03 - loss: 0.6905 - accuracy: 0.5388For batch 193, loss is    0.69.\n"," 195/1531 [==>...........................] - ETA: 1:03 - loss: 0.6904 - accuracy: 0.5390For batch 194, loss is    0.69.\n","For batch 195, loss is    0.69.\n"," 197/1531 [==>...........................] - ETA: 1:03 - loss: 0.6904 - accuracy: 0.5393For batch 196, loss is    0.69.\n","For batch 197, loss is    0.69.\n"," 199/1531 [==>...........................] - ETA: 1:03 - loss: 0.6903 - accuracy: 0.5397For batch 198, loss is    0.69.\n","For batch 199, loss is    0.69.\n"," 201/1531 [==>...........................] - ETA: 1:02 - loss: 0.6903 - accuracy: 0.5400For batch 200, loss is    0.69.\n"," 202/1531 [==>...........................] - ETA: 1:02 - loss: 0.6903 - accuracy: 0.5402For batch 201, loss is    0.69.\n"," 203/1531 [==>...........................] - ETA: 1:02 - loss: 0.6902 - accuracy: 0.5403For batch 202, loss is    0.69.\n","For batch 203, loss is    0.69.\n"," 205/1531 [===>..........................] - ETA: 1:02 - loss: 0.6902 - accuracy: 0.5407For batch 204, loss is    0.69.\n","For batch 205, loss is    0.69.\n"," 207/1531 [===>..........................] - ETA: 1:02 - loss: 0.6902 - accuracy: 0.5410For batch 206, loss is    0.69.\n"," 208/1531 [===>..........................] - ETA: 1:02 - loss: 0.6901 - accuracy: 0.5411For batch 207, loss is    0.69.\n","For batch 208, loss is    0.68.\n"," 210/1531 [===>..........................] - ETA: 1:02 - loss: 0.6901 - accuracy: 0.5415For batch 209, loss is    0.68.\n","For batch 210, loss is    0.68.\n"," 212/1531 [===>..........................] - ETA: 1:02 - loss: 0.6900 - accuracy: 0.5418For batch 211, loss is    0.68.\n","For batch 212, loss is    0.68.\n"," 214/1531 [===>..........................] - ETA: 1:02 - loss: 0.6900 - accuracy: 0.5421For batch 213, loss is    0.68.\n","For batch 214, loss is    0.68.\n"," 216/1531 [===>..........................] - ETA: 1:02 - loss: 0.6899 - accuracy: 0.5424For batch 215, loss is    0.68.\n"," 217/1531 [===>..........................] - ETA: 1:02 - loss: 0.6899 - accuracy: 0.5426For batch 216, loss is    0.68.\n"," 218/1531 [===>..........................] - ETA: 1:02 - loss: 0.6899 - accuracy: 0.5427For batch 217, loss is    0.68.\n","For batch 218, loss is    0.68.\n"," 220/1531 [===>..........................] - ETA: 1:02 - loss: 0.6898 - accuracy: 0.5431For batch 219, loss is    0.68.\n","For batch 220, loss is    0.68.\n"," 222/1531 [===>..........................] - ETA: 1:02 - loss: 0.6898 - accuracy: 0.5434For batch 221, loss is    0.68.\n","For batch 222, loss is    0.68.\n"," 224/1531 [===>..........................] - ETA: 1:01 - loss: 0.6897 - accuracy: 0.5437For batch 223, loss is    0.68.\n","For batch 224, loss is    0.68.\n"," 226/1531 [===>..........................] - ETA: 1:01 - loss: 0.6897 - accuracy: 0.5440For batch 225, loss is    0.68.\n"," 227/1531 [===>..........................] - ETA: 1:01 - loss: 0.6897 - accuracy: 0.5441For batch 226, loss is    0.68.\n"," 228/1531 [===>..........................] - ETA: 1:01 - loss: 0.6896 - accuracy: 0.5443For batch 227, loss is    0.68.\n","For batch 228, loss is    0.68.\n"," 230/1531 [===>..........................] - ETA: 1:01 - loss: 0.6896 - accuracy: 0.5446For batch 229, loss is    0.68.\n"," 231/1531 [===>..........................] - ETA: 1:01 - loss: 0.6896 - accuracy: 0.5447For batch 230, loss is    0.68.\n","For batch 231, loss is    0.68.\n"," 233/1531 [===>..........................] - ETA: 1:01 - loss: 0.6895 - accuracy: 0.5450For batch 232, loss is    0.68.\n","For batch 233, loss is    0.68.\n"," 235/1531 [===>..........................] - ETA: 1:01 - loss: 0.6894 - accuracy: 0.5454For batch 234, loss is    0.68.\n","For batch 235, loss is    0.68.\n"," 237/1531 [===>..........................] - ETA: 1:01 - loss: 0.6894 - accuracy: 0.5457For batch 236, loss is    0.68.\n","For batch 237, loss is    0.68.\n"," 239/1531 [===>..........................] - ETA: 1:01 - loss: 0.6893 - accuracy: 0.5460For batch 238, loss is    0.68.\n"," 240/1531 [===>..........................] - ETA: 1:01 - loss: 0.6893 - accuracy: 0.5461For batch 239, loss is    0.68.\n","For batch 240, loss is    0.68.\n"," 242/1531 [===>..........................] - ETA: 1:01 - loss: 0.6893 - accuracy: 0.5464For batch 241, loss is    0.68.\n"," 243/1531 [===>..........................] - ETA: 1:01 - loss: 0.6892 - accuracy: 0.5465For batch 242, loss is    0.68.\n"," 244/1531 [===>..........................] - ETA: 1:01 - loss: 0.6892 - accuracy: 0.5467For batch 243, loss is    0.68.\n"," 245/1531 [===>..........................] - ETA: 1:01 - loss: 0.6892 - accuracy: 0.5468For batch 244, loss is    0.68.\n"," 246/1531 [===>..........................] - ETA: 1:01 - loss: 0.6892 - accuracy: 0.5470For batch 245, loss is    0.68.\n"," 247/1531 [===>..........................] - ETA: 1:01 - loss: 0.6891 - accuracy: 0.5471For batch 246, loss is    0.68.\n","For batch 247, loss is    0.68.\n"," 249/1531 [===>..........................] - ETA: 1:00 - loss: 0.6891 - accuracy: 0.5474For batch 248, loss is    0.68.\n"," 250/1531 [===>..........................] - ETA: 1:00 - loss: 0.6890 - accuracy: 0.5475For batch 249, loss is    0.68.\n","For batch 250, loss is    0.68.\n"," 252/1531 [===>..........................] - ETA: 1:00 - loss: 0.6890 - accuracy: 0.5478For batch 251, loss is    0.68.\n","For batch 252, loss is    0.68.\n"," 254/1531 [===>..........................] - ETA: 1:00 - loss: 0.6889 - accuracy: 0.5481For batch 253, loss is    0.68.\n","For batch 254, loss is    0.68.\n"," 256/1531 [====>.........................] - ETA: 1:00 - loss: 0.6889 - accuracy: 0.5483For batch 255, loss is    0.68.\n"," 257/1531 [====>.........................] - ETA: 1:00 - loss: 0.6889 - accuracy: 0.5484For batch 256, loss is    0.68.\n"," 258/1531 [====>.........................] - ETA: 1:00 - loss: 0.6888 - accuracy: 0.5486For batch 257, loss is    0.68.\n","For batch 258, loss is    0.68.\n"," 260/1531 [====>.........................] - ETA: 1:00 - loss: 0.6888 - accuracy: 0.5488For batch 259, loss is    0.68.\n","For batch 260, loss is    0.68.\n"," 262/1531 [====>.........................] - ETA: 1:00 - loss: 0.6887 - accuracy: 0.5491For batch 261, loss is    0.68.\n"," 263/1531 [====>.........................] - ETA: 1:00 - loss: 0.6887 - accuracy: 0.5492For batch 262, loss is    0.68.\n"," 264/1531 [====>.........................] - ETA: 1:00 - loss: 0.6887 - accuracy: 0.5493For batch 263, loss is    0.68.\n","For batch 264, loss is    0.68.\n"," 266/1531 [====>.........................] - ETA: 1:00 - loss: 0.6886 - accuracy: 0.5496For batch 265, loss is    0.68.\n","For batch 266, loss is    0.68.\n"," 268/1531 [====>.........................] - ETA: 1:00 - loss: 0.6886 - accuracy: 0.5499For batch 267, loss is    0.68.\n"," 269/1531 [====>.........................] - ETA: 1:00 - loss: 0.6885 - accuracy: 0.5500For batch 268, loss is    0.68.\n","For batch 269, loss is    0.68.\n"," 271/1531 [====>.........................] - ETA: 59s - loss: 0.6885 - accuracy: 0.5502 For batch 270, loss is    0.68.\n","For batch 271, loss is    0.68.\n"," 273/1531 [====>.........................] - ETA: 59s - loss: 0.6884 - accuracy: 0.5505For batch 272, loss is    0.68.\n"," 274/1531 [====>.........................] - ETA: 59s - loss: 0.6884 - accuracy: 0.5506For batch 273, loss is    0.68.\n"," 275/1531 [====>.........................] - ETA: 59s - loss: 0.6884 - accuracy: 0.5507For batch 274, loss is    0.68.\n"," 276/1531 [====>.........................] - ETA: 59s - loss: 0.6884 - accuracy: 0.5509For batch 275, loss is    0.68.\n","For batch 276, loss is    0.68.\n"," 278/1531 [====>.........................] - ETA: 59s - loss: 0.6883 - accuracy: 0.5511For batch 277, loss is    0.68.\n","For batch 278, loss is    0.68.\n"," 280/1531 [====>.........................] - ETA: 59s - loss: 0.6882 - accuracy: 0.5514For batch 279, loss is    0.68.\n","For batch 280, loss is    0.68.\n"," 282/1531 [====>.........................] - ETA: 59s - loss: 0.6882 - accuracy: 0.5516For batch 281, loss is    0.68.\n"," 283/1531 [====>.........................] - ETA: 59s - loss: 0.6882 - accuracy: 0.5517For batch 282, loss is    0.68.\n","For batch 283, loss is    0.68.\n"," 285/1531 [====>.........................] - ETA: 59s - loss: 0.6881 - accuracy: 0.5519For batch 284, loss is    0.68.\n"," 286/1531 [====>.........................] - ETA: 59s - loss: 0.6881 - accuracy: 0.5521For batch 285, loss is    0.68.\n"," 287/1531 [====>.........................] - ETA: 59s - loss: 0.6881 - accuracy: 0.5522For batch 286, loss is    0.68.\n"," 288/1531 [====>.........................] - ETA: 59s - loss: 0.6880 - accuracy: 0.5523For batch 287, loss is    0.68.\n","For batch 288, loss is    0.68.\n"," 290/1531 [====>.........................] - ETA: 59s - loss: 0.6880 - accuracy: 0.5525For batch 289, loss is    0.68.\n"," 291/1531 [====>.........................] - ETA: 59s - loss: 0.6880 - accuracy: 0.5526For batch 290, loss is    0.68.\n"," 292/1531 [====>.........................] - ETA: 59s - loss: 0.6879 - accuracy: 0.5528For batch 291, loss is    0.68.\n"," 293/1531 [====>.........................] - ETA: 58s - loss: 0.6879 - accuracy: 0.5529For batch 292, loss is    0.68.\n"," 294/1531 [====>.........................] - ETA: 58s - loss: 0.6879 - accuracy: 0.5530For batch 293, loss is    0.68.\n"," 295/1531 [====>.........................] - ETA: 58s - loss: 0.6878 - accuracy: 0.5531For batch 294, loss is    0.68.\n","For batch 295, loss is    0.68.\n"," 297/1531 [====>.........................] - ETA: 58s - loss: 0.6878 - accuracy: 0.5533For batch 296, loss is    0.68.\n","For batch 297, loss is    0.68.\n"," 299/1531 [====>.........................] - ETA: 58s - loss: 0.6877 - accuracy: 0.5536For batch 298, loss is    0.68.\n","For batch 299, loss is    0.68.\n"," 301/1531 [====>.........................] - ETA: 58s - loss: 0.6877 - accuracy: 0.5538For batch 300, loss is    0.68.\n"," 302/1531 [====>.........................] - ETA: 58s - loss: 0.6876 - accuracy: 0.5539For batch 301, loss is    0.68.\n"," 303/1531 [====>.........................] - ETA: 58s - loss: 0.6876 - accuracy: 0.5540For batch 302, loss is    0.68.\n"," 304/1531 [====>.........................] - ETA: 58s - loss: 0.6876 - accuracy: 0.5541For batch 303, loss is    0.68.\n","For batch 304, loss is    0.68.\n"," 306/1531 [====>.........................] - ETA: 58s - loss: 0.6875 - accuracy: 0.5544For batch 305, loss is    0.68.\n"," 307/1531 [=====>........................] - ETA: 58s - loss: 0.6875 - accuracy: 0.5545For batch 306, loss is    0.68.\n","For batch 307, loss is    0.68.\n"," 309/1531 [=====>........................] - ETA: 58s - loss: 0.6874 - accuracy: 0.5547For batch 308, loss is    0.68.\n"," 310/1531 [=====>........................] - ETA: 58s - loss: 0.6874 - accuracy: 0.5548For batch 309, loss is    0.68.\n"," 311/1531 [=====>........................] - ETA: 58s - loss: 0.6874 - accuracy: 0.5549For batch 310, loss is    0.68.\n","For batch 311, loss is    0.68.\n"," 313/1531 [=====>........................] - ETA: 58s - loss: 0.6873 - accuracy: 0.5551For batch 312, loss is    0.68.\n","For batch 313, loss is    0.68.\n"," 315/1531 [=====>........................] - ETA: 58s - loss: 0.6873 - accuracy: 0.5553For batch 314, loss is    0.68.\n","For batch 315, loss is    0.68.\n"," 317/1531 [=====>........................] - ETA: 57s - loss: 0.6872 - accuracy: 0.5555For batch 316, loss is    0.68.\n"," 318/1531 [=====>........................] - ETA: 57s - loss: 0.6872 - accuracy: 0.5556For batch 317, loss is    0.68.\n"," 319/1531 [=====>........................] - ETA: 57s - loss: 0.6872 - accuracy: 0.5557For batch 318, loss is    0.68.\n"," 320/1531 [=====>........................] - ETA: 57s - loss: 0.6871 - accuracy: 0.5558For batch 319, loss is    0.68.\n","For batch 320, loss is    0.68.\n"," 322/1531 [=====>........................] - ETA: 57s - loss: 0.6871 - accuracy: 0.5560For batch 321, loss is    0.68.\n","For batch 322, loss is    0.68.\n"," 324/1531 [=====>........................] - ETA: 57s - loss: 0.6870 - accuracy: 0.5562For batch 323, loss is    0.68.\n","For batch 324, loss is    0.68.\n"," 326/1531 [=====>........................] - ETA: 57s - loss: 0.6870 - accuracy: 0.5564For batch 325, loss is    0.68.\n"," 327/1531 [=====>........................] - ETA: 57s - loss: 0.6869 - accuracy: 0.5565For batch 326, loss is    0.68.\n","For batch 327, loss is    0.68.\n"," 329/1531 [=====>........................] - ETA: 57s - loss: 0.6869 - accuracy: 0.5567For batch 328, loss is    0.68.\n","For batch 329, loss is    0.68.\n"," 331/1531 [=====>........................] - ETA: 57s - loss: 0.6868 - accuracy: 0.5570For batch 330, loss is    0.68.\n"," 332/1531 [=====>........................] - ETA: 57s - loss: 0.6868 - accuracy: 0.5571For batch 331, loss is    0.68.\n","For batch 332, loss is    0.68.\n"," 334/1531 [=====>........................] - ETA: 57s - loss: 0.6867 - accuracy: 0.5573For batch 333, loss is    0.68.\n"," 335/1531 [=====>........................] - ETA: 57s - loss: 0.6867 - accuracy: 0.5574For batch 334, loss is    0.68.\n"," 336/1531 [=====>........................] - ETA: 57s - loss: 0.6867 - accuracy: 0.5575For batch 335, loss is    0.68.\n","For batch 336, loss is    0.68.\n"," 338/1531 [=====>........................] - ETA: 57s - loss: 0.6866 - accuracy: 0.5576For batch 337, loss is    0.68.\n"," 339/1531 [=====>........................] - ETA: 56s - loss: 0.6866 - accuracy: 0.5577For batch 338, loss is    0.68.\n"," 340/1531 [=====>........................] - ETA: 56s - loss: 0.6866 - accuracy: 0.5578For batch 339, loss is    0.68.\n"," 341/1531 [=====>........................] - ETA: 56s - loss: 0.6865 - accuracy: 0.5579For batch 340, loss is    0.68.\n"," 342/1531 [=====>........................] - ETA: 56s - loss: 0.6865 - accuracy: 0.5580For batch 341, loss is    0.68.\n"," 343/1531 [=====>........................] - ETA: 56s - loss: 0.6865 - accuracy: 0.5581For batch 342, loss is    0.68.\n"," 344/1531 [=====>........................] - ETA: 56s - loss: 0.6864 - accuracy: 0.5582For batch 343, loss is    0.68.\n","For batch 344, loss is    0.68.\n"," 346/1531 [=====>........................] - ETA: 56s - loss: 0.6864 - accuracy: 0.5584For batch 345, loss is    0.68.\n","For batch 346, loss is    0.68.\n"," 348/1531 [=====>........................] - ETA: 56s - loss: 0.6863 - accuracy: 0.5586For batch 347, loss is    0.68.\n"," 349/1531 [=====>........................] - ETA: 56s - loss: 0.6863 - accuracy: 0.5587For batch 348, loss is    0.68.\n"," 350/1531 [=====>........................] - ETA: 56s - loss: 0.6863 - accuracy: 0.5588For batch 349, loss is    0.68.\n"," 351/1531 [=====>........................] - ETA: 56s - loss: 0.6862 - accuracy: 0.5589For batch 350, loss is    0.68.\n"," 352/1531 [=====>........................] - ETA: 56s - loss: 0.6862 - accuracy: 0.5590For batch 351, loss is    0.68.\n"," 353/1531 [=====>........................] - ETA: 56s - loss: 0.6862 - accuracy: 0.5591For batch 352, loss is    0.68.\n"," 354/1531 [=====>........................] - ETA: 56s - loss: 0.6862 - accuracy: 0.5592For batch 353, loss is    0.68.\n"," 355/1531 [=====>........................] - ETA: 56s - loss: 0.6861 - accuracy: 0.5593For batch 354, loss is    0.68.\n"," 356/1531 [=====>........................] - ETA: 56s - loss: 0.6861 - accuracy: 0.5594For batch 355, loss is    0.68.\n"," 357/1531 [=====>........................] - ETA: 56s - loss: 0.6861 - accuracy: 0.5594For batch 356, loss is    0.68.\n"," 358/1531 [======>.......................] - ETA: 56s - loss: 0.6860 - accuracy: 0.5595For batch 357, loss is    0.68.\n"," 359/1531 [======>.......................] - ETA: 56s - loss: 0.6860 - accuracy: 0.5596For batch 358, loss is    0.68.\n","For batch 359, loss is    0.68.\n"," 361/1531 [======>.......................] - ETA: 56s - loss: 0.6860 - accuracy: 0.5598For batch 360, loss is    0.68.\n"," 362/1531 [======>.......................] - ETA: 56s - loss: 0.6859 - accuracy: 0.5599For batch 361, loss is    0.68.\n"," 363/1531 [======>.......................] - ETA: 56s - loss: 0.6859 - accuracy: 0.5600For batch 362, loss is    0.68.\n"," 364/1531 [======>.......................] - ETA: 56s - loss: 0.6859 - accuracy: 0.5601For batch 363, loss is    0.68.\n"," 365/1531 [======>.......................] - ETA: 56s - loss: 0.6858 - accuracy: 0.5602For batch 364, loss is    0.68.\n"," 366/1531 [======>.......................] - ETA: 56s - loss: 0.6858 - accuracy: 0.5603For batch 365, loss is    0.68.\n"," 367/1531 [======>.......................] - ETA: 55s - loss: 0.6858 - accuracy: 0.5603For batch 366, loss is    0.68.\n"," 368/1531 [======>.......................] - ETA: 55s - loss: 0.6858 - accuracy: 0.5604For batch 367, loss is    0.68.\n"," 369/1531 [======>.......................] - ETA: 55s - loss: 0.6857 - accuracy: 0.5605For batch 368, loss is    0.68.\n"," 370/1531 [======>.......................] - ETA: 55s - loss: 0.6857 - accuracy: 0.5606For batch 369, loss is    0.68.\n"," 371/1531 [======>.......................] - ETA: 55s - loss: 0.6857 - accuracy: 0.5607For batch 370, loss is    0.68.\n"," 372/1531 [======>.......................] - ETA: 55s - loss: 0.6856 - accuracy: 0.5608For batch 371, loss is    0.68.\n"," 373/1531 [======>.......................] - ETA: 55s - loss: 0.6856 - accuracy: 0.5609For batch 372, loss is    0.68.\n"," 374/1531 [======>.......................] - ETA: 55s - loss: 0.6856 - accuracy: 0.5609For batch 373, loss is    0.68.\n","For batch 374, loss is    0.67.\n"," 376/1531 [======>.......................] - ETA: 55s - loss: 0.6855 - accuracy: 0.5611For batch 375, loss is    0.67.\n","For batch 376, loss is    0.67.\n"," 378/1531 [======>.......................] - ETA: 55s - loss: 0.6855 - accuracy: 0.5613For batch 377, loss is    0.67.\n"," 379/1531 [======>.......................] - ETA: 55s - loss: 0.6854 - accuracy: 0.5614For batch 378, loss is    0.67.\n","For batch 379, loss is    0.67.\n"," 381/1531 [======>.......................] - ETA: 55s - loss: 0.6854 - accuracy: 0.5615For batch 380, loss is    0.67.\n"," 382/1531 [======>.......................] - ETA: 55s - loss: 0.6853 - accuracy: 0.5616For batch 381, loss is    0.67.\n","For batch 382, loss is    0.67.\n"," 384/1531 [======>.......................] - ETA: 55s - loss: 0.6853 - accuracy: 0.5618For batch 383, loss is    0.67.\n","For batch 384, loss is    0.67.\n"," 386/1531 [======>.......................] - ETA: 55s - loss: 0.6852 - accuracy: 0.5620For batch 385, loss is    0.67.\n","For batch 386, loss is    0.67.\n"," 388/1531 [======>.......................] - ETA: 55s - loss: 0.6852 - accuracy: 0.5621For batch 387, loss is    0.67.\n","For batch 388, loss is    0.67.\n"," 390/1531 [======>.......................] - ETA: 54s - loss: 0.6851 - accuracy: 0.5623For batch 389, loss is    0.67.\n","For batch 390, loss is    0.67.\n"," 392/1531 [======>.......................] - ETA: 54s - loss: 0.6851 - accuracy: 0.5625For batch 391, loss is    0.67.\n"," 393/1531 [======>.......................] - ETA: 54s - loss: 0.6850 - accuracy: 0.5626For batch 392, loss is    0.67.\n"," 394/1531 [======>.......................] - ETA: 54s - loss: 0.6850 - accuracy: 0.5626For batch 393, loss is    0.67.\n"," 395/1531 [======>.......................] - ETA: 54s - loss: 0.6850 - accuracy: 0.5627For batch 394, loss is    0.67.\n"," 396/1531 [======>.......................] - ETA: 54s - loss: 0.6849 - accuracy: 0.5628For batch 395, loss is    0.67.\n"," 397/1531 [======>.......................] - ETA: 54s - loss: 0.6849 - accuracy: 0.5629For batch 396, loss is    0.67.\n"," 398/1531 [======>.......................] - ETA: 54s - loss: 0.6849 - accuracy: 0.5630For batch 397, loss is    0.67.\n","For batch 398, loss is    0.67.\n"," 400/1531 [======>.......................] - ETA: 54s - loss: 0.6848 - accuracy: 0.5631For batch 399, loss is    0.67.\n","For batch 400, loss is    0.67.\n"," 402/1531 [======>.......................] - ETA: 54s - loss: 0.6848 - accuracy: 0.5633For batch 401, loss is    0.67.\n","For batch 402, loss is    0.67.\n"," 404/1531 [======>.......................] - ETA: 54s - loss: 0.6847 - accuracy: 0.5634For batch 403, loss is    0.67.\n","For batch 404, loss is    0.67.\n"," 406/1531 [======>.......................] - ETA: 54s - loss: 0.6846 - accuracy: 0.5636For batch 405, loss is    0.67.\n","For batch 406, loss is    0.67.\n"," 408/1531 [======>.......................] - ETA: 54s - loss: 0.6846 - accuracy: 0.5637For batch 407, loss is    0.67.\n"," 409/1531 [=======>......................] - ETA: 54s - loss: 0.6846 - accuracy: 0.5638For batch 408, loss is    0.67.\n"," 410/1531 [=======>......................] - ETA: 54s - loss: 0.6845 - accuracy: 0.5639For batch 409, loss is    0.67.\n","For batch 410, loss is    0.67.\n"," 412/1531 [=======>......................] - ETA: 53s - loss: 0.6845 - accuracy: 0.5640For batch 411, loss is    0.67.\n"," 413/1531 [=======>......................] - ETA: 53s - loss: 0.6844 - accuracy: 0.5641For batch 412, loss is    0.67.\n","For batch 413, loss is    0.67.\n"," 415/1531 [=======>......................] - ETA: 53s - loss: 0.6844 - accuracy: 0.5643For batch 414, loss is    0.67.\n","For batch 415, loss is    0.67.\n"," 417/1531 [=======>......................] - ETA: 53s - loss: 0.6843 - accuracy: 0.5644For batch 416, loss is    0.67.\n"," 418/1531 [=======>......................] - ETA: 53s - loss: 0.6843 - accuracy: 0.5645For batch 417, loss is    0.67.\n"," 419/1531 [=======>......................] - ETA: 53s - loss: 0.6843 - accuracy: 0.5646For batch 418, loss is    0.67.\n"," 420/1531 [=======>......................] - ETA: 53s - loss: 0.6842 - accuracy: 0.5646For batch 419, loss is    0.67.\n"," 421/1531 [=======>......................] - ETA: 53s - loss: 0.6842 - accuracy: 0.5647For batch 420, loss is    0.67.\n"," 422/1531 [=======>......................] - ETA: 53s - loss: 0.6842 - accuracy: 0.5648For batch 421, loss is    0.67.\n"," 423/1531 [=======>......................] - ETA: 53s - loss: 0.6842 - accuracy: 0.5648For batch 422, loss is    0.67.\n"," 424/1531 [=======>......................] - ETA: 53s - loss: 0.6841 - accuracy: 0.5649For batch 423, loss is    0.67.\n","For batch 424, loss is    0.67.\n"," 426/1531 [=======>......................] - ETA: 53s - loss: 0.6841 - accuracy: 0.5651For batch 425, loss is    0.67.\n"," 427/1531 [=======>......................] - ETA: 53s - loss: 0.6840 - accuracy: 0.5651For batch 426, loss is    0.67.\n"," 428/1531 [=======>......................] - ETA: 53s - loss: 0.6840 - accuracy: 0.5652For batch 427, loss is    0.67.\n"," 429/1531 [=======>......................] - ETA: 53s - loss: 0.6840 - accuracy: 0.5653For batch 428, loss is    0.67.\n"," 430/1531 [=======>......................] - ETA: 53s - loss: 0.6840 - accuracy: 0.5653For batch 429, loss is    0.67.\n"," 431/1531 [=======>......................] - ETA: 53s - loss: 0.6839 - accuracy: 0.5654For batch 430, loss is    0.67.\n"," 432/1531 [=======>......................] - ETA: 53s - loss: 0.6839 - accuracy: 0.5655For batch 431, loss is    0.67.\n"," 433/1531 [=======>......................] - ETA: 53s - loss: 0.6839 - accuracy: 0.5656For batch 432, loss is    0.67.\n"," 434/1531 [=======>......................] - ETA: 53s - loss: 0.6838 - accuracy: 0.5656For batch 433, loss is    0.67.\n","For batch 434, loss is    0.67.\n"," 436/1531 [=======>......................] - ETA: 53s - loss: 0.6838 - accuracy: 0.5658For batch 435, loss is    0.67.\n"," 437/1531 [=======>......................] - ETA: 53s - loss: 0.6838 - accuracy: 0.5658For batch 436, loss is    0.67.\n"," 438/1531 [=======>......................] - ETA: 52s - loss: 0.6837 - accuracy: 0.5659For batch 437, loss is    0.67.\n"," 439/1531 [=======>......................] - ETA: 52s - loss: 0.6837 - accuracy: 0.5660For batch 438, loss is    0.67.\n","For batch 439, loss is    0.67.\n"," 441/1531 [=======>......................] - ETA: 52s - loss: 0.6837 - accuracy: 0.5661For batch 440, loss is    0.67.\n"," 442/1531 [=======>......................] - ETA: 52s - loss: 0.6836 - accuracy: 0.5662For batch 441, loss is    0.67.\n"," 443/1531 [=======>......................] - ETA: 52s - loss: 0.6836 - accuracy: 0.5662For batch 442, loss is    0.67.\n"," 444/1531 [=======>......................] - ETA: 52s - loss: 0.6836 - accuracy: 0.5663For batch 443, loss is    0.67.\n"," 445/1531 [=======>......................] - ETA: 52s - loss: 0.6835 - accuracy: 0.5664For batch 444, loss is    0.67.\n"," 446/1531 [=======>......................] - ETA: 52s - loss: 0.6835 - accuracy: 0.5664For batch 445, loss is    0.67.\n","For batch 446, loss is    0.67.\n"," 448/1531 [=======>......................] - ETA: 52s - loss: 0.6835 - accuracy: 0.5666For batch 447, loss is    0.67.\n"," 449/1531 [=======>......................] - ETA: 52s - loss: 0.6834 - accuracy: 0.5666For batch 448, loss is    0.67.\n"," 450/1531 [=======>......................] - ETA: 52s - loss: 0.6834 - accuracy: 0.5667For batch 449, loss is    0.67.\n","For batch 450, loss is    0.67.\n"," 452/1531 [=======>......................] - ETA: 52s - loss: 0.6833 - accuracy: 0.5668For batch 451, loss is    0.67.\n"," 453/1531 [=======>......................] - ETA: 52s - loss: 0.6833 - accuracy: 0.5669For batch 452, loss is    0.67.\n"," 454/1531 [=======>......................] - ETA: 52s - loss: 0.6833 - accuracy: 0.5670For batch 453, loss is    0.67.\n"," 455/1531 [=======>......................] - ETA: 52s - loss: 0.6833 - accuracy: 0.5670For batch 454, loss is    0.67.\n"," 456/1531 [=======>......................] - ETA: 52s - loss: 0.6832 - accuracy: 0.5671For batch 455, loss is    0.67.\n"," 457/1531 [=======>......................] - ETA: 52s - loss: 0.6832 - accuracy: 0.5671For batch 456, loss is    0.67.\n"," 458/1531 [=======>......................] - ETA: 52s - loss: 0.6832 - accuracy: 0.5672For batch 457, loss is    0.67.\n"," 459/1531 [=======>......................] - ETA: 52s - loss: 0.6831 - accuracy: 0.5673For batch 458, loss is    0.67.\n"," 460/1531 [========>.....................] - ETA: 52s - loss: 0.6831 - accuracy: 0.5673For batch 459, loss is    0.67.\n"," 461/1531 [========>.....................] - ETA: 52s - loss: 0.6831 - accuracy: 0.5674For batch 460, loss is    0.67.\n","For batch 461, loss is    0.67.\n"," 463/1531 [========>.....................] - ETA: 51s - loss: 0.6830 - accuracy: 0.5675For batch 462, loss is    0.67.\n"," 464/1531 [========>.....................] - ETA: 51s - loss: 0.6830 - accuracy: 0.5676For batch 463, loss is    0.67.\n"," 465/1531 [========>.....................] - ETA: 51s - loss: 0.6830 - accuracy: 0.5677For batch 464, loss is    0.67.\n"," 466/1531 [========>.....................] - ETA: 51s - loss: 0.6830 - accuracy: 0.5677For batch 465, loss is    0.67.\n"," 467/1531 [========>.....................] - ETA: 51s - loss: 0.6829 - accuracy: 0.5678For batch 466, loss is    0.67.\n"," 468/1531 [========>.....................] - ETA: 51s - loss: 0.6829 - accuracy: 0.5679For batch 467, loss is    0.67.\n","For batch 468, loss is    0.67.\n"," 470/1531 [========>.....................] - ETA: 51s - loss: 0.6828 - accuracy: 0.5680For batch 469, loss is    0.67.\n"," 471/1531 [========>.....................] - ETA: 51s - loss: 0.6828 - accuracy: 0.5680For batch 470, loss is    0.67.\n"," 472/1531 [========>.....................] - ETA: 51s - loss: 0.6828 - accuracy: 0.5681For batch 471, loss is    0.67.\n"," 473/1531 [========>.....................] - ETA: 51s - loss: 0.6828 - accuracy: 0.5682For batch 472, loss is    0.67.\n"," 474/1531 [========>.....................] - ETA: 51s - loss: 0.6827 - accuracy: 0.5682For batch 473, loss is    0.67.\n"," 475/1531 [========>.....................] - ETA: 51s - loss: 0.6827 - accuracy: 0.5683For batch 474, loss is    0.67.\n"," 476/1531 [========>.....................] - ETA: 51s - loss: 0.6827 - accuracy: 0.5684For batch 475, loss is    0.67.\n"," 477/1531 [========>.....................] - ETA: 51s - loss: 0.6826 - accuracy: 0.5684For batch 476, loss is    0.67.\n"," 478/1531 [========>.....................] - ETA: 51s - loss: 0.6826 - accuracy: 0.5685For batch 477, loss is    0.67.\n"," 479/1531 [========>.....................] - ETA: 51s - loss: 0.6826 - accuracy: 0.5685For batch 478, loss is    0.67.\n"," 480/1531 [========>.....................] - ETA: 51s - loss: 0.6826 - accuracy: 0.5686For batch 479, loss is    0.67.\n"," 481/1531 [========>.....................] - ETA: 51s - loss: 0.6825 - accuracy: 0.5687For batch 480, loss is    0.67.\n","For batch 481, loss is    0.67.\n"," 483/1531 [========>.....................] - ETA: 51s - loss: 0.6825 - accuracy: 0.5688For batch 482, loss is    0.67.\n"," 484/1531 [========>.....................] - ETA: 51s - loss: 0.6824 - accuracy: 0.5688For batch 483, loss is    0.67.\n"," 485/1531 [========>.....................] - ETA: 51s - loss: 0.6824 - accuracy: 0.5689For batch 484, loss is    0.67.\n"," 486/1531 [========>.....................] - ETA: 51s - loss: 0.6824 - accuracy: 0.5690For batch 485, loss is    0.67.\n"," 487/1531 [========>.....................] - ETA: 50s - loss: 0.6824 - accuracy: 0.5690For batch 486, loss is    0.67.\n"," 488/1531 [========>.....................] - ETA: 50s - loss: 0.6823 - accuracy: 0.5691For batch 487, loss is    0.67.\n"," 489/1531 [========>.....................] - ETA: 50s - loss: 0.6823 - accuracy: 0.5691For batch 488, loss is    0.67.\n"," 490/1531 [========>.....................] - ETA: 50s - loss: 0.6823 - accuracy: 0.5692For batch 489, loss is    0.67.\n"," 491/1531 [========>.....................] - ETA: 50s - loss: 0.6823 - accuracy: 0.5693For batch 490, loss is    0.67.\n"," 492/1531 [========>.....................] - ETA: 50s - loss: 0.6822 - accuracy: 0.5693For batch 491, loss is    0.67.\n"," 493/1531 [========>.....................] - ETA: 50s - loss: 0.6822 - accuracy: 0.5694For batch 492, loss is    0.67.\n"," 494/1531 [========>.....................] - ETA: 50s - loss: 0.6822 - accuracy: 0.5694For batch 493, loss is    0.67.\n"," 495/1531 [========>.....................] - ETA: 50s - loss: 0.6821 - accuracy: 0.5695For batch 494, loss is    0.67.\n"," 496/1531 [========>.....................] - ETA: 50s - loss: 0.6821 - accuracy: 0.5696For batch 495, loss is    0.67.\n"," 497/1531 [========>.....................] - ETA: 50s - loss: 0.6821 - accuracy: 0.5696For batch 496, loss is    0.67.\n"," 498/1531 [========>.....................] - ETA: 50s - loss: 0.6821 - accuracy: 0.5697For batch 497, loss is    0.67.\n"," 499/1531 [========>.....................] - ETA: 50s - loss: 0.6820 - accuracy: 0.5697For batch 498, loss is    0.67.\n"," 500/1531 [========>.....................] - ETA: 50s - loss: 0.6820 - accuracy: 0.5698For batch 499, loss is    0.67.\n"," 501/1531 [========>.....................] - ETA: 50s - loss: 0.6820 - accuracy: 0.5699For batch 500, loss is    0.67.\n"," 502/1531 [========>.....................] - ETA: 50s - loss: 0.6819 - accuracy: 0.5699For batch 501, loss is    0.67.\n"," 503/1531 [========>.....................] - ETA: 50s - loss: 0.6819 - accuracy: 0.5700For batch 502, loss is    0.67.\n"," 504/1531 [========>.....................] - ETA: 50s - loss: 0.6819 - accuracy: 0.5700For batch 503, loss is    0.67.\n"," 505/1531 [========>.....................] - ETA: 50s - loss: 0.6819 - accuracy: 0.5701For batch 504, loss is    0.67.\n"," 506/1531 [========>.....................] - ETA: 50s - loss: 0.6818 - accuracy: 0.5701For batch 505, loss is    0.67.\n"," 507/1531 [========>.....................] - ETA: 50s - loss: 0.6818 - accuracy: 0.5702For batch 506, loss is    0.67.\n"," 508/1531 [========>.....................] - ETA: 50s - loss: 0.6818 - accuracy: 0.5703For batch 507, loss is    0.67.\n"," 509/1531 [========>.....................] - ETA: 50s - loss: 0.6818 - accuracy: 0.5703For batch 508, loss is    0.67.\n"," 510/1531 [========>.....................] - ETA: 50s - loss: 0.6817 - accuracy: 0.5704For batch 509, loss is    0.67.\n","For batch 510, loss is    0.67.\n"," 512/1531 [=========>....................] - ETA: 50s - loss: 0.6817 - accuracy: 0.5705For batch 511, loss is    0.67.\n","For batch 512, loss is    0.67.\n"," 514/1531 [=========>....................] - ETA: 49s - loss: 0.6816 - accuracy: 0.5706For batch 513, loss is    0.67.\n"," 515/1531 [=========>....................] - ETA: 49s - loss: 0.6816 - accuracy: 0.5706For batch 514, loss is    0.67.\n"," 516/1531 [=========>....................] - ETA: 49s - loss: 0.6816 - accuracy: 0.5707For batch 515, loss is    0.67.\n"," 517/1531 [=========>....................] - ETA: 49s - loss: 0.6815 - accuracy: 0.5708For batch 516, loss is    0.67.\n"," 518/1531 [=========>....................] - ETA: 49s - loss: 0.6815 - accuracy: 0.5708For batch 517, loss is    0.67.\n"," 519/1531 [=========>....................] - ETA: 49s - loss: 0.6815 - accuracy: 0.5709For batch 518, loss is    0.67.\n"," 520/1531 [=========>....................] - ETA: 49s - loss: 0.6815 - accuracy: 0.5709For batch 519, loss is    0.67.\n"," 521/1531 [=========>....................] - ETA: 49s - loss: 0.6814 - accuracy: 0.5710For batch 520, loss is    0.67.\n"," 522/1531 [=========>....................] - ETA: 49s - loss: 0.6814 - accuracy: 0.5710For batch 521, loss is    0.67.\n"," 523/1531 [=========>....................] - ETA: 49s - loss: 0.6814 - accuracy: 0.5711For batch 522, loss is    0.67.\n"," 524/1531 [=========>....................] - ETA: 49s - loss: 0.6814 - accuracy: 0.5711For batch 523, loss is    0.67.\n"," 525/1531 [=========>....................] - ETA: 49s - loss: 0.6813 - accuracy: 0.5712For batch 524, loss is    0.67.\n"," 526/1531 [=========>....................] - ETA: 49s - loss: 0.6813 - accuracy: 0.5713For batch 525, loss is    0.67.\n","For batch 526, loss is    0.67.\n"," 528/1531 [=========>....................] - ETA: 49s - loss: 0.6813 - accuracy: 0.5714For batch 527, loss is    0.67.\n"," 529/1531 [=========>....................] - ETA: 49s - loss: 0.6812 - accuracy: 0.5714For batch 528, loss is    0.67.\n","For batch 529, loss is    0.67.\n"," 531/1531 [=========>....................] - ETA: 49s - loss: 0.6812 - accuracy: 0.5715For batch 530, loss is    0.67.\n"," 532/1531 [=========>....................] - ETA: 49s - loss: 0.6811 - accuracy: 0.5716For batch 531, loss is    0.67.\n","For batch 532, loss is    0.67.\n"," 534/1531 [=========>....................] - ETA: 49s - loss: 0.6811 - accuracy: 0.5717For batch 533, loss is    0.67.\n"," 535/1531 [=========>....................] - ETA: 49s - loss: 0.6811 - accuracy: 0.5717For batch 534, loss is    0.67.\n"," 536/1531 [=========>....................] - ETA: 48s - loss: 0.6810 - accuracy: 0.5718For batch 535, loss is    0.67.\n","For batch 536, loss is    0.67.\n"," 538/1531 [=========>....................] - ETA: 48s - loss: 0.6810 - accuracy: 0.5719For batch 537, loss is    0.67.\n"," 539/1531 [=========>....................] - ETA: 48s - loss: 0.6810 - accuracy: 0.5720For batch 538, loss is    0.67.\n"," 540/1531 [=========>....................] - ETA: 48s - loss: 0.6809 - accuracy: 0.5720For batch 539, loss is    0.67.\n"," 541/1531 [=========>....................] - ETA: 48s - loss: 0.6809 - accuracy: 0.5721For batch 540, loss is    0.67.\n"," 542/1531 [=========>....................] - ETA: 48s - loss: 0.6809 - accuracy: 0.5721For batch 541, loss is    0.67.\n"," 543/1531 [=========>....................] - ETA: 48s - loss: 0.6809 - accuracy: 0.5722For batch 542, loss is    0.67.\n"," 544/1531 [=========>....................] - ETA: 48s - loss: 0.6808 - accuracy: 0.5722For batch 543, loss is    0.67.\n"," 545/1531 [=========>....................] - ETA: 48s - loss: 0.6808 - accuracy: 0.5723For batch 544, loss is    0.67.\n"," 546/1531 [=========>....................] - ETA: 48s - loss: 0.6808 - accuracy: 0.5723For batch 545, loss is    0.67.\n"," 547/1531 [=========>....................] - ETA: 48s - loss: 0.6807 - accuracy: 0.5724For batch 546, loss is    0.67.\n"," 548/1531 [=========>....................] - ETA: 48s - loss: 0.6807 - accuracy: 0.5725For batch 547, loss is    0.67.\n"," 549/1531 [=========>....................] - ETA: 48s - loss: 0.6807 - accuracy: 0.5725For batch 548, loss is    0.67.\n"," 550/1531 [=========>....................] - ETA: 48s - loss: 0.6807 - accuracy: 0.5726For batch 549, loss is    0.67.\n"," 551/1531 [=========>....................] - ETA: 48s - loss: 0.6806 - accuracy: 0.5726For batch 550, loss is    0.67.\n"," 552/1531 [=========>....................] - ETA: 48s - loss: 0.6806 - accuracy: 0.5727For batch 551, loss is    0.67.\n"," 553/1531 [=========>....................] - ETA: 48s - loss: 0.6806 - accuracy: 0.5727For batch 552, loss is    0.67.\n"," 554/1531 [=========>....................] - ETA: 48s - loss: 0.6806 - accuracy: 0.5728For batch 553, loss is    0.67.\n"," 555/1531 [=========>....................] - ETA: 48s - loss: 0.6805 - accuracy: 0.5728For batch 554, loss is    0.67.\n"," 556/1531 [=========>....................] - ETA: 48s - loss: 0.6805 - accuracy: 0.5729For batch 555, loss is    0.67.\n"," 557/1531 [=========>....................] - ETA: 48s - loss: 0.6805 - accuracy: 0.5729For batch 556, loss is    0.67.\n"," 558/1531 [=========>....................] - ETA: 48s - loss: 0.6804 - accuracy: 0.5730For batch 557, loss is    0.67.\n"," 559/1531 [=========>....................] - ETA: 48s - loss: 0.6804 - accuracy: 0.5730For batch 558, loss is    0.67.\n"," 560/1531 [=========>....................] - ETA: 48s - loss: 0.6804 - accuracy: 0.5731For batch 559, loss is    0.67.\n","For batch 560, loss is    0.67.\n"," 562/1531 [==========>...................] - ETA: 47s - loss: 0.6803 - accuracy: 0.5732For batch 561, loss is    0.67.\n"," 563/1531 [==========>...................] - ETA: 47s - loss: 0.6803 - accuracy: 0.5733For batch 562, loss is    0.67.\n"," 564/1531 [==========>...................] - ETA: 47s - loss: 0.6803 - accuracy: 0.5733For batch 563, loss is    0.67.\n"," 565/1531 [==========>...................] - ETA: 47s - loss: 0.6803 - accuracy: 0.5734For batch 564, loss is    0.67.\n"," 566/1531 [==========>...................] - ETA: 47s - loss: 0.6802 - accuracy: 0.5734For batch 565, loss is    0.67.\n"," 567/1531 [==========>...................] - ETA: 47s - loss: 0.6802 - accuracy: 0.5735For batch 566, loss is    0.67.\n"," 568/1531 [==========>...................] - ETA: 47s - loss: 0.6802 - accuracy: 0.5735For batch 567, loss is    0.67.\n"," 569/1531 [==========>...................] - ETA: 47s - loss: 0.6802 - accuracy: 0.5736For batch 568, loss is    0.67.\n"," 570/1531 [==========>...................] - ETA: 47s - loss: 0.6801 - accuracy: 0.5736For batch 569, loss is    0.67.\n"," 571/1531 [==========>...................] - ETA: 47s - loss: 0.6801 - accuracy: 0.5737For batch 570, loss is    0.67.\n"," 572/1531 [==========>...................] - ETA: 47s - loss: 0.6801 - accuracy: 0.5737For batch 571, loss is    0.67.\n"," 573/1531 [==========>...................] - ETA: 47s - loss: 0.6800 - accuracy: 0.5738For batch 572, loss is    0.66.\n"," 574/1531 [==========>...................] - ETA: 47s - loss: 0.6800 - accuracy: 0.5738For batch 573, loss is    0.66.\n"," 575/1531 [==========>...................] - ETA: 47s - loss: 0.6800 - accuracy: 0.5739For batch 574, loss is    0.66.\n"," 576/1531 [==========>...................] - ETA: 47s - loss: 0.6800 - accuracy: 0.5739For batch 575, loss is    0.66.\n"," 577/1531 [==========>...................] - ETA: 47s - loss: 0.6799 - accuracy: 0.5740For batch 576, loss is    0.66.\n"," 578/1531 [==========>...................] - ETA: 47s - loss: 0.6799 - accuracy: 0.5740For batch 577, loss is    0.66.\n"," 579/1531 [==========>...................] - ETA: 47s - loss: 0.6799 - accuracy: 0.5741For batch 578, loss is    0.66.\n"," 580/1531 [==========>...................] - ETA: 47s - loss: 0.6799 - accuracy: 0.5741For batch 579, loss is    0.66.\n"," 581/1531 [==========>...................] - ETA: 47s - loss: 0.6798 - accuracy: 0.5742For batch 580, loss is    0.66.\n"," 582/1531 [==========>...................] - ETA: 47s - loss: 0.6798 - accuracy: 0.5742For batch 581, loss is    0.66.\n"," 583/1531 [==========>...................] - ETA: 47s - loss: 0.6798 - accuracy: 0.5743For batch 582, loss is    0.66.\n"," 584/1531 [==========>...................] - ETA: 47s - loss: 0.6798 - accuracy: 0.5743For batch 583, loss is    0.66.\n"," 585/1531 [==========>...................] - ETA: 47s - loss: 0.6797 - accuracy: 0.5744For batch 584, loss is    0.66.\n"," 586/1531 [==========>...................] - ETA: 47s - loss: 0.6797 - accuracy: 0.5744For batch 585, loss is    0.66.\n"," 587/1531 [==========>...................] - ETA: 46s - loss: 0.6797 - accuracy: 0.5745For batch 586, loss is    0.66.\n"," 588/1531 [==========>...................] - ETA: 46s - loss: 0.6797 - accuracy: 0.5745For batch 587, loss is    0.66.\n"," 589/1531 [==========>...................] - ETA: 46s - loss: 0.6796 - accuracy: 0.5746For batch 588, loss is    0.66.\n"," 590/1531 [==========>...................] - ETA: 46s - loss: 0.6796 - accuracy: 0.5746For batch 589, loss is    0.66.\n","For batch 590, loss is    0.66.\n"," 592/1531 [==========>...................] - ETA: 46s - loss: 0.6795 - accuracy: 0.5747For batch 591, loss is    0.66.\n"," 593/1531 [==========>...................] - ETA: 46s - loss: 0.6795 - accuracy: 0.5748For batch 592, loss is    0.66.\n"," 594/1531 [==========>...................] - ETA: 46s - loss: 0.6795 - accuracy: 0.5748For batch 593, loss is    0.66.\n"," 595/1531 [==========>...................] - ETA: 46s - loss: 0.6795 - accuracy: 0.5749For batch 594, loss is    0.66.\n"," 596/1531 [==========>...................] - ETA: 46s - loss: 0.6794 - accuracy: 0.5749For batch 595, loss is    0.66.\n"," 597/1531 [==========>...................] - ETA: 46s - loss: 0.6794 - accuracy: 0.5750For batch 596, loss is    0.66.\n"," 598/1531 [==========>...................] - ETA: 46s - loss: 0.6794 - accuracy: 0.5750For batch 597, loss is    0.66.\n"," 599/1531 [==========>...................] - ETA: 46s - loss: 0.6794 - accuracy: 0.5751For batch 598, loss is    0.66.\n"," 600/1531 [==========>...................] - ETA: 46s - loss: 0.6793 - accuracy: 0.5752For batch 599, loss is    0.66.\n"," 601/1531 [==========>...................] - ETA: 46s - loss: 0.6793 - accuracy: 0.5752For batch 600, loss is    0.66.\n"," 602/1531 [==========>...................] - ETA: 46s - loss: 0.6793 - accuracy: 0.5753For batch 601, loss is    0.66.\n"," 603/1531 [==========>...................] - ETA: 46s - loss: 0.6793 - accuracy: 0.5753For batch 602, loss is    0.66.\n"," 604/1531 [==========>...................] - ETA: 46s - loss: 0.6792 - accuracy: 0.5754For batch 603, loss is    0.66.\n"," 605/1531 [==========>...................] - ETA: 46s - loss: 0.6792 - accuracy: 0.5754For batch 604, loss is    0.66.\n"," 606/1531 [==========>...................] - ETA: 46s - loss: 0.6792 - accuracy: 0.5755For batch 605, loss is    0.66.\n"," 607/1531 [==========>...................] - ETA: 46s - loss: 0.6791 - accuracy: 0.5755For batch 606, loss is    0.66.\n"," 608/1531 [==========>...................] - ETA: 46s - loss: 0.6791 - accuracy: 0.5756For batch 607, loss is    0.66.\n"," 609/1531 [==========>...................] - ETA: 46s - loss: 0.6791 - accuracy: 0.5756For batch 608, loss is    0.66.\n"," 610/1531 [==========>...................] - ETA: 46s - loss: 0.6791 - accuracy: 0.5757For batch 609, loss is    0.66.\n"," 611/1531 [==========>...................] - ETA: 45s - loss: 0.6790 - accuracy: 0.5757For batch 610, loss is    0.66.\n"," 612/1531 [==========>...................] - ETA: 45s - loss: 0.6790 - accuracy: 0.5758For batch 611, loss is    0.66.\n"," 613/1531 [===========>..................] - ETA: 45s - loss: 0.6790 - accuracy: 0.5758For batch 612, loss is    0.66.\n"," 614/1531 [===========>..................] - ETA: 45s - loss: 0.6790 - accuracy: 0.5759For batch 613, loss is    0.66.\n"," 615/1531 [===========>..................] - ETA: 45s - loss: 0.6789 - accuracy: 0.5759For batch 614, loss is    0.66.\n"," 616/1531 [===========>..................] - ETA: 45s - loss: 0.6789 - accuracy: 0.5760For batch 615, loss is    0.66.\n"," 617/1531 [===========>..................] - ETA: 45s - loss: 0.6789 - accuracy: 0.5760For batch 616, loss is    0.66.\n"," 618/1531 [===========>..................] - ETA: 45s - loss: 0.6788 - accuracy: 0.5761For batch 617, loss is    0.66.\n"," 619/1531 [===========>..................] - ETA: 45s - loss: 0.6788 - accuracy: 0.5761For batch 618, loss is    0.66.\n"," 620/1531 [===========>..................] - ETA: 45s - loss: 0.6788 - accuracy: 0.5762For batch 619, loss is    0.66.\n"," 621/1531 [===========>..................] - ETA: 45s - loss: 0.6788 - accuracy: 0.5762For batch 620, loss is    0.66.\n"," 622/1531 [===========>..................] - ETA: 45s - loss: 0.6787 - accuracy: 0.5763For batch 621, loss is    0.66.\n"," 623/1531 [===========>..................] - ETA: 45s - loss: 0.6787 - accuracy: 0.5763For batch 622, loss is    0.66.\n"," 624/1531 [===========>..................] - ETA: 45s - loss: 0.6787 - accuracy: 0.5764For batch 623, loss is    0.66.\n"," 625/1531 [===========>..................] - ETA: 45s - loss: 0.6787 - accuracy: 0.5764For batch 624, loss is    0.66.\n"," 626/1531 [===========>..................] - ETA: 45s - loss: 0.6786 - accuracy: 0.5765For batch 625, loss is    0.66.\n"," 627/1531 [===========>..................] - ETA: 45s - loss: 0.6786 - accuracy: 0.5765For batch 626, loss is    0.66.\n"," 628/1531 [===========>..................] - ETA: 45s - loss: 0.6786 - accuracy: 0.5766For batch 627, loss is    0.66.\n"," 629/1531 [===========>..................] - ETA: 45s - loss: 0.6785 - accuracy: 0.5766For batch 628, loss is    0.66.\n"," 630/1531 [===========>..................] - ETA: 45s - loss: 0.6785 - accuracy: 0.5767For batch 629, loss is    0.66.\n"," 631/1531 [===========>..................] - ETA: 45s - loss: 0.6785 - accuracy: 0.5768For batch 630, loss is    0.66.\n"," 632/1531 [===========>..................] - ETA: 45s - loss: 0.6785 - accuracy: 0.5768For batch 631, loss is    0.66.\n"," 633/1531 [===========>..................] - ETA: 45s - loss: 0.6784 - accuracy: 0.5769For batch 632, loss is    0.66.\n"," 634/1531 [===========>..................] - ETA: 44s - loss: 0.6784 - accuracy: 0.5769For batch 633, loss is    0.66.\n"," 635/1531 [===========>..................] - ETA: 44s - loss: 0.6784 - accuracy: 0.5770For batch 634, loss is    0.66.\n"," 636/1531 [===========>..................] - ETA: 44s - loss: 0.6784 - accuracy: 0.5770For batch 635, loss is    0.66.\n"," 637/1531 [===========>..................] - ETA: 44s - loss: 0.6783 - accuracy: 0.5771For batch 636, loss is    0.66.\n"," 638/1531 [===========>..................] - ETA: 44s - loss: 0.6783 - accuracy: 0.5771For batch 637, loss is    0.66.\n"," 639/1531 [===========>..................] - ETA: 44s - loss: 0.6783 - accuracy: 0.5772For batch 638, loss is    0.66.\n"," 640/1531 [===========>..................] - ETA: 44s - loss: 0.6782 - accuracy: 0.5772For batch 639, loss is    0.66.\n"," 641/1531 [===========>..................] - ETA: 44s - loss: 0.6782 - accuracy: 0.5773For batch 640, loss is    0.66.\n"," 642/1531 [===========>..................] - ETA: 44s - loss: 0.6782 - accuracy: 0.5773For batch 641, loss is    0.66.\n"," 643/1531 [===========>..................] - ETA: 44s - loss: 0.6782 - accuracy: 0.5774For batch 642, loss is    0.66.\n"," 644/1531 [===========>..................] - ETA: 44s - loss: 0.6781 - accuracy: 0.5774For batch 643, loss is    0.66.\n"," 645/1531 [===========>..................] - ETA: 44s - loss: 0.6781 - accuracy: 0.5774For batch 644, loss is    0.66.\n"," 646/1531 [===========>..................] - ETA: 44s - loss: 0.6781 - accuracy: 0.5775For batch 645, loss is    0.66.\n"," 647/1531 [===========>..................] - ETA: 44s - loss: 0.6781 - accuracy: 0.5775For batch 646, loss is    0.66.\n"," 648/1531 [===========>..................] - ETA: 44s - loss: 0.6780 - accuracy: 0.5776For batch 647, loss is    0.66.\n"," 649/1531 [===========>..................] - ETA: 44s - loss: 0.6780 - accuracy: 0.5776For batch 648, loss is    0.66.\n"," 650/1531 [===========>..................] - ETA: 44s - loss: 0.6780 - accuracy: 0.5777For batch 649, loss is    0.66.\n"," 651/1531 [===========>..................] - ETA: 44s - loss: 0.6780 - accuracy: 0.5777For batch 650, loss is    0.66.\n"," 652/1531 [===========>..................] - ETA: 44s - loss: 0.6779 - accuracy: 0.5778For batch 651, loss is    0.66.\n"," 653/1531 [===========>..................] - ETA: 44s - loss: 0.6779 - accuracy: 0.5778For batch 652, loss is    0.66.\n"," 654/1531 [===========>..................] - ETA: 44s - loss: 0.6779 - accuracy: 0.5779For batch 653, loss is    0.66.\n"," 655/1531 [===========>..................] - ETA: 44s - loss: 0.6779 - accuracy: 0.5779For batch 654, loss is    0.66.\n"," 656/1531 [===========>..................] - ETA: 44s - loss: 0.6778 - accuracy: 0.5780For batch 655, loss is    0.66.\n"," 657/1531 [===========>..................] - ETA: 44s - loss: 0.6778 - accuracy: 0.5780For batch 656, loss is    0.66.\n"," 658/1531 [===========>..................] - ETA: 43s - loss: 0.6778 - accuracy: 0.5781For batch 657, loss is    0.66.\n"," 659/1531 [===========>..................] - ETA: 43s - loss: 0.6778 - accuracy: 0.5781For batch 658, loss is    0.66.\n"," 660/1531 [===========>..................] - ETA: 43s - loss: 0.6777 - accuracy: 0.5782For batch 659, loss is    0.66.\n"," 661/1531 [===========>..................] - ETA: 43s - loss: 0.6777 - accuracy: 0.5782For batch 660, loss is    0.66.\n"," 662/1531 [===========>..................] - ETA: 43s - loss: 0.6777 - accuracy: 0.5783For batch 661, loss is    0.66.\n"," 663/1531 [===========>..................] - ETA: 43s - loss: 0.6777 - accuracy: 0.5783For batch 662, loss is    0.66.\n"," 664/1531 [============>.................] - ETA: 43s - loss: 0.6776 - accuracy: 0.5784For batch 663, loss is    0.66.\n"," 665/1531 [============>.................] - ETA: 43s - loss: 0.6776 - accuracy: 0.5784For batch 664, loss is    0.66.\n"," 666/1531 [============>.................] - ETA: 43s - loss: 0.6776 - accuracy: 0.5785For batch 665, loss is    0.66.\n"," 667/1531 [============>.................] - ETA: 43s - loss: 0.6775 - accuracy: 0.5785For batch 666, loss is    0.66.\n"," 668/1531 [============>.................] - ETA: 43s - loss: 0.6775 - accuracy: 0.5786For batch 667, loss is    0.66.\n"," 669/1531 [============>.................] - ETA: 43s - loss: 0.6775 - accuracy: 0.5786For batch 668, loss is    0.66.\n"," 670/1531 [============>.................] - ETA: 43s - loss: 0.6775 - accuracy: 0.5786For batch 669, loss is    0.66.\n"," 671/1531 [============>.................] - ETA: 43s - loss: 0.6774 - accuracy: 0.5787For batch 670, loss is    0.66.\n"," 672/1531 [============>.................] - ETA: 43s - loss: 0.6774 - accuracy: 0.5787For batch 671, loss is    0.66.\n"," 673/1531 [============>.................] - ETA: 43s - loss: 0.6774 - accuracy: 0.5788For batch 672, loss is    0.66.\n"," 674/1531 [============>.................] - ETA: 43s - loss: 0.6774 - accuracy: 0.5788For batch 673, loss is    0.66.\n"," 675/1531 [============>.................] - ETA: 43s - loss: 0.6773 - accuracy: 0.5789For batch 674, loss is    0.66.\n"," 676/1531 [============>.................] - ETA: 43s - loss: 0.6773 - accuracy: 0.5789For batch 675, loss is    0.66.\n"," 677/1531 [============>.................] - ETA: 43s - loss: 0.6773 - accuracy: 0.5790For batch 676, loss is    0.66.\n"," 678/1531 [============>.................] - ETA: 43s - loss: 0.6773 - accuracy: 0.5790For batch 677, loss is    0.66.\n"," 679/1531 [============>.................] - ETA: 43s - loss: 0.6772 - accuracy: 0.5791For batch 678, loss is    0.66.\n"," 680/1531 [============>.................] - ETA: 43s - loss: 0.6772 - accuracy: 0.5791For batch 679, loss is    0.66.\n"," 681/1531 [============>.................] - ETA: 43s - loss: 0.6772 - accuracy: 0.5791For batch 680, loss is    0.66.\n"," 682/1531 [============>.................] - ETA: 42s - loss: 0.6772 - accuracy: 0.5792For batch 681, loss is    0.66.\n"," 683/1531 [============>.................] - ETA: 42s - loss: 0.6771 - accuracy: 0.5792For batch 682, loss is    0.66.\n"," 684/1531 [============>.................] - ETA: 42s - loss: 0.6771 - accuracy: 0.5793For batch 683, loss is    0.66.\n"," 685/1531 [============>.................] - ETA: 42s - loss: 0.6771 - accuracy: 0.5793For batch 684, loss is    0.66.\n"," 686/1531 [============>.................] - ETA: 42s - loss: 0.6771 - accuracy: 0.5794For batch 685, loss is    0.66.\n"," 687/1531 [============>.................] - ETA: 42s - loss: 0.6770 - accuracy: 0.5794For batch 686, loss is    0.66.\n"," 688/1531 [============>.................] - ETA: 42s - loss: 0.6770 - accuracy: 0.5795For batch 687, loss is    0.66.\n"," 689/1531 [============>.................] - ETA: 42s - loss: 0.6770 - accuracy: 0.5795For batch 688, loss is    0.66.\n"," 690/1531 [============>.................] - ETA: 42s - loss: 0.6770 - accuracy: 0.5796For batch 689, loss is    0.66.\n"," 691/1531 [============>.................] - ETA: 42s - loss: 0.6769 - accuracy: 0.5796For batch 690, loss is    0.66.\n"," 692/1531 [============>.................] - ETA: 42s - loss: 0.6769 - accuracy: 0.5796For batch 691, loss is    0.66.\n"," 693/1531 [============>.................] - ETA: 42s - loss: 0.6769 - accuracy: 0.5797For batch 692, loss is    0.66.\n"," 694/1531 [============>.................] - ETA: 42s - loss: 0.6769 - accuracy: 0.5797For batch 693, loss is    0.66.\n"," 695/1531 [============>.................] - ETA: 42s - loss: 0.6768 - accuracy: 0.5798For batch 694, loss is    0.66.\n"," 696/1531 [============>.................] - ETA: 42s - loss: 0.6768 - accuracy: 0.5798For batch 695, loss is    0.66.\n"," 697/1531 [============>.................] - ETA: 42s - loss: 0.6768 - accuracy: 0.5799For batch 696, loss is    0.66.\n"," 698/1531 [============>.................] - ETA: 42s - loss: 0.6768 - accuracy: 0.5799For batch 697, loss is    0.66.\n"," 699/1531 [============>.................] - ETA: 42s - loss: 0.6767 - accuracy: 0.5799For batch 698, loss is    0.66.\n"," 700/1531 [============>.................] - ETA: 42s - loss: 0.6767 - accuracy: 0.5800For batch 699, loss is    0.66.\n"," 701/1531 [============>.................] - ETA: 42s - loss: 0.6767 - accuracy: 0.5800For batch 700, loss is    0.66.\n"," 702/1531 [============>.................] - ETA: 42s - loss: 0.6767 - accuracy: 0.5801For batch 701, loss is    0.66.\n"," 703/1531 [============>.................] - ETA: 42s - loss: 0.6766 - accuracy: 0.5801For batch 702, loss is    0.66.\n"," 704/1531 [============>.................] - ETA: 42s - loss: 0.6766 - accuracy: 0.5802For batch 703, loss is    0.66.\n"," 705/1531 [============>.................] - ETA: 41s - loss: 0.6766 - accuracy: 0.5802For batch 704, loss is    0.66.\n"," 706/1531 [============>.................] - ETA: 41s - loss: 0.6766 - accuracy: 0.5803For batch 705, loss is    0.66.\n"," 707/1531 [============>.................] - ETA: 41s - loss: 0.6765 - accuracy: 0.5803For batch 706, loss is    0.66.\n"," 708/1531 [============>.................] - ETA: 41s - loss: 0.6765 - accuracy: 0.5803For batch 707, loss is    0.66.\n"," 709/1531 [============>.................] - ETA: 41s - loss: 0.6765 - accuracy: 0.5804For batch 708, loss is    0.66.\n"," 710/1531 [============>.................] - ETA: 41s - loss: 0.6765 - accuracy: 0.5804For batch 709, loss is    0.66.\n"," 711/1531 [============>.................] - ETA: 41s - loss: 0.6765 - accuracy: 0.5805For batch 710, loss is    0.66.\n"," 712/1531 [============>.................] - ETA: 41s - loss: 0.6764 - accuracy: 0.5805For batch 711, loss is    0.66.\n"," 713/1531 [============>.................] - ETA: 41s - loss: 0.6764 - accuracy: 0.5806For batch 712, loss is    0.66.\n"," 714/1531 [============>.................] - ETA: 41s - loss: 0.6764 - accuracy: 0.5806For batch 713, loss is    0.66.\n"," 715/1531 [=============>................] - ETA: 41s - loss: 0.6764 - accuracy: 0.5806For batch 714, loss is    0.66.\n"," 716/1531 [=============>................] - ETA: 41s - loss: 0.6763 - accuracy: 0.5807For batch 715, loss is    0.66.\n"," 717/1531 [=============>................] - ETA: 41s - loss: 0.6763 - accuracy: 0.5807For batch 716, loss is    0.66.\n"," 718/1531 [=============>................] - ETA: 41s - loss: 0.6763 - accuracy: 0.5808For batch 717, loss is    0.66.\n"," 719/1531 [=============>................] - ETA: 41s - loss: 0.6763 - accuracy: 0.5808For batch 718, loss is    0.66.\n"," 720/1531 [=============>................] - ETA: 41s - loss: 0.6762 - accuracy: 0.5809For batch 719, loss is    0.66.\n"," 721/1531 [=============>................] - ETA: 41s - loss: 0.6762 - accuracy: 0.5809For batch 720, loss is    0.66.\n"," 722/1531 [=============>................] - ETA: 41s - loss: 0.6762 - accuracy: 0.5809For batch 721, loss is    0.66.\n"," 723/1531 [=============>................] - ETA: 41s - loss: 0.6762 - accuracy: 0.5810For batch 722, loss is    0.66.\n"," 724/1531 [=============>................] - ETA: 41s - loss: 0.6761 - accuracy: 0.5810For batch 723, loss is    0.66.\n"," 725/1531 [=============>................] - ETA: 41s - loss: 0.6761 - accuracy: 0.5811For batch 724, loss is    0.66.\n"," 726/1531 [=============>................] - ETA: 41s - loss: 0.6761 - accuracy: 0.5811For batch 725, loss is    0.66.\n"," 727/1531 [=============>................] - ETA: 41s - loss: 0.6761 - accuracy: 0.5812For batch 726, loss is    0.66.\n"," 728/1531 [=============>................] - ETA: 40s - loss: 0.6760 - accuracy: 0.5812For batch 727, loss is    0.66.\n"," 729/1531 [=============>................] - ETA: 40s - loss: 0.6760 - accuracy: 0.5812For batch 728, loss is    0.66.\n"," 730/1531 [=============>................] - ETA: 40s - loss: 0.6760 - accuracy: 0.5813For batch 729, loss is    0.66.\n"," 731/1531 [=============>................] - ETA: 40s - loss: 0.6760 - accuracy: 0.5813For batch 730, loss is    0.66.\n"," 732/1531 [=============>................] - ETA: 40s - loss: 0.6759 - accuracy: 0.5814For batch 731, loss is    0.66.\n"," 733/1531 [=============>................] - ETA: 40s - loss: 0.6759 - accuracy: 0.5814For batch 732, loss is    0.66.\n"," 734/1531 [=============>................] - ETA: 40s - loss: 0.6759 - accuracy: 0.5814For batch 733, loss is    0.66.\n"," 735/1531 [=============>................] - ETA: 40s - loss: 0.6759 - accuracy: 0.5815For batch 734, loss is    0.66.\n"," 736/1531 [=============>................] - ETA: 40s - loss: 0.6758 - accuracy: 0.5815For batch 735, loss is    0.66.\n"," 737/1531 [=============>................] - ETA: 40s - loss: 0.6758 - accuracy: 0.5816For batch 736, loss is    0.66.\n"," 738/1531 [=============>................] - ETA: 40s - loss: 0.6758 - accuracy: 0.5816For batch 737, loss is    0.66.\n"," 739/1531 [=============>................] - ETA: 40s - loss: 0.6758 - accuracy: 0.5816For batch 738, loss is    0.66.\n"," 740/1531 [=============>................] - ETA: 40s - loss: 0.6757 - accuracy: 0.5817For batch 739, loss is    0.66.\n"," 741/1531 [=============>................] - ETA: 40s - loss: 0.6757 - accuracy: 0.5817For batch 740, loss is    0.66.\n"," 742/1531 [=============>................] - ETA: 40s - loss: 0.6757 - accuracy: 0.5818For batch 741, loss is    0.66.\n"," 743/1531 [=============>................] - ETA: 40s - loss: 0.6757 - accuracy: 0.5818For batch 742, loss is    0.66.\n"," 744/1531 [=============>................] - ETA: 40s - loss: 0.6756 - accuracy: 0.5818For batch 743, loss is    0.66.\n"," 745/1531 [=============>................] - ETA: 40s - loss: 0.6756 - accuracy: 0.5819For batch 744, loss is    0.66.\n"," 746/1531 [=============>................] - ETA: 40s - loss: 0.6756 - accuracy: 0.5819For batch 745, loss is    0.66.\n"," 747/1531 [=============>................] - ETA: 40s - loss: 0.6756 - accuracy: 0.5820For batch 746, loss is    0.66.\n"," 748/1531 [=============>................] - ETA: 40s - loss: 0.6755 - accuracy: 0.5820For batch 747, loss is    0.66.\n"," 749/1531 [=============>................] - ETA: 40s - loss: 0.6755 - accuracy: 0.5820For batch 748, loss is    0.66.\n"," 750/1531 [=============>................] - ETA: 39s - loss: 0.6755 - accuracy: 0.5821For batch 749, loss is    0.66.\n"," 751/1531 [=============>................] - ETA: 39s - loss: 0.6755 - accuracy: 0.5821For batch 750, loss is    0.66.\n"," 752/1531 [=============>................] - ETA: 39s - loss: 0.6754 - accuracy: 0.5822For batch 751, loss is    0.66.\n"," 753/1531 [=============>................] - ETA: 39s - loss: 0.6754 - accuracy: 0.5822For batch 752, loss is    0.66.\n"," 754/1531 [=============>................] - ETA: 39s - loss: 0.6754 - accuracy: 0.5822For batch 753, loss is    0.66.\n"," 755/1531 [=============>................] - ETA: 39s - loss: 0.6754 - accuracy: 0.5823For batch 754, loss is    0.66.\n"," 756/1531 [=============>................] - ETA: 39s - loss: 0.6753 - accuracy: 0.5823For batch 755, loss is    0.66.\n"," 757/1531 [=============>................] - ETA: 39s - loss: 0.6753 - accuracy: 0.5824For batch 756, loss is    0.66.\n"," 758/1531 [=============>................] - ETA: 39s - loss: 0.6753 - accuracy: 0.5824For batch 757, loss is    0.66.\n"," 759/1531 [=============>................] - ETA: 39s - loss: 0.6753 - accuracy: 0.5824For batch 758, loss is    0.66.\n"," 760/1531 [=============>................] - ETA: 39s - loss: 0.6752 - accuracy: 0.5825For batch 759, loss is    0.66.\n"," 761/1531 [=============>................] - ETA: 39s - loss: 0.6752 - accuracy: 0.5825For batch 760, loss is    0.66.\n"," 762/1531 [=============>................] - ETA: 39s - loss: 0.6752 - accuracy: 0.5826For batch 761, loss is    0.66.\n"," 763/1531 [=============>................] - ETA: 39s - loss: 0.6752 - accuracy: 0.5826For batch 762, loss is    0.66.\n"," 764/1531 [=============>................] - ETA: 39s - loss: 0.6752 - accuracy: 0.5826For batch 763, loss is    0.66.\n"," 765/1531 [=============>................] - ETA: 39s - loss: 0.6751 - accuracy: 0.5827For batch 764, loss is    0.66.\n"," 766/1531 [==============>...............] - ETA: 39s - loss: 0.6751 - accuracy: 0.5827For batch 765, loss is    0.66.\n"," 767/1531 [==============>...............] - ETA: 39s - loss: 0.6751 - accuracy: 0.5828For batch 766, loss is    0.66.\n"," 768/1531 [==============>...............] - ETA: 39s - loss: 0.6751 - accuracy: 0.5828For batch 767, loss is    0.66.\n"," 769/1531 [==============>...............] - ETA: 39s - loss: 0.6750 - accuracy: 0.5828For batch 768, loss is    0.66.\n"," 770/1531 [==============>...............] - ETA: 39s - loss: 0.6750 - accuracy: 0.5829For batch 769, loss is    0.66.\n"," 771/1531 [==============>...............] - ETA: 39s - loss: 0.6750 - accuracy: 0.5829For batch 770, loss is    0.66.\n"," 772/1531 [==============>...............] - ETA: 38s - loss: 0.6750 - accuracy: 0.5829For batch 771, loss is    0.66.\n"," 773/1531 [==============>...............] - ETA: 38s - loss: 0.6749 - accuracy: 0.5830For batch 772, loss is    0.66.\n"," 774/1531 [==============>...............] - ETA: 38s - loss: 0.6749 - accuracy: 0.5830For batch 773, loss is    0.66.\n"," 775/1531 [==============>...............] - ETA: 38s - loss: 0.6749 - accuracy: 0.5831For batch 774, loss is    0.66.\n"," 776/1531 [==============>...............] - ETA: 38s - loss: 0.6749 - accuracy: 0.5831For batch 775, loss is    0.66.\n"," 777/1531 [==============>...............] - ETA: 38s - loss: 0.6749 - accuracy: 0.5831For batch 776, loss is    0.66.\n"," 778/1531 [==============>...............] - ETA: 38s - loss: 0.6748 - accuracy: 0.5832For batch 777, loss is    0.66.\n"," 779/1531 [==============>...............] - ETA: 38s - loss: 0.6748 - accuracy: 0.5832For batch 778, loss is    0.66.\n"," 780/1531 [==============>...............] - ETA: 38s - loss: 0.6748 - accuracy: 0.5833For batch 779, loss is    0.66.\n"," 781/1531 [==============>...............] - ETA: 38s - loss: 0.6748 - accuracy: 0.5833For batch 780, loss is    0.66.\n"," 782/1531 [==============>...............] - ETA: 38s - loss: 0.6747 - accuracy: 0.5833For batch 781, loss is    0.66.\n"," 783/1531 [==============>...............] - ETA: 38s - loss: 0.6747 - accuracy: 0.5834For batch 782, loss is    0.66.\n"," 784/1531 [==============>...............] - ETA: 38s - loss: 0.6747 - accuracy: 0.5834For batch 783, loss is    0.66.\n"," 785/1531 [==============>...............] - ETA: 38s - loss: 0.6747 - accuracy: 0.5834For batch 784, loss is    0.66.\n"," 786/1531 [==============>...............] - ETA: 38s - loss: 0.6747 - accuracy: 0.5835For batch 785, loss is    0.66.\n"," 787/1531 [==============>...............] - ETA: 38s - loss: 0.6746 - accuracy: 0.5835For batch 786, loss is    0.66.\n"," 788/1531 [==============>...............] - ETA: 38s - loss: 0.6746 - accuracy: 0.5835For batch 787, loss is    0.66.\n"," 789/1531 [==============>...............] - ETA: 38s - loss: 0.6746 - accuracy: 0.5836For batch 788, loss is    0.66.\n"," 790/1531 [==============>...............] - ETA: 38s - loss: 0.6746 - accuracy: 0.5836For batch 789, loss is    0.66.\n"," 791/1531 [==============>...............] - ETA: 38s - loss: 0.6745 - accuracy: 0.5837For batch 790, loss is    0.66.\n"," 792/1531 [==============>...............] - ETA: 38s - loss: 0.6745 - accuracy: 0.5837For batch 791, loss is    0.66.\n"," 793/1531 [==============>...............] - ETA: 38s - loss: 0.6745 - accuracy: 0.5837For batch 792, loss is    0.66.\n"," 794/1531 [==============>...............] - ETA: 37s - loss: 0.6745 - accuracy: 0.5838For batch 793, loss is    0.66.\n"," 795/1531 [==============>...............] - ETA: 37s - loss: 0.6745 - accuracy: 0.5838For batch 794, loss is    0.66.\n"," 796/1531 [==============>...............] - ETA: 37s - loss: 0.6744 - accuracy: 0.5838For batch 795, loss is    0.66.\n"," 797/1531 [==============>...............] - ETA: 37s - loss: 0.6744 - accuracy: 0.5839For batch 796, loss is    0.66.\n"," 798/1531 [==============>...............] - ETA: 37s - loss: 0.6744 - accuracy: 0.5839For batch 797, loss is    0.66.\n"," 799/1531 [==============>...............] - ETA: 37s - loss: 0.6744 - accuracy: 0.5839For batch 798, loss is    0.66.\n"," 800/1531 [==============>...............] - ETA: 37s - loss: 0.6743 - accuracy: 0.5840For batch 799, loss is    0.66.\n"," 801/1531 [==============>...............] - ETA: 37s - loss: 0.6743 - accuracy: 0.5840For batch 800, loss is    0.66.\n"," 802/1531 [==============>...............] - ETA: 37s - loss: 0.6743 - accuracy: 0.5841For batch 801, loss is    0.66.\n"," 803/1531 [==============>...............] - ETA: 37s - loss: 0.6743 - accuracy: 0.5841For batch 802, loss is    0.66.\n"," 804/1531 [==============>...............] - ETA: 37s - loss: 0.6743 - accuracy: 0.5841For batch 803, loss is    0.66.\n"," 805/1531 [==============>...............] - ETA: 37s - loss: 0.6742 - accuracy: 0.5842For batch 804, loss is    0.66.\n"," 806/1531 [==============>...............] - ETA: 37s - loss: 0.6742 - accuracy: 0.5842For batch 805, loss is    0.66.\n"," 807/1531 [==============>...............] - ETA: 37s - loss: 0.6742 - accuracy: 0.5842For batch 806, loss is    0.66.\n"," 808/1531 [==============>...............] - ETA: 37s - loss: 0.6742 - accuracy: 0.5843For batch 807, loss is    0.66.\n"," 809/1531 [==============>...............] - ETA: 37s - loss: 0.6742 - accuracy: 0.5843For batch 808, loss is    0.66.\n"," 810/1531 [==============>...............] - ETA: 37s - loss: 0.6741 - accuracy: 0.5843For batch 809, loss is    0.66.\n"," 811/1531 [==============>...............] - ETA: 37s - loss: 0.6741 - accuracy: 0.5844For batch 810, loss is    0.66.\n"," 812/1531 [==============>...............] - ETA: 37s - loss: 0.6741 - accuracy: 0.5844For batch 811, loss is    0.66.\n"," 813/1531 [==============>...............] - ETA: 37s - loss: 0.6741 - accuracy: 0.5844For batch 812, loss is    0.66.\n"," 814/1531 [==============>...............] - ETA: 37s - loss: 0.6740 - accuracy: 0.5845For batch 813, loss is    0.66.\n"," 815/1531 [==============>...............] - ETA: 37s - loss: 0.6740 - accuracy: 0.5845For batch 814, loss is    0.66.\n"," 816/1531 [==============>...............] - ETA: 36s - loss: 0.6740 - accuracy: 0.5845For batch 815, loss is    0.66.\n"," 817/1531 [===============>..............] - ETA: 36s - loss: 0.6740 - accuracy: 0.5846For batch 816, loss is    0.66.\n"," 818/1531 [===============>..............] - ETA: 36s - loss: 0.6740 - accuracy: 0.5846For batch 817, loss is    0.66.\n"," 819/1531 [===============>..............] - ETA: 36s - loss: 0.6739 - accuracy: 0.5847For batch 818, loss is    0.66.\n"," 820/1531 [===============>..............] - ETA: 36s - loss: 0.6739 - accuracy: 0.5847For batch 819, loss is    0.66.\n"," 821/1531 [===============>..............] - ETA: 36s - loss: 0.6739 - accuracy: 0.5847For batch 820, loss is    0.66.\n"," 822/1531 [===============>..............] - ETA: 36s - loss: 0.6739 - accuracy: 0.5848For batch 821, loss is    0.66.\n"," 823/1531 [===============>..............] - ETA: 36s - loss: 0.6739 - accuracy: 0.5848For batch 822, loss is    0.66.\n"," 824/1531 [===============>..............] - ETA: 36s - loss: 0.6738 - accuracy: 0.5848For batch 823, loss is    0.66.\n"," 825/1531 [===============>..............] - ETA: 36s - loss: 0.6738 - accuracy: 0.5849For batch 824, loss is    0.66.\n"," 826/1531 [===============>..............] - ETA: 36s - loss: 0.6738 - accuracy: 0.5849For batch 825, loss is    0.66.\n"," 827/1531 [===============>..............] - ETA: 36s - loss: 0.6738 - accuracy: 0.5849For batch 826, loss is    0.66.\n"," 828/1531 [===============>..............] - ETA: 36s - loss: 0.6737 - accuracy: 0.5850For batch 827, loss is    0.66.\n"," 829/1531 [===============>..............] - ETA: 36s - loss: 0.6737 - accuracy: 0.5850For batch 828, loss is    0.66.\n"," 830/1531 [===============>..............] - ETA: 36s - loss: 0.6737 - accuracy: 0.5850For batch 829, loss is    0.66.\n"," 831/1531 [===============>..............] - ETA: 36s - loss: 0.6737 - accuracy: 0.5851For batch 830, loss is    0.66.\n"," 832/1531 [===============>..............] - ETA: 36s - loss: 0.6737 - accuracy: 0.5851For batch 831, loss is    0.66.\n"," 833/1531 [===============>..............] - ETA: 36s - loss: 0.6736 - accuracy: 0.5851For batch 832, loss is    0.66.\n"," 834/1531 [===============>..............] - ETA: 36s - loss: 0.6736 - accuracy: 0.5852For batch 833, loss is    0.66.\n"," 835/1531 [===============>..............] - ETA: 36s - loss: 0.6736 - accuracy: 0.5852For batch 834, loss is    0.66.\n"," 836/1531 [===============>..............] - ETA: 36s - loss: 0.6736 - accuracy: 0.5852For batch 835, loss is    0.66.\n"," 837/1531 [===============>..............] - ETA: 36s - loss: 0.6736 - accuracy: 0.5853For batch 836, loss is    0.66.\n"," 838/1531 [===============>..............] - ETA: 35s - loss: 0.6735 - accuracy: 0.5853For batch 837, loss is    0.66.\n"," 839/1531 [===============>..............] - ETA: 35s - loss: 0.6735 - accuracy: 0.5853For batch 838, loss is    0.66.\n"," 840/1531 [===============>..............] - ETA: 35s - loss: 0.6735 - accuracy: 0.5854For batch 839, loss is    0.66.\n"," 841/1531 [===============>..............] - ETA: 35s - loss: 0.6735 - accuracy: 0.5854For batch 840, loss is    0.66.\n"," 842/1531 [===============>..............] - ETA: 35s - loss: 0.6735 - accuracy: 0.5854For batch 841, loss is    0.66.\n"," 843/1531 [===============>..............] - ETA: 35s - loss: 0.6734 - accuracy: 0.5855For batch 842, loss is    0.66.\n"," 844/1531 [===============>..............] - ETA: 35s - loss: 0.6734 - accuracy: 0.5855For batch 843, loss is    0.66.\n"," 845/1531 [===============>..............] - ETA: 35s - loss: 0.6734 - accuracy: 0.5855For batch 844, loss is    0.66.\n"," 846/1531 [===============>..............] - ETA: 35s - loss: 0.6734 - accuracy: 0.5856For batch 845, loss is    0.66.\n"," 847/1531 [===============>..............] - ETA: 35s - loss: 0.6734 - accuracy: 0.5856For batch 846, loss is    0.66.\n"," 848/1531 [===============>..............] - ETA: 35s - loss: 0.6733 - accuracy: 0.5856For batch 847, loss is    0.66.\n"," 849/1531 [===============>..............] - ETA: 35s - loss: 0.6733 - accuracy: 0.5857For batch 848, loss is    0.66.\n"," 850/1531 [===============>..............] - ETA: 35s - loss: 0.6733 - accuracy: 0.5857For batch 849, loss is    0.66.\n"," 851/1531 [===============>..............] - ETA: 35s - loss: 0.6733 - accuracy: 0.5857For batch 850, loss is    0.66.\n"," 852/1531 [===============>..............] - ETA: 35s - loss: 0.6733 - accuracy: 0.5858For batch 851, loss is    0.66.\n"," 853/1531 [===============>..............] - ETA: 35s - loss: 0.6732 - accuracy: 0.5858For batch 852, loss is    0.66.\n"," 854/1531 [===============>..............] - ETA: 35s - loss: 0.6732 - accuracy: 0.5858For batch 853, loss is    0.66.\n"," 855/1531 [===============>..............] - ETA: 35s - loss: 0.6732 - accuracy: 0.5859For batch 854, loss is    0.66.\n"," 856/1531 [===============>..............] - ETA: 35s - loss: 0.6732 - accuracy: 0.5859For batch 855, loss is    0.66.\n"," 857/1531 [===============>..............] - ETA: 35s - loss: 0.6732 - accuracy: 0.5859For batch 856, loss is    0.66.\n"," 858/1531 [===============>..............] - ETA: 35s - loss: 0.6731 - accuracy: 0.5860For batch 857, loss is    0.66.\n"," 859/1531 [===============>..............] - ETA: 34s - loss: 0.6731 - accuracy: 0.5860For batch 858, loss is    0.66.\n"," 860/1531 [===============>..............] - ETA: 34s - loss: 0.6731 - accuracy: 0.5860For batch 859, loss is    0.66.\n"," 861/1531 [===============>..............] - ETA: 34s - loss: 0.6731 - accuracy: 0.5861For batch 860, loss is    0.66.\n"," 862/1531 [===============>..............] - ETA: 34s - loss: 0.6730 - accuracy: 0.5861For batch 861, loss is    0.66.\n"," 863/1531 [===============>..............] - ETA: 34s - loss: 0.6730 - accuracy: 0.5861For batch 862, loss is    0.66.\n"," 864/1531 [===============>..............] - ETA: 34s - loss: 0.6730 - accuracy: 0.5862For batch 863, loss is    0.66.\n"," 865/1531 [===============>..............] - ETA: 34s - loss: 0.6730 - accuracy: 0.5862For batch 864, loss is    0.66.\n"," 866/1531 [===============>..............] - ETA: 34s - loss: 0.6730 - accuracy: 0.5862For batch 865, loss is    0.66.\n"," 867/1531 [===============>..............] - ETA: 34s - loss: 0.6729 - accuracy: 0.5863For batch 866, loss is    0.66.\n"," 868/1531 [================>.............] - ETA: 34s - loss: 0.6729 - accuracy: 0.5863For batch 867, loss is    0.66.\n"," 869/1531 [================>.............] - ETA: 34s - loss: 0.6729 - accuracy: 0.5863For batch 868, loss is    0.66.\n"," 870/1531 [================>.............] - ETA: 34s - loss: 0.6729 - accuracy: 0.5863For batch 869, loss is    0.66.\n"," 871/1531 [================>.............] - ETA: 34s - loss: 0.6729 - accuracy: 0.5864For batch 870, loss is    0.66.\n"," 872/1531 [================>.............] - ETA: 34s - loss: 0.6728 - accuracy: 0.5864For batch 871, loss is    0.66.\n"," 873/1531 [================>.............] - ETA: 34s - loss: 0.6728 - accuracy: 0.5864For batch 872, loss is    0.65.\n"," 874/1531 [================>.............] - ETA: 34s - loss: 0.6728 - accuracy: 0.5865For batch 873, loss is    0.65.\n"," 875/1531 [================>.............] - ETA: 34s - loss: 0.6728 - accuracy: 0.5865For batch 874, loss is    0.65.\n"," 876/1531 [================>.............] - ETA: 34s - loss: 0.6728 - accuracy: 0.5865For batch 875, loss is    0.65.\n"," 877/1531 [================>.............] - ETA: 34s - loss: 0.6727 - accuracy: 0.5866For batch 876, loss is    0.65.\n"," 878/1531 [================>.............] - ETA: 34s - loss: 0.6727 - accuracy: 0.5866For batch 877, loss is    0.65.\n"," 879/1531 [================>.............] - ETA: 34s - loss: 0.6727 - accuracy: 0.5866For batch 878, loss is    0.65.\n"," 880/1531 [================>.............] - ETA: 33s - loss: 0.6727 - accuracy: 0.5867For batch 879, loss is    0.65.\n"," 881/1531 [================>.............] - ETA: 33s - loss: 0.6727 - accuracy: 0.5867For batch 880, loss is    0.65.\n"," 882/1531 [================>.............] - ETA: 33s - loss: 0.6726 - accuracy: 0.5867For batch 881, loss is    0.65.\n"," 883/1531 [================>.............] - ETA: 33s - loss: 0.6726 - accuracy: 0.5868For batch 882, loss is    0.65.\n"," 884/1531 [================>.............] - ETA: 33s - loss: 0.6726 - accuracy: 0.5868For batch 883, loss is    0.65.\n"," 885/1531 [================>.............] - ETA: 33s - loss: 0.6726 - accuracy: 0.5868For batch 884, loss is    0.65.\n"," 886/1531 [================>.............] - ETA: 33s - loss: 0.6726 - accuracy: 0.5869For batch 885, loss is    0.65.\n"," 887/1531 [================>.............] - ETA: 33s - loss: 0.6725 - accuracy: 0.5869For batch 886, loss is    0.65.\n"," 888/1531 [================>.............] - ETA: 33s - loss: 0.6725 - accuracy: 0.5869For batch 887, loss is    0.65.\n"," 889/1531 [================>.............] - ETA: 33s - loss: 0.6725 - accuracy: 0.5870For batch 888, loss is    0.65.\n"," 890/1531 [================>.............] - ETA: 33s - loss: 0.6725 - accuracy: 0.5870For batch 889, loss is    0.65.\n"," 891/1531 [================>.............] - ETA: 33s - loss: 0.6725 - accuracy: 0.5870For batch 890, loss is    0.65.\n"," 892/1531 [================>.............] - ETA: 33s - loss: 0.6724 - accuracy: 0.5871For batch 891, loss is    0.65.\n"," 893/1531 [================>.............] - ETA: 33s - loss: 0.6724 - accuracy: 0.5871For batch 892, loss is    0.65.\n"," 894/1531 [================>.............] - ETA: 33s - loss: 0.6724 - accuracy: 0.5871For batch 893, loss is    0.65.\n"," 895/1531 [================>.............] - ETA: 33s - loss: 0.6724 - accuracy: 0.5872For batch 894, loss is    0.65.\n"," 896/1531 [================>.............] - ETA: 33s - loss: 0.6724 - accuracy: 0.5872For batch 895, loss is    0.65.\n"," 897/1531 [================>.............] - ETA: 33s - loss: 0.6723 - accuracy: 0.5872For batch 896, loss is    0.65.\n"," 898/1531 [================>.............] - ETA: 33s - loss: 0.6723 - accuracy: 0.5872For batch 897, loss is    0.65.\n"," 899/1531 [================>.............] - ETA: 33s - loss: 0.6723 - accuracy: 0.5873For batch 898, loss is    0.65.\n"," 900/1531 [================>.............] - ETA: 33s - loss: 0.6723 - accuracy: 0.5873For batch 899, loss is    0.65.\n"," 901/1531 [================>.............] - ETA: 32s - loss: 0.6723 - accuracy: 0.5873For batch 900, loss is    0.65.\n"," 902/1531 [================>.............] - ETA: 32s - loss: 0.6722 - accuracy: 0.5874For batch 901, loss is    0.65.\n"," 903/1531 [================>.............] - ETA: 32s - loss: 0.6722 - accuracy: 0.5874For batch 902, loss is    0.65.\n"," 904/1531 [================>.............] - ETA: 32s - loss: 0.6722 - accuracy: 0.5874For batch 903, loss is    0.65.\n"," 905/1531 [================>.............] - ETA: 32s - loss: 0.6722 - accuracy: 0.5875For batch 904, loss is    0.65.\n"," 906/1531 [================>.............] - ETA: 32s - loss: 0.6722 - accuracy: 0.5875For batch 905, loss is    0.65.\n"," 907/1531 [================>.............] - ETA: 32s - loss: 0.6721 - accuracy: 0.5875For batch 906, loss is    0.65.\n"," 908/1531 [================>.............] - ETA: 32s - loss: 0.6721 - accuracy: 0.5875For batch 907, loss is    0.65.\n"," 909/1531 [================>.............] - ETA: 32s - loss: 0.6721 - accuracy: 0.5876For batch 908, loss is    0.65.\n"," 910/1531 [================>.............] - ETA: 32s - loss: 0.6721 - accuracy: 0.5876For batch 909, loss is    0.65.\n"," 911/1531 [================>.............] - ETA: 32s - loss: 0.6721 - accuracy: 0.5876For batch 910, loss is    0.65.\n"," 912/1531 [================>.............] - ETA: 32s - loss: 0.6720 - accuracy: 0.5877For batch 911, loss is    0.65.\n"," 913/1531 [================>.............] - ETA: 32s - loss: 0.6720 - accuracy: 0.5877For batch 912, loss is    0.65.\n"," 914/1531 [================>.............] - ETA: 32s - loss: 0.6720 - accuracy: 0.5877For batch 913, loss is    0.65.\n"," 915/1531 [================>.............] - ETA: 32s - loss: 0.6720 - accuracy: 0.5878For batch 914, loss is    0.65.\n"," 916/1531 [================>.............] - ETA: 32s - loss: 0.6720 - accuracy: 0.5878For batch 915, loss is    0.65.\n"," 917/1531 [================>.............] - ETA: 32s - loss: 0.6719 - accuracy: 0.5878For batch 916, loss is    0.65.\n"," 918/1531 [================>.............] - ETA: 32s - loss: 0.6719 - accuracy: 0.5878For batch 917, loss is    0.65.\n"," 919/1531 [=================>............] - ETA: 32s - loss: 0.6719 - accuracy: 0.5879For batch 918, loss is    0.65.\n"," 920/1531 [=================>............] - ETA: 32s - loss: 0.6719 - accuracy: 0.5879For batch 919, loss is    0.65.\n"," 921/1531 [=================>............] - ETA: 31s - loss: 0.6719 - accuracy: 0.5879For batch 920, loss is    0.65.\n"," 922/1531 [=================>............] - ETA: 31s - loss: 0.6718 - accuracy: 0.5880For batch 921, loss is    0.65.\n"," 923/1531 [=================>............] - ETA: 31s - loss: 0.6718 - accuracy: 0.5880For batch 922, loss is    0.65.\n"," 924/1531 [=================>............] - ETA: 31s - loss: 0.6718 - accuracy: 0.5880For batch 923, loss is    0.65.\n"," 925/1531 [=================>............] - ETA: 31s - loss: 0.6718 - accuracy: 0.5881For batch 924, loss is    0.65.\n"," 926/1531 [=================>............] - ETA: 31s - loss: 0.6718 - accuracy: 0.5881For batch 925, loss is    0.65.\n"," 927/1531 [=================>............] - ETA: 31s - loss: 0.6717 - accuracy: 0.5881For batch 926, loss is    0.65.\n"," 928/1531 [=================>............] - ETA: 31s - loss: 0.6717 - accuracy: 0.5881For batch 927, loss is    0.65.\n"," 929/1531 [=================>............] - ETA: 31s - loss: 0.6717 - accuracy: 0.5882For batch 928, loss is    0.65.\n"," 930/1531 [=================>............] - ETA: 31s - loss: 0.6717 - accuracy: 0.5882For batch 929, loss is    0.65.\n"," 931/1531 [=================>............] - ETA: 31s - loss: 0.6717 - accuracy: 0.5882For batch 930, loss is    0.65.\n"," 932/1531 [=================>............] - ETA: 31s - loss: 0.6716 - accuracy: 0.5883For batch 931, loss is    0.65.\n"," 933/1531 [=================>............] - ETA: 31s - loss: 0.6716 - accuracy: 0.5883For batch 932, loss is    0.65.\n"," 934/1531 [=================>............] - ETA: 31s - loss: 0.6716 - accuracy: 0.5883For batch 933, loss is    0.65.\n"," 935/1531 [=================>............] - ETA: 31s - loss: 0.6716 - accuracy: 0.5884For batch 934, loss is    0.65.\n"," 936/1531 [=================>............] - ETA: 31s - loss: 0.6716 - accuracy: 0.5884For batch 935, loss is    0.65.\n"," 937/1531 [=================>............] - ETA: 31s - loss: 0.6715 - accuracy: 0.5884For batch 936, loss is    0.65.\n"," 938/1531 [=================>............] - ETA: 31s - loss: 0.6715 - accuracy: 0.5884For batch 937, loss is    0.65.\n"," 939/1531 [=================>............] - ETA: 31s - loss: 0.6715 - accuracy: 0.5885For batch 938, loss is    0.65.\n"," 940/1531 [=================>............] - ETA: 31s - loss: 0.6715 - accuracy: 0.5885For batch 939, loss is    0.65.\n"," 941/1531 [=================>............] - ETA: 31s - loss: 0.6715 - accuracy: 0.5885For batch 940, loss is    0.65.\n"," 942/1531 [=================>............] - ETA: 30s - loss: 0.6714 - accuracy: 0.5886For batch 941, loss is    0.65.\n"," 943/1531 [=================>............] - ETA: 30s - loss: 0.6714 - accuracy: 0.5886For batch 942, loss is    0.65.\n"," 944/1531 [=================>............] - ETA: 30s - loss: 0.6714 - accuracy: 0.5886For batch 943, loss is    0.65.\n"," 945/1531 [=================>............] - ETA: 30s - loss: 0.6714 - accuracy: 0.5886For batch 944, loss is    0.65.\n"," 946/1531 [=================>............] - ETA: 30s - loss: 0.6714 - accuracy: 0.5887For batch 945, loss is    0.65.\n"," 947/1531 [=================>............] - ETA: 30s - loss: 0.6713 - accuracy: 0.5887For batch 946, loss is    0.65.\n"," 948/1531 [=================>............] - ETA: 30s - loss: 0.6713 - accuracy: 0.5887For batch 947, loss is    0.65.\n"," 949/1531 [=================>............] - ETA: 30s - loss: 0.6713 - accuracy: 0.5888For batch 948, loss is    0.65.\n"," 950/1531 [=================>............] - ETA: 30s - loss: 0.6713 - accuracy: 0.5888For batch 949, loss is    0.65.\n"," 951/1531 [=================>............] - ETA: 30s - loss: 0.6713 - accuracy: 0.5888For batch 950, loss is    0.65.\n"," 952/1531 [=================>............] - ETA: 30s - loss: 0.6712 - accuracy: 0.5888For batch 951, loss is    0.65.\n"," 953/1531 [=================>............] - ETA: 30s - loss: 0.6712 - accuracy: 0.5889For batch 952, loss is    0.65.\n"," 954/1531 [=================>............] - ETA: 30s - loss: 0.6712 - accuracy: 0.5889For batch 953, loss is    0.65.\n"," 955/1531 [=================>............] - ETA: 30s - loss: 0.6712 - accuracy: 0.5889For batch 954, loss is    0.65.\n"," 956/1531 [=================>............] - ETA: 30s - loss: 0.6712 - accuracy: 0.5890For batch 955, loss is    0.65.\n"," 957/1531 [=================>............] - ETA: 30s - loss: 0.6712 - accuracy: 0.5890For batch 956, loss is    0.65.\n"," 958/1531 [=================>............] - ETA: 30s - loss: 0.6711 - accuracy: 0.5890For batch 957, loss is    0.65.\n"," 959/1531 [=================>............] - ETA: 30s - loss: 0.6711 - accuracy: 0.5890For batch 958, loss is    0.65.\n"," 960/1531 [=================>............] - ETA: 30s - loss: 0.6711 - accuracy: 0.5891For batch 959, loss is    0.65.\n"," 961/1531 [=================>............] - ETA: 30s - loss: 0.6711 - accuracy: 0.5891For batch 960, loss is    0.65.\n"," 962/1531 [=================>............] - ETA: 29s - loss: 0.6711 - accuracy: 0.5891For batch 961, loss is    0.65.\n"," 963/1531 [=================>............] - ETA: 29s - loss: 0.6710 - accuracy: 0.5892For batch 962, loss is    0.65.\n"," 964/1531 [=================>............] - ETA: 29s - loss: 0.6710 - accuracy: 0.5892For batch 963, loss is    0.65.\n"," 965/1531 [=================>............] - ETA: 29s - loss: 0.6710 - accuracy: 0.5892For batch 964, loss is    0.65.\n"," 966/1531 [=================>............] - ETA: 29s - loss: 0.6710 - accuracy: 0.5892For batch 965, loss is    0.65.\n"," 967/1531 [=================>............] - ETA: 29s - loss: 0.6710 - accuracy: 0.5893For batch 966, loss is    0.65.\n"," 968/1531 [=================>............] - ETA: 29s - loss: 0.6709 - accuracy: 0.5893For batch 967, loss is    0.65.\n"," 969/1531 [=================>............] - ETA: 29s - loss: 0.6709 - accuracy: 0.5893For batch 968, loss is    0.65.\n"," 970/1531 [==================>...........] - ETA: 29s - loss: 0.6709 - accuracy: 0.5894For batch 969, loss is    0.65.\n"," 971/1531 [==================>...........] - ETA: 29s - loss: 0.6709 - accuracy: 0.5894For batch 970, loss is    0.65.\n"," 972/1531 [==================>...........] - ETA: 29s - loss: 0.6709 - accuracy: 0.5894For batch 971, loss is    0.65.\n"," 973/1531 [==================>...........] - ETA: 29s - loss: 0.6708 - accuracy: 0.5894For batch 972, loss is    0.65.\n"," 974/1531 [==================>...........] - ETA: 29s - loss: 0.6708 - accuracy: 0.5895For batch 973, loss is    0.65.\n"," 975/1531 [==================>...........] - ETA: 29s - loss: 0.6708 - accuracy: 0.5895For batch 974, loss is    0.65.\n"," 976/1531 [==================>...........] - ETA: 29s - loss: 0.6708 - accuracy: 0.5895For batch 975, loss is    0.65.\n"," 977/1531 [==================>...........] - ETA: 29s - loss: 0.6708 - accuracy: 0.5896For batch 976, loss is    0.65.\n"," 978/1531 [==================>...........] - ETA: 29s - loss: 0.6707 - accuracy: 0.5896For batch 977, loss is    0.65.\n"," 979/1531 [==================>...........] - ETA: 29s - loss: 0.6707 - accuracy: 0.5896For batch 978, loss is    0.65.\n"," 980/1531 [==================>...........] - ETA: 29s - loss: 0.6707 - accuracy: 0.5896For batch 979, loss is    0.65.\n"," 981/1531 [==================>...........] - ETA: 29s - loss: 0.6707 - accuracy: 0.5897For batch 980, loss is    0.65.\n"," 982/1531 [==================>...........] - ETA: 29s - loss: 0.6707 - accuracy: 0.5897For batch 981, loss is    0.65.\n"," 983/1531 [==================>...........] - ETA: 28s - loss: 0.6706 - accuracy: 0.5897For batch 982, loss is    0.65.\n"," 984/1531 [==================>...........] - ETA: 28s - loss: 0.6706 - accuracy: 0.5898For batch 983, loss is    0.65.\n"," 985/1531 [==================>...........] - ETA: 28s - loss: 0.6706 - accuracy: 0.5898For batch 984, loss is    0.65.\n"," 986/1531 [==================>...........] - ETA: 28s - loss: 0.6706 - accuracy: 0.5898For batch 985, loss is    0.65.\n"," 987/1531 [==================>...........] - ETA: 28s - loss: 0.6706 - accuracy: 0.5898For batch 986, loss is    0.65.\n"," 988/1531 [==================>...........] - ETA: 28s - loss: 0.6705 - accuracy: 0.5899For batch 987, loss is    0.65.\n"," 989/1531 [==================>...........] - ETA: 28s - loss: 0.6705 - accuracy: 0.5899For batch 988, loss is    0.65.\n"," 990/1531 [==================>...........] - ETA: 28s - loss: 0.6705 - accuracy: 0.5899For batch 989, loss is    0.65.\n"," 991/1531 [==================>...........] - ETA: 28s - loss: 0.6705 - accuracy: 0.5900For batch 990, loss is    0.65.\n"," 992/1531 [==================>...........] - ETA: 28s - loss: 0.6705 - accuracy: 0.5900For batch 991, loss is    0.65.\n"," 993/1531 [==================>...........] - ETA: 28s - loss: 0.6705 - accuracy: 0.5900For batch 992, loss is    0.65.\n"," 994/1531 [==================>...........] - ETA: 28s - loss: 0.6704 - accuracy: 0.5900For batch 993, loss is    0.65.\n"," 995/1531 [==================>...........] - ETA: 28s - loss: 0.6704 - accuracy: 0.5901For batch 994, loss is    0.65.\n"," 996/1531 [==================>...........] - ETA: 28s - loss: 0.6704 - accuracy: 0.5901For batch 995, loss is    0.65.\n"," 997/1531 [==================>...........] - ETA: 28s - loss: 0.6704 - accuracy: 0.5901For batch 996, loss is    0.65.\n"," 998/1531 [==================>...........] - ETA: 28s - loss: 0.6704 - accuracy: 0.5901For batch 997, loss is    0.65.\n"," 999/1531 [==================>...........] - ETA: 28s - loss: 0.6703 - accuracy: 0.5902For batch 998, loss is    0.65.\n","1000/1531 [==================>...........] - ETA: 28s - loss: 0.6703 - accuracy: 0.5902For batch 999, loss is    0.65.\n","1001/1531 [==================>...........] - ETA: 28s - loss: 0.6703 - accuracy: 0.5902For batch 1000, loss is    0.65.\n","1002/1531 [==================>...........] - ETA: 28s - loss: 0.6703 - accuracy: 0.5903For batch 1001, loss is    0.65.\n","1003/1531 [==================>...........] - ETA: 27s - loss: 0.6703 - accuracy: 0.5903For batch 1002, loss is    0.65.\n","1004/1531 [==================>...........] - ETA: 27s - loss: 0.6702 - accuracy: 0.5903For batch 1003, loss is    0.65.\n","1005/1531 [==================>...........] - ETA: 27s - loss: 0.6702 - accuracy: 0.5903For batch 1004, loss is    0.65.\n","1006/1531 [==================>...........] - ETA: 27s - loss: 0.6702 - accuracy: 0.5904For batch 1005, loss is    0.65.\n","1007/1531 [==================>...........] - ETA: 27s - loss: 0.6702 - accuracy: 0.5904For batch 1006, loss is    0.65.\n","1008/1531 [==================>...........] - ETA: 27s - loss: 0.6702 - accuracy: 0.5904For batch 1007, loss is    0.65.\n","1009/1531 [==================>...........] - ETA: 27s - loss: 0.6702 - accuracy: 0.5904For batch 1008, loss is    0.65.\n","1010/1531 [==================>...........] - ETA: 27s - loss: 0.6701 - accuracy: 0.5905For batch 1009, loss is    0.65.\n","1011/1531 [==================>...........] - ETA: 27s - loss: 0.6701 - accuracy: 0.5905For batch 1010, loss is    0.65.\n","1012/1531 [==================>...........] - ETA: 27s - loss: 0.6701 - accuracy: 0.5905For batch 1011, loss is    0.65.\n","1013/1531 [==================>...........] - ETA: 27s - loss: 0.6701 - accuracy: 0.5906For batch 1012, loss is    0.65.\n","1014/1531 [==================>...........] - ETA: 27s - loss: 0.6701 - accuracy: 0.5906For batch 1013, loss is    0.65.\n","1015/1531 [==================>...........] - ETA: 27s - loss: 0.6700 - accuracy: 0.5906For batch 1014, loss is    0.65.\n","1016/1531 [==================>...........] - ETA: 27s - loss: 0.6700 - accuracy: 0.5906For batch 1015, loss is    0.65.\n","1017/1531 [==================>...........] - ETA: 27s - loss: 0.6700 - accuracy: 0.5907For batch 1016, loss is    0.65.\n","1018/1531 [==================>...........] - ETA: 27s - loss: 0.6700 - accuracy: 0.5907For batch 1017, loss is    0.65.\n","1019/1531 [==================>...........] - ETA: 27s - loss: 0.6700 - accuracy: 0.5907For batch 1018, loss is    0.65.\n","1020/1531 [==================>...........] - ETA: 27s - loss: 0.6699 - accuracy: 0.5907For batch 1019, loss is    0.65.\n","1021/1531 [===================>..........] - ETA: 27s - loss: 0.6699 - accuracy: 0.5908For batch 1020, loss is    0.65.\n","1022/1531 [===================>..........] - ETA: 27s - loss: 0.6699 - accuracy: 0.5908For batch 1021, loss is    0.65.\n","1023/1531 [===================>..........] - ETA: 26s - loss: 0.6699 - accuracy: 0.5908For batch 1022, loss is    0.65.\n","1024/1531 [===================>..........] - ETA: 26s - loss: 0.6699 - accuracy: 0.5908For batch 1023, loss is    0.65.\n","1025/1531 [===================>..........] - ETA: 26s - loss: 0.6699 - accuracy: 0.5909For batch 1024, loss is    0.65.\n","1026/1531 [===================>..........] - ETA: 26s - loss: 0.6698 - accuracy: 0.5909For batch 1025, loss is    0.65.\n","1027/1531 [===================>..........] - ETA: 26s - loss: 0.6698 - accuracy: 0.5909For batch 1026, loss is    0.65.\n","1028/1531 [===================>..........] - ETA: 26s - loss: 0.6698 - accuracy: 0.5910For batch 1027, loss is    0.65.\n","1029/1531 [===================>..........] - ETA: 26s - loss: 0.6698 - accuracy: 0.5910For batch 1028, loss is    0.65.\n","1030/1531 [===================>..........] - ETA: 26s - loss: 0.6698 - accuracy: 0.5910For batch 1029, loss is    0.65.\n","1031/1531 [===================>..........] - ETA: 26s - loss: 0.6697 - accuracy: 0.5910For batch 1030, loss is    0.65.\n","1032/1531 [===================>..........] - ETA: 26s - loss: 0.6697 - accuracy: 0.5911For batch 1031, loss is    0.65.\n","1033/1531 [===================>..........] - ETA: 26s - loss: 0.6697 - accuracy: 0.5911For batch 1032, loss is    0.65.\n","1034/1531 [===================>..........] - ETA: 26s - loss: 0.6697 - accuracy: 0.5911For batch 1033, loss is    0.65.\n","1035/1531 [===================>..........] - ETA: 26s - loss: 0.6697 - accuracy: 0.5911For batch 1034, loss is    0.65.\n","1036/1531 [===================>..........] - ETA: 26s - loss: 0.6697 - accuracy: 0.5912For batch 1035, loss is    0.65.\n","1037/1531 [===================>..........] - ETA: 26s - loss: 0.6696 - accuracy: 0.5912For batch 1036, loss is    0.65.\n","1038/1531 [===================>..........] - ETA: 26s - loss: 0.6696 - accuracy: 0.5912For batch 1037, loss is    0.65.\n","1039/1531 [===================>..........] - ETA: 26s - loss: 0.6696 - accuracy: 0.5912For batch 1038, loss is    0.65.\n","1040/1531 [===================>..........] - ETA: 26s - loss: 0.6696 - accuracy: 0.5913For batch 1039, loss is    0.65.\n","1041/1531 [===================>..........] - ETA: 26s - loss: 0.6696 - accuracy: 0.5913For batch 1040, loss is    0.65.\n","1042/1531 [===================>..........] - ETA: 26s - loss: 0.6695 - accuracy: 0.5913For batch 1041, loss is    0.65.\n","1043/1531 [===================>..........] - ETA: 25s - loss: 0.6695 - accuracy: 0.5913For batch 1042, loss is    0.65.\n","1044/1531 [===================>..........] - ETA: 25s - loss: 0.6695 - accuracy: 0.5914For batch 1043, loss is    0.65.\n","1045/1531 [===================>..........] - ETA: 25s - loss: 0.6695 - accuracy: 0.5914For batch 1044, loss is    0.65.\n","1046/1531 [===================>..........] - ETA: 25s - loss: 0.6695 - accuracy: 0.5914For batch 1045, loss is    0.65.\n","1047/1531 [===================>..........] - ETA: 25s - loss: 0.6695 - accuracy: 0.5914For batch 1046, loss is    0.65.\n","1048/1531 [===================>..........] - ETA: 25s - loss: 0.6694 - accuracy: 0.5915For batch 1047, loss is    0.65.\n","1049/1531 [===================>..........] - ETA: 25s - loss: 0.6694 - accuracy: 0.5915For batch 1048, loss is    0.65.\n","1050/1531 [===================>..........] - ETA: 25s - loss: 0.6694 - accuracy: 0.5915For batch 1049, loss is    0.65.\n","1051/1531 [===================>..........] - ETA: 25s - loss: 0.6694 - accuracy: 0.5916For batch 1050, loss is    0.65.\n","1052/1531 [===================>..........] - ETA: 25s - loss: 0.6694 - accuracy: 0.5916For batch 1051, loss is    0.65.\n","1053/1531 [===================>..........] - ETA: 25s - loss: 0.6693 - accuracy: 0.5916For batch 1052, loss is    0.65.\n","1054/1531 [===================>..........] - ETA: 25s - loss: 0.6693 - accuracy: 0.5916For batch 1053, loss is    0.65.\n","1055/1531 [===================>..........] - ETA: 25s - loss: 0.6693 - accuracy: 0.5917For batch 1054, loss is    0.65.\n","1056/1531 [===================>..........] - ETA: 25s - loss: 0.6693 - accuracy: 0.5917For batch 1055, loss is    0.65.\n","1057/1531 [===================>..........] - ETA: 25s - loss: 0.6693 - accuracy: 0.5917For batch 1056, loss is    0.65.\n","1058/1531 [===================>..........] - ETA: 25s - loss: 0.6693 - accuracy: 0.5917For batch 1057, loss is    0.65.\n","1059/1531 [===================>..........] - ETA: 25s - loss: 0.6692 - accuracy: 0.5918For batch 1058, loss is    0.65.\n","1060/1531 [===================>..........] - ETA: 25s - loss: 0.6692 - accuracy: 0.5918For batch 1059, loss is    0.65.\n","1061/1531 [===================>..........] - ETA: 25s - loss: 0.6692 - accuracy: 0.5918For batch 1060, loss is    0.65.\n","1062/1531 [===================>..........] - ETA: 25s - loss: 0.6692 - accuracy: 0.5918For batch 1061, loss is    0.65.\n","1063/1531 [===================>..........] - ETA: 24s - loss: 0.6692 - accuracy: 0.5919For batch 1062, loss is    0.65.\n","1064/1531 [===================>..........] - ETA: 24s - loss: 0.6691 - accuracy: 0.5919For batch 1063, loss is    0.65.\n","1065/1531 [===================>..........] - ETA: 24s - loss: 0.6691 - accuracy: 0.5919For batch 1064, loss is    0.65.\n","1066/1531 [===================>..........] - ETA: 24s - loss: 0.6691 - accuracy: 0.5919For batch 1065, loss is    0.65.\n","1067/1531 [===================>..........] - ETA: 24s - loss: 0.6691 - accuracy: 0.5920For batch 1066, loss is    0.65.\n","1068/1531 [===================>..........] - ETA: 24s - loss: 0.6691 - accuracy: 0.5920For batch 1067, loss is    0.65.\n","1069/1531 [===================>..........] - ETA: 24s - loss: 0.6691 - accuracy: 0.5920For batch 1068, loss is    0.65.\n","1070/1531 [===================>..........] - ETA: 24s - loss: 0.6690 - accuracy: 0.5920For batch 1069, loss is    0.65.\n","1071/1531 [===================>..........] - ETA: 24s - loss: 0.6690 - accuracy: 0.5921For batch 1070, loss is    0.65.\n","1072/1531 [====================>.........] - ETA: 24s - loss: 0.6690 - accuracy: 0.5921For batch 1071, loss is    0.65.\n","1073/1531 [====================>.........] - ETA: 24s - loss: 0.6690 - accuracy: 0.5921For batch 1072, loss is    0.65.\n","1074/1531 [====================>.........] - ETA: 24s - loss: 0.6690 - accuracy: 0.5921For batch 1073, loss is    0.65.\n","1075/1531 [====================>.........] - ETA: 24s - loss: 0.6690 - accuracy: 0.5922For batch 1074, loss is    0.65.\n","1076/1531 [====================>.........] - ETA: 24s - loss: 0.6689 - accuracy: 0.5922For batch 1075, loss is    0.65.\n","1077/1531 [====================>.........] - ETA: 24s - loss: 0.6689 - accuracy: 0.5922For batch 1076, loss is    0.65.\n","1078/1531 [====================>.........] - ETA: 24s - loss: 0.6689 - accuracy: 0.5923For batch 1077, loss is    0.65.\n","1079/1531 [====================>.........] - ETA: 24s - loss: 0.6689 - accuracy: 0.5923For batch 1078, loss is    0.65.\n","1080/1531 [====================>.........] - ETA: 24s - loss: 0.6689 - accuracy: 0.5923For batch 1079, loss is    0.65.\n","1081/1531 [====================>.........] - ETA: 24s - loss: 0.6688 - accuracy: 0.5923For batch 1080, loss is    0.65.\n","1082/1531 [====================>.........] - ETA: 23s - loss: 0.6688 - accuracy: 0.5924For batch 1081, loss is    0.65.\n","1083/1531 [====================>.........] - ETA: 23s - loss: 0.6688 - accuracy: 0.5924For batch 1082, loss is    0.65.\n","1084/1531 [====================>.........] - ETA: 23s - loss: 0.6688 - accuracy: 0.5924For batch 1083, loss is    0.65.\n","1085/1531 [====================>.........] - ETA: 23s - loss: 0.6688 - accuracy: 0.5924For batch 1084, loss is    0.65.\n","1086/1531 [====================>.........] - ETA: 23s - loss: 0.6688 - accuracy: 0.5925For batch 1085, loss is    0.65.\n","1087/1531 [====================>.........] - ETA: 23s - loss: 0.6687 - accuracy: 0.5925For batch 1086, loss is    0.65.\n","1088/1531 [====================>.........] - ETA: 23s - loss: 0.6687 - accuracy: 0.5925For batch 1087, loss is    0.65.\n","1089/1531 [====================>.........] - ETA: 23s - loss: 0.6687 - accuracy: 0.5925For batch 1088, loss is    0.65.\n","1090/1531 [====================>.........] - ETA: 23s - loss: 0.6687 - accuracy: 0.5926For batch 1089, loss is    0.65.\n","1091/1531 [====================>.........] - ETA: 23s - loss: 0.6687 - accuracy: 0.5926For batch 1090, loss is    0.65.\n","1092/1531 [====================>.........] - ETA: 23s - loss: 0.6686 - accuracy: 0.5926For batch 1091, loss is    0.65.\n","1093/1531 [====================>.........] - ETA: 23s - loss: 0.6686 - accuracy: 0.5926For batch 1092, loss is    0.65.\n","1094/1531 [====================>.........] - ETA: 23s - loss: 0.6686 - accuracy: 0.5927For batch 1093, loss is    0.65.\n","1095/1531 [====================>.........] - ETA: 23s - loss: 0.6686 - accuracy: 0.5927For batch 1094, loss is    0.65.\n","1096/1531 [====================>.........] - ETA: 23s - loss: 0.6686 - accuracy: 0.5927For batch 1095, loss is    0.65.\n","1097/1531 [====================>.........] - ETA: 23s - loss: 0.6686 - accuracy: 0.5927For batch 1096, loss is    0.65.\n","1098/1531 [====================>.........] - ETA: 23s - loss: 0.6685 - accuracy: 0.5928For batch 1097, loss is    0.65.\n","1099/1531 [====================>.........] - ETA: 23s - loss: 0.6685 - accuracy: 0.5928For batch 1098, loss is    0.65.\n","1100/1531 [====================>.........] - ETA: 23s - loss: 0.6685 - accuracy: 0.5928For batch 1099, loss is    0.65.\n","1101/1531 [====================>.........] - ETA: 23s - loss: 0.6685 - accuracy: 0.5928For batch 1100, loss is    0.65.\n","1102/1531 [====================>.........] - ETA: 22s - loss: 0.6685 - accuracy: 0.5929For batch 1101, loss is    0.65.\n","1103/1531 [====================>.........] - ETA: 22s - loss: 0.6685 - accuracy: 0.5929For batch 1102, loss is    0.65.\n","1104/1531 [====================>.........] - ETA: 22s - loss: 0.6684 - accuracy: 0.5929For batch 1103, loss is    0.65.\n","1105/1531 [====================>.........] - ETA: 22s - loss: 0.6684 - accuracy: 0.5929For batch 1104, loss is    0.65.\n","1106/1531 [====================>.........] - ETA: 22s - loss: 0.6684 - accuracy: 0.5930For batch 1105, loss is    0.65.\n","1107/1531 [====================>.........] - ETA: 22s - loss: 0.6684 - accuracy: 0.5930For batch 1106, loss is    0.65.\n","1108/1531 [====================>.........] - ETA: 22s - loss: 0.6684 - accuracy: 0.5930For batch 1107, loss is    0.65.\n","1109/1531 [====================>.........] - ETA: 22s - loss: 0.6683 - accuracy: 0.5930For batch 1108, loss is    0.65.\n","1110/1531 [====================>.........] - ETA: 22s - loss: 0.6683 - accuracy: 0.5930For batch 1109, loss is    0.65.\n","1111/1531 [====================>.........] - ETA: 22s - loss: 0.6683 - accuracy: 0.5931For batch 1110, loss is    0.65.\n","1112/1531 [====================>.........] - ETA: 22s - loss: 0.6683 - accuracy: 0.5931For batch 1111, loss is    0.65.\n","1113/1531 [====================>.........] - ETA: 22s - loss: 0.6683 - accuracy: 0.5931For batch 1112, loss is    0.65.\n","1114/1531 [====================>.........] - ETA: 22s - loss: 0.6683 - accuracy: 0.5931For batch 1113, loss is    0.65.\n","1115/1531 [====================>.........] - ETA: 22s - loss: 0.6682 - accuracy: 0.5932For batch 1114, loss is    0.65.\n","1116/1531 [====================>.........] - ETA: 22s - loss: 0.6682 - accuracy: 0.5932For batch 1115, loss is    0.65.\n","1117/1531 [====================>.........] - ETA: 22s - loss: 0.6682 - accuracy: 0.5932For batch 1116, loss is    0.65.\n","1118/1531 [====================>.........] - ETA: 22s - loss: 0.6682 - accuracy: 0.5932For batch 1117, loss is    0.65.\n","1119/1531 [====================>.........] - ETA: 22s - loss: 0.6682 - accuracy: 0.5933For batch 1118, loss is    0.65.\n","1120/1531 [====================>.........] - ETA: 22s - loss: 0.6682 - accuracy: 0.5933For batch 1119, loss is    0.65.\n","1121/1531 [====================>.........] - ETA: 21s - loss: 0.6681 - accuracy: 0.5933For batch 1120, loss is    0.65.\n","1122/1531 [====================>.........] - ETA: 21s - loss: 0.6681 - accuracy: 0.5933For batch 1121, loss is    0.65.\n","1123/1531 [=====================>........] - ETA: 21s - loss: 0.6681 - accuracy: 0.5934For batch 1122, loss is    0.65.\n","1124/1531 [=====================>........] - ETA: 21s - loss: 0.6681 - accuracy: 0.5934For batch 1123, loss is    0.65.\n","1125/1531 [=====================>........] - ETA: 21s - loss: 0.6681 - accuracy: 0.5934For batch 1124, loss is    0.65.\n","1126/1531 [=====================>........] - ETA: 21s - loss: 0.6681 - accuracy: 0.5934For batch 1125, loss is    0.65.\n","1127/1531 [=====================>........] - ETA: 21s - loss: 0.6680 - accuracy: 0.5935For batch 1126, loss is    0.65.\n","1128/1531 [=====================>........] - ETA: 21s - loss: 0.6680 - accuracy: 0.5935For batch 1127, loss is    0.65.\n","1129/1531 [=====================>........] - ETA: 21s - loss: 0.6680 - accuracy: 0.5935For batch 1128, loss is    0.65.\n","1130/1531 [=====================>........] - ETA: 21s - loss: 0.6680 - accuracy: 0.5935For batch 1129, loss is    0.65.\n","1131/1531 [=====================>........] - ETA: 21s - loss: 0.6680 - accuracy: 0.5936For batch 1130, loss is    0.65.\n","1132/1531 [=====================>........] - ETA: 21s - loss: 0.6680 - accuracy: 0.5936For batch 1131, loss is    0.65.\n","1133/1531 [=====================>........] - ETA: 21s - loss: 0.6679 - accuracy: 0.5936For batch 1132, loss is    0.65.\n","1134/1531 [=====================>........] - ETA: 21s - loss: 0.6679 - accuracy: 0.5936For batch 1133, loss is    0.65.\n","1135/1531 [=====================>........] - ETA: 21s - loss: 0.6679 - accuracy: 0.5936For batch 1134, loss is    0.65.\n","1136/1531 [=====================>........] - ETA: 21s - loss: 0.6679 - accuracy: 0.5937For batch 1135, loss is    0.65.\n","1137/1531 [=====================>........] - ETA: 21s - loss: 0.6679 - accuracy: 0.5937For batch 1136, loss is    0.65.\n","1138/1531 [=====================>........] - ETA: 21s - loss: 0.6679 - accuracy: 0.5937For batch 1137, loss is    0.65.\n","1139/1531 [=====================>........] - ETA: 21s - loss: 0.6678 - accuracy: 0.5937For batch 1138, loss is    0.65.\n","1140/1531 [=====================>........] - ETA: 21s - loss: 0.6678 - accuracy: 0.5938For batch 1139, loss is    0.65.\n","1141/1531 [=====================>........] - ETA: 20s - loss: 0.6678 - accuracy: 0.5938For batch 1140, loss is    0.65.\n","1142/1531 [=====================>........] - ETA: 20s - loss: 0.6678 - accuracy: 0.5938For batch 1141, loss is    0.65.\n","1143/1531 [=====================>........] - ETA: 20s - loss: 0.6678 - accuracy: 0.5938For batch 1142, loss is    0.65.\n","1144/1531 [=====================>........] - ETA: 20s - loss: 0.6678 - accuracy: 0.5939For batch 1143, loss is    0.65.\n","1145/1531 [=====================>........] - ETA: 20s - loss: 0.6677 - accuracy: 0.5939For batch 1144, loss is    0.65.\n","1146/1531 [=====================>........] - ETA: 20s - loss: 0.6677 - accuracy: 0.5939For batch 1145, loss is    0.65.\n","1147/1531 [=====================>........] - ETA: 20s - loss: 0.6677 - accuracy: 0.5939For batch 1146, loss is    0.65.\n","1148/1531 [=====================>........] - ETA: 20s - loss: 0.6677 - accuracy: 0.5939For batch 1147, loss is    0.65.\n","1149/1531 [=====================>........] - ETA: 20s - loss: 0.6677 - accuracy: 0.5940For batch 1148, loss is    0.65.\n","1150/1531 [=====================>........] - ETA: 20s - loss: 0.6677 - accuracy: 0.5940For batch 1149, loss is    0.65.\n","1151/1531 [=====================>........] - ETA: 20s - loss: 0.6676 - accuracy: 0.5940For batch 1150, loss is    0.65.\n","1152/1531 [=====================>........] - ETA: 20s - loss: 0.6676 - accuracy: 0.5940For batch 1151, loss is    0.65.\n","1153/1531 [=====================>........] - ETA: 20s - loss: 0.6676 - accuracy: 0.5941For batch 1152, loss is    0.65.\n","1154/1531 [=====================>........] - ETA: 20s - loss: 0.6676 - accuracy: 0.5941For batch 1153, loss is    0.65.\n","1155/1531 [=====================>........] - ETA: 20s - loss: 0.6676 - accuracy: 0.5941For batch 1154, loss is    0.65.\n","1156/1531 [=====================>........] - ETA: 20s - loss: 0.6676 - accuracy: 0.5941For batch 1155, loss is    0.65.\n","1157/1531 [=====================>........] - ETA: 20s - loss: 0.6675 - accuracy: 0.5941For batch 1156, loss is    0.65.\n","1158/1531 [=====================>........] - ETA: 20s - loss: 0.6675 - accuracy: 0.5942For batch 1157, loss is    0.65.\n","1159/1531 [=====================>........] - ETA: 20s - loss: 0.6675 - accuracy: 0.5942For batch 1158, loss is    0.65.\n","1160/1531 [=====================>........] - ETA: 19s - loss: 0.6675 - accuracy: 0.5942For batch 1159, loss is    0.65.\n","1161/1531 [=====================>........] - ETA: 19s - loss: 0.6675 - accuracy: 0.5942For batch 1160, loss is    0.65.\n","1162/1531 [=====================>........] - ETA: 19s - loss: 0.6675 - accuracy: 0.5943For batch 1161, loss is    0.65.\n","1163/1531 [=====================>........] - ETA: 19s - loss: 0.6675 - accuracy: 0.5943For batch 1162, loss is    0.65.\n","1164/1531 [=====================>........] - ETA: 19s - loss: 0.6674 - accuracy: 0.5943For batch 1163, loss is    0.65.\n","1165/1531 [=====================>........] - ETA: 19s - loss: 0.6674 - accuracy: 0.5943For batch 1164, loss is    0.65.\n","1166/1531 [=====================>........] - ETA: 19s - loss: 0.6674 - accuracy: 0.5944For batch 1165, loss is    0.65.\n","1167/1531 [=====================>........] - ETA: 19s - loss: 0.6674 - accuracy: 0.5944For batch 1166, loss is    0.65.\n","1168/1531 [=====================>........] - ETA: 19s - loss: 0.6674 - accuracy: 0.5944For batch 1167, loss is    0.65.\n","1169/1531 [=====================>........] - ETA: 19s - loss: 0.6674 - accuracy: 0.5944For batch 1168, loss is    0.65.\n","1170/1531 [=====================>........] - ETA: 19s - loss: 0.6673 - accuracy: 0.5944For batch 1169, loss is    0.65.\n","1171/1531 [=====================>........] - ETA: 19s - loss: 0.6673 - accuracy: 0.5945For batch 1170, loss is    0.65.\n","1172/1531 [=====================>........] - ETA: 19s - loss: 0.6673 - accuracy: 0.5945For batch 1171, loss is    0.65.\n","1173/1531 [=====================>........] - ETA: 19s - loss: 0.6673 - accuracy: 0.5945For batch 1172, loss is    0.65.\n","1174/1531 [======================>.......] - ETA: 19s - loss: 0.6673 - accuracy: 0.5945For batch 1173, loss is    0.65.\n","1175/1531 [======================>.......] - ETA: 19s - loss: 0.6673 - accuracy: 0.5946For batch 1174, loss is    0.65.\n","1176/1531 [======================>.......] - ETA: 19s - loss: 0.6672 - accuracy: 0.5946For batch 1175, loss is    0.65.\n","1177/1531 [======================>.......] - ETA: 19s - loss: 0.6672 - accuracy: 0.5946For batch 1176, loss is    0.65.\n","1178/1531 [======================>.......] - ETA: 19s - loss: 0.6672 - accuracy: 0.5946For batch 1177, loss is    0.65.\n","1179/1531 [======================>.......] - ETA: 19s - loss: 0.6672 - accuracy: 0.5946For batch 1178, loss is    0.65.\n","1180/1531 [======================>.......] - ETA: 18s - loss: 0.6672 - accuracy: 0.5947For batch 1179, loss is    0.65.\n","1181/1531 [======================>.......] - ETA: 18s - loss: 0.6672 - accuracy: 0.5947For batch 1180, loss is    0.65.\n","1182/1531 [======================>.......] - ETA: 18s - loss: 0.6672 - accuracy: 0.5947For batch 1181, loss is    0.65.\n","1183/1531 [======================>.......] - ETA: 18s - loss: 0.6671 - accuracy: 0.5947For batch 1182, loss is    0.65.\n","1184/1531 [======================>.......] - ETA: 18s - loss: 0.6671 - accuracy: 0.5948For batch 1183, loss is    0.65.\n","1185/1531 [======================>.......] - ETA: 18s - loss: 0.6671 - accuracy: 0.5948For batch 1184, loss is    0.65.\n","1186/1531 [======================>.......] - ETA: 18s - loss: 0.6671 - accuracy: 0.5948For batch 1185, loss is    0.65.\n","1187/1531 [======================>.......] - ETA: 18s - loss: 0.6671 - accuracy: 0.5948For batch 1186, loss is    0.65.\n","1188/1531 [======================>.......] - ETA: 18s - loss: 0.6671 - accuracy: 0.5948For batch 1187, loss is    0.65.\n","1189/1531 [======================>.......] - ETA: 18s - loss: 0.6670 - accuracy: 0.5949For batch 1188, loss is    0.65.\n","1190/1531 [======================>.......] - ETA: 18s - loss: 0.6670 - accuracy: 0.5949For batch 1189, loss is    0.65.\n","1191/1531 [======================>.......] - ETA: 18s - loss: 0.6670 - accuracy: 0.5949For batch 1190, loss is    0.65.\n","1192/1531 [======================>.......] - ETA: 18s - loss: 0.6670 - accuracy: 0.5949For batch 1191, loss is    0.65.\n","1193/1531 [======================>.......] - ETA: 18s - loss: 0.6670 - accuracy: 0.5949For batch 1192, loss is    0.65.\n","1194/1531 [======================>.......] - ETA: 18s - loss: 0.6670 - accuracy: 0.5950For batch 1193, loss is    0.65.\n","1195/1531 [======================>.......] - ETA: 18s - loss: 0.6669 - accuracy: 0.5950For batch 1194, loss is    0.65.\n","1196/1531 [======================>.......] - ETA: 18s - loss: 0.6669 - accuracy: 0.5950For batch 1195, loss is    0.65.\n","1197/1531 [======================>.......] - ETA: 18s - loss: 0.6669 - accuracy: 0.5950For batch 1196, loss is    0.65.\n","1198/1531 [======================>.......] - ETA: 18s - loss: 0.6669 - accuracy: 0.5951For batch 1197, loss is    0.65.\n","1199/1531 [======================>.......] - ETA: 17s - loss: 0.6669 - accuracy: 0.5951For batch 1198, loss is    0.65.\n","1200/1531 [======================>.......] - ETA: 17s - loss: 0.6669 - accuracy: 0.5951For batch 1199, loss is    0.65.\n","1201/1531 [======================>.......] - ETA: 17s - loss: 0.6669 - accuracy: 0.5951For batch 1200, loss is    0.65.\n","1202/1531 [======================>.......] - ETA: 17s - loss: 0.6668 - accuracy: 0.5951For batch 1201, loss is    0.65.\n","1203/1531 [======================>.......] - ETA: 17s - loss: 0.6668 - accuracy: 0.5952For batch 1202, loss is    0.65.\n","1204/1531 [======================>.......] - ETA: 17s - loss: 0.6668 - accuracy: 0.5952For batch 1203, loss is    0.65.\n","1205/1531 [======================>.......] - ETA: 17s - loss: 0.6668 - accuracy: 0.5952For batch 1204, loss is    0.65.\n","1206/1531 [======================>.......] - ETA: 17s - loss: 0.6668 - accuracy: 0.5952For batch 1205, loss is    0.65.\n","1207/1531 [======================>.......] - ETA: 17s - loss: 0.6668 - accuracy: 0.5953For batch 1206, loss is    0.65.\n","1208/1531 [======================>.......] - ETA: 17s - loss: 0.6667 - accuracy: 0.5953For batch 1207, loss is    0.65.\n","1209/1531 [======================>.......] - ETA: 17s - loss: 0.6667 - accuracy: 0.5953For batch 1208, loss is    0.65.\n","1210/1531 [======================>.......] - ETA: 17s - loss: 0.6667 - accuracy: 0.5953For batch 1209, loss is    0.65.\n","1211/1531 [======================>.......] - ETA: 17s - loss: 0.6667 - accuracy: 0.5953For batch 1210, loss is    0.65.\n","1212/1531 [======================>.......] - ETA: 17s - loss: 0.6667 - accuracy: 0.5954For batch 1211, loss is    0.65.\n","1213/1531 [======================>.......] - ETA: 17s - loss: 0.6667 - accuracy: 0.5954For batch 1212, loss is    0.65.\n","1214/1531 [======================>.......] - ETA: 17s - loss: 0.6667 - accuracy: 0.5954For batch 1213, loss is    0.65.\n","1215/1531 [======================>.......] - ETA: 17s - loss: 0.6666 - accuracy: 0.5954For batch 1214, loss is    0.65.\n","1216/1531 [======================>.......] - ETA: 17s - loss: 0.6666 - accuracy: 0.5954For batch 1215, loss is    0.65.\n","1217/1531 [======================>.......] - ETA: 17s - loss: 0.6666 - accuracy: 0.5955For batch 1216, loss is    0.65.\n","1218/1531 [======================>.......] - ETA: 16s - loss: 0.6666 - accuracy: 0.5955For batch 1217, loss is    0.65.\n","1219/1531 [======================>.......] - ETA: 16s - loss: 0.6666 - accuracy: 0.5955For batch 1218, loss is    0.65.\n","1220/1531 [======================>.......] - ETA: 16s - loss: 0.6666 - accuracy: 0.5955For batch 1219, loss is    0.65.\n","1221/1531 [======================>.......] - ETA: 16s - loss: 0.6665 - accuracy: 0.5956For batch 1220, loss is    0.65.\n","1222/1531 [======================>.......] - ETA: 16s - loss: 0.6665 - accuracy: 0.5956For batch 1221, loss is    0.65.\n","1223/1531 [======================>.......] - ETA: 16s - loss: 0.6665 - accuracy: 0.5956For batch 1222, loss is    0.65.\n","1224/1531 [======================>.......] - ETA: 16s - loss: 0.6665 - accuracy: 0.5956For batch 1223, loss is    0.65.\n","1225/1531 [=======================>......] - ETA: 16s - loss: 0.6665 - accuracy: 0.5956For batch 1224, loss is    0.65.\n","1226/1531 [=======================>......] - ETA: 16s - loss: 0.6665 - accuracy: 0.5957For batch 1225, loss is    0.65.\n","1227/1531 [=======================>......] - ETA: 16s - loss: 0.6665 - accuracy: 0.5957For batch 1226, loss is    0.65.\n","1228/1531 [=======================>......] - ETA: 16s - loss: 0.6664 - accuracy: 0.5957For batch 1227, loss is    0.65.\n","1229/1531 [=======================>......] - ETA: 16s - loss: 0.6664 - accuracy: 0.5957For batch 1228, loss is    0.65.\n","1230/1531 [=======================>......] - ETA: 16s - loss: 0.6664 - accuracy: 0.5957For batch 1229, loss is    0.65.\n","1231/1531 [=======================>......] - ETA: 16s - loss: 0.6664 - accuracy: 0.5958For batch 1230, loss is    0.65.\n","1232/1531 [=======================>......] - ETA: 16s - loss: 0.6664 - accuracy: 0.5958For batch 1231, loss is    0.65.\n","1233/1531 [=======================>......] - ETA: 16s - loss: 0.6664 - accuracy: 0.5958For batch 1232, loss is    0.65.\n","1234/1531 [=======================>......] - ETA: 16s - loss: 0.6664 - accuracy: 0.5958For batch 1233, loss is    0.65.\n","1235/1531 [=======================>......] - ETA: 16s - loss: 0.6663 - accuracy: 0.5958For batch 1234, loss is    0.65.\n","1236/1531 [=======================>......] - ETA: 16s - loss: 0.6663 - accuracy: 0.5959For batch 1235, loss is    0.65.\n","1237/1531 [=======================>......] - ETA: 15s - loss: 0.6663 - accuracy: 0.5959For batch 1236, loss is    0.65.\n","1238/1531 [=======================>......] - ETA: 15s - loss: 0.6663 - accuracy: 0.5959For batch 1237, loss is    0.65.\n","1239/1531 [=======================>......] - ETA: 15s - loss: 0.6663 - accuracy: 0.5959For batch 1238, loss is    0.65.\n","1240/1531 [=======================>......] - ETA: 15s - loss: 0.6663 - accuracy: 0.5960For batch 1239, loss is    0.65.\n","1241/1531 [=======================>......] - ETA: 15s - loss: 0.6662 - accuracy: 0.5960For batch 1240, loss is    0.65.\n","1242/1531 [=======================>......] - ETA: 15s - loss: 0.6662 - accuracy: 0.5960For batch 1241, loss is    0.65.\n","1243/1531 [=======================>......] - ETA: 15s - loss: 0.6662 - accuracy: 0.5960For batch 1242, loss is    0.65.\n","1244/1531 [=======================>......] - ETA: 15s - loss: 0.6662 - accuracy: 0.5960For batch 1243, loss is    0.65.\n","1245/1531 [=======================>......] - ETA: 15s - loss: 0.6662 - accuracy: 0.5961For batch 1244, loss is    0.65.\n","1246/1531 [=======================>......] - ETA: 15s - loss: 0.6662 - accuracy: 0.5961For batch 1245, loss is    0.65.\n","1247/1531 [=======================>......] - ETA: 15s - loss: 0.6662 - accuracy: 0.5961For batch 1246, loss is    0.65.\n","1248/1531 [=======================>......] - ETA: 15s - loss: 0.6661 - accuracy: 0.5961For batch 1247, loss is    0.65.\n","1249/1531 [=======================>......] - ETA: 15s - loss: 0.6661 - accuracy: 0.5961For batch 1248, loss is    0.65.\n","1250/1531 [=======================>......] - ETA: 15s - loss: 0.6661 - accuracy: 0.5962For batch 1249, loss is    0.65.\n","1251/1531 [=======================>......] - ETA: 15s - loss: 0.6661 - accuracy: 0.5962For batch 1250, loss is    0.65.\n","1252/1531 [=======================>......] - ETA: 15s - loss: 0.6661 - accuracy: 0.5962For batch 1251, loss is    0.65.\n","1253/1531 [=======================>......] - ETA: 15s - loss: 0.6661 - accuracy: 0.5962For batch 1252, loss is    0.65.\n","1254/1531 [=======================>......] - ETA: 15s - loss: 0.6661 - accuracy: 0.5962For batch 1253, loss is    0.65.\n","1255/1531 [=======================>......] - ETA: 15s - loss: 0.6660 - accuracy: 0.5963For batch 1254, loss is    0.65.\n","1256/1531 [=======================>......] - ETA: 14s - loss: 0.6660 - accuracy: 0.5963For batch 1255, loss is    0.65.\n","1257/1531 [=======================>......] - ETA: 14s - loss: 0.6660 - accuracy: 0.5963For batch 1256, loss is    0.65.\n","1258/1531 [=======================>......] - ETA: 14s - loss: 0.6660 - accuracy: 0.5963For batch 1257, loss is    0.65.\n","1259/1531 [=======================>......] - ETA: 14s - loss: 0.6660 - accuracy: 0.5964For batch 1258, loss is    0.65.\n","1260/1531 [=======================>......] - ETA: 14s - loss: 0.6660 - accuracy: 0.5964For batch 1259, loss is    0.65.\n","1261/1531 [=======================>......] - ETA: 14s - loss: 0.6659 - accuracy: 0.5964For batch 1260, loss is    0.65.\n","1262/1531 [=======================>......] - ETA: 14s - loss: 0.6659 - accuracy: 0.5964For batch 1261, loss is    0.65.\n","1263/1531 [=======================>......] - ETA: 14s - loss: 0.6659 - accuracy: 0.5964For batch 1262, loss is    0.65.\n","1264/1531 [=======================>......] - ETA: 14s - loss: 0.6659 - accuracy: 0.5965For batch 1263, loss is    0.65.\n","1265/1531 [=======================>......] - ETA: 14s - loss: 0.6659 - accuracy: 0.5965For batch 1264, loss is    0.65.\n","1266/1531 [=======================>......] - ETA: 14s - loss: 0.6659 - accuracy: 0.5965For batch 1265, loss is    0.65.\n","1267/1531 [=======================>......] - ETA: 14s - loss: 0.6659 - accuracy: 0.5965For batch 1266, loss is    0.65.\n","1268/1531 [=======================>......] - ETA: 14s - loss: 0.6658 - accuracy: 0.5965For batch 1267, loss is    0.65.\n","1269/1531 [=======================>......] - ETA: 14s - loss: 0.6658 - accuracy: 0.5966For batch 1268, loss is    0.65.\n","1270/1531 [=======================>......] - ETA: 14s - loss: 0.6658 - accuracy: 0.5966For batch 1269, loss is    0.65.\n","1271/1531 [=======================>......] - ETA: 14s - loss: 0.6658 - accuracy: 0.5966For batch 1270, loss is    0.65.\n","1272/1531 [=======================>......] - ETA: 14s - loss: 0.6658 - accuracy: 0.5966For batch 1271, loss is    0.65.\n","1273/1531 [=======================>......] - ETA: 14s - loss: 0.6658 - accuracy: 0.5966For batch 1272, loss is    0.65.\n","1274/1531 [=======================>......] - ETA: 14s - loss: 0.6658 - accuracy: 0.5967For batch 1273, loss is    0.65.\n","1275/1531 [=======================>......] - ETA: 13s - loss: 0.6657 - accuracy: 0.5967For batch 1274, loss is    0.65.\n","1276/1531 [=======================>......] - ETA: 13s - loss: 0.6657 - accuracy: 0.5967For batch 1275, loss is    0.65.\n","1277/1531 [========================>.....] - ETA: 13s - loss: 0.6657 - accuracy: 0.5967For batch 1276, loss is    0.65.\n","1278/1531 [========================>.....] - ETA: 13s - loss: 0.6657 - accuracy: 0.5967For batch 1277, loss is    0.65.\n","1279/1531 [========================>.....] - ETA: 13s - loss: 0.6657 - accuracy: 0.5968For batch 1278, loss is    0.65.\n","1280/1531 [========================>.....] - ETA: 13s - loss: 0.6657 - accuracy: 0.5968For batch 1279, loss is    0.65.\n","1281/1531 [========================>.....] - ETA: 13s - loss: 0.6656 - accuracy: 0.5968For batch 1280, loss is    0.65.\n","1282/1531 [========================>.....] - ETA: 13s - loss: 0.6656 - accuracy: 0.5968For batch 1281, loss is    0.65.\n","1283/1531 [========================>.....] - ETA: 13s - loss: 0.6656 - accuracy: 0.5968For batch 1282, loss is    0.65.\n","1284/1531 [========================>.....] - ETA: 13s - loss: 0.6656 - accuracy: 0.5969For batch 1283, loss is    0.65.\n","1285/1531 [========================>.....] - ETA: 13s - loss: 0.6656 - accuracy: 0.5969For batch 1284, loss is    0.65.\n","1286/1531 [========================>.....] - ETA: 13s - loss: 0.6656 - accuracy: 0.5969For batch 1285, loss is    0.65.\n","1287/1531 [========================>.....] - ETA: 13s - loss: 0.6656 - accuracy: 0.5969For batch 1286, loss is    0.65.\n","1288/1531 [========================>.....] - ETA: 13s - loss: 0.6655 - accuracy: 0.5969For batch 1287, loss is    0.65.\n","1289/1531 [========================>.....] - ETA: 13s - loss: 0.6655 - accuracy: 0.5970For batch 1288, loss is    0.65.\n","1290/1531 [========================>.....] - ETA: 13s - loss: 0.6655 - accuracy: 0.5970For batch 1289, loss is    0.65.\n","1291/1531 [========================>.....] - ETA: 13s - loss: 0.6655 - accuracy: 0.5970For batch 1290, loss is    0.65.\n","1292/1531 [========================>.....] - ETA: 13s - loss: 0.6655 - accuracy: 0.5970For batch 1291, loss is    0.65.\n","1293/1531 [========================>.....] - ETA: 13s - loss: 0.6655 - accuracy: 0.5970For batch 1292, loss is    0.65.\n","1294/1531 [========================>.....] - ETA: 12s - loss: 0.6655 - accuracy: 0.5971For batch 1293, loss is    0.65.\n","1295/1531 [========================>.....] - ETA: 12s - loss: 0.6654 - accuracy: 0.5971For batch 1294, loss is    0.65.\n","1296/1531 [========================>.....] - ETA: 12s - loss: 0.6654 - accuracy: 0.5971For batch 1295, loss is    0.65.\n","1297/1531 [========================>.....] - ETA: 12s - loss: 0.6654 - accuracy: 0.5971For batch 1296, loss is    0.65.\n","1298/1531 [========================>.....] - ETA: 12s - loss: 0.6654 - accuracy: 0.5972For batch 1297, loss is    0.65.\n","1299/1531 [========================>.....] - ETA: 12s - loss: 0.6654 - accuracy: 0.5972For batch 1298, loss is    0.65.\n","1300/1531 [========================>.....] - ETA: 12s - loss: 0.6654 - accuracy: 0.5972For batch 1299, loss is    0.65.\n","1301/1531 [========================>.....] - ETA: 12s - loss: 0.6654 - accuracy: 0.5972For batch 1300, loss is    0.65.\n","1302/1531 [========================>.....] - ETA: 12s - loss: 0.6653 - accuracy: 0.5972For batch 1301, loss is    0.65.\n","1303/1531 [========================>.....] - ETA: 12s - loss: 0.6653 - accuracy: 0.5973For batch 1302, loss is    0.65.\n","1304/1531 [========================>.....] - ETA: 12s - loss: 0.6653 - accuracy: 0.5973For batch 1303, loss is    0.65.\n","1305/1531 [========================>.....] - ETA: 12s - loss: 0.6653 - accuracy: 0.5973For batch 1304, loss is    0.65.\n","1306/1531 [========================>.....] - ETA: 12s - loss: 0.6653 - accuracy: 0.5973For batch 1305, loss is    0.65.\n","1307/1531 [========================>.....] - ETA: 12s - loss: 0.6653 - accuracy: 0.5973For batch 1306, loss is    0.65.\n","1308/1531 [========================>.....] - ETA: 12s - loss: 0.6653 - accuracy: 0.5974For batch 1307, loss is    0.65.\n","1309/1531 [========================>.....] - ETA: 12s - loss: 0.6652 - accuracy: 0.5974For batch 1308, loss is    0.65.\n","1310/1531 [========================>.....] - ETA: 12s - loss: 0.6652 - accuracy: 0.5974For batch 1309, loss is    0.65.\n","1311/1531 [========================>.....] - ETA: 12s - loss: 0.6652 - accuracy: 0.5974For batch 1310, loss is    0.65.\n","1312/1531 [========================>.....] - ETA: 11s - loss: 0.6652 - accuracy: 0.5974For batch 1311, loss is    0.65.\n","1313/1531 [========================>.....] - ETA: 11s - loss: 0.6652 - accuracy: 0.5974For batch 1312, loss is    0.65.\n","1314/1531 [========================>.....] - ETA: 11s - loss: 0.6652 - accuracy: 0.5975For batch 1313, loss is    0.65.\n","1315/1531 [========================>.....] - ETA: 11s - loss: 0.6652 - accuracy: 0.5975For batch 1314, loss is    0.65.\n","1316/1531 [========================>.....] - ETA: 11s - loss: 0.6651 - accuracy: 0.5975For batch 1315, loss is    0.65.\n","1317/1531 [========================>.....] - ETA: 11s - loss: 0.6651 - accuracy: 0.5975For batch 1316, loss is    0.65.\n","1318/1531 [========================>.....] - ETA: 11s - loss: 0.6651 - accuracy: 0.5975For batch 1317, loss is    0.65.\n","1319/1531 [========================>.....] - ETA: 11s - loss: 0.6651 - accuracy: 0.5976For batch 1318, loss is    0.65.\n","1320/1531 [========================>.....] - ETA: 11s - loss: 0.6651 - accuracy: 0.5976For batch 1319, loss is    0.65.\n","1321/1531 [========================>.....] - ETA: 11s - loss: 0.6651 - accuracy: 0.5976For batch 1320, loss is    0.65.\n","1322/1531 [========================>.....] - ETA: 11s - loss: 0.6651 - accuracy: 0.5976For batch 1321, loss is    0.65.\n","1323/1531 [========================>.....] - ETA: 11s - loss: 0.6650 - accuracy: 0.5976For batch 1322, loss is    0.65.\n","1324/1531 [========================>.....] - ETA: 11s - loss: 0.6650 - accuracy: 0.5977For batch 1323, loss is    0.65.\n","1325/1531 [========================>.....] - ETA: 11s - loss: 0.6650 - accuracy: 0.5977For batch 1324, loss is    0.65.\n","1326/1531 [========================>.....] - ETA: 11s - loss: 0.6650 - accuracy: 0.5977For batch 1325, loss is    0.65.\n","1327/1531 [========================>.....] - ETA: 11s - loss: 0.6650 - accuracy: 0.5977For batch 1326, loss is    0.65.\n","1328/1531 [=========================>....] - ETA: 11s - loss: 0.6650 - accuracy: 0.5977For batch 1327, loss is    0.65.\n","1329/1531 [=========================>....] - ETA: 11s - loss: 0.6650 - accuracy: 0.5978For batch 1328, loss is    0.65.\n","1330/1531 [=========================>....] - ETA: 11s - loss: 0.6649 - accuracy: 0.5978For batch 1329, loss is    0.65.\n","1331/1531 [=========================>....] - ETA: 10s - loss: 0.6649 - accuracy: 0.5978For batch 1330, loss is    0.65.\n","1332/1531 [=========================>....] - ETA: 10s - loss: 0.6649 - accuracy: 0.5978For batch 1331, loss is    0.65.\n","1333/1531 [=========================>....] - ETA: 10s - loss: 0.6649 - accuracy: 0.5978For batch 1332, loss is    0.65.\n","1334/1531 [=========================>....] - ETA: 10s - loss: 0.6649 - accuracy: 0.5979For batch 1333, loss is    0.65.\n","1335/1531 [=========================>....] - ETA: 10s - loss: 0.6649 - accuracy: 0.5979For batch 1334, loss is    0.65.\n","1336/1531 [=========================>....] - ETA: 10s - loss: 0.6649 - accuracy: 0.5979For batch 1335, loss is    0.65.\n","1337/1531 [=========================>....] - ETA: 10s - loss: 0.6648 - accuracy: 0.5979For batch 1336, loss is    0.65.\n","1338/1531 [=========================>....] - ETA: 10s - loss: 0.6648 - accuracy: 0.5979For batch 1337, loss is    0.65.\n","1339/1531 [=========================>....] - ETA: 10s - loss: 0.6648 - accuracy: 0.5980For batch 1338, loss is    0.65.\n","1340/1531 [=========================>....] - ETA: 10s - loss: 0.6648 - accuracy: 0.5980For batch 1339, loss is    0.65.\n","1341/1531 [=========================>....] - ETA: 10s - loss: 0.6648 - accuracy: 0.5980For batch 1340, loss is    0.65.\n","1342/1531 [=========================>....] - ETA: 10s - loss: 0.6648 - accuracy: 0.5980For batch 1341, loss is    0.65.\n","1343/1531 [=========================>....] - ETA: 10s - loss: 0.6648 - accuracy: 0.5980For batch 1342, loss is    0.65.\n","1344/1531 [=========================>....] - ETA: 10s - loss: 0.6647 - accuracy: 0.5981For batch 1343, loss is    0.65.\n","1345/1531 [=========================>....] - ETA: 10s - loss: 0.6647 - accuracy: 0.5981For batch 1344, loss is    0.65.\n","1346/1531 [=========================>....] - ETA: 10s - loss: 0.6647 - accuracy: 0.5981For batch 1345, loss is    0.65.\n","1347/1531 [=========================>....] - ETA: 10s - loss: 0.6647 - accuracy: 0.5981For batch 1346, loss is    0.65.\n","1348/1531 [=========================>....] - ETA: 10s - loss: 0.6647 - accuracy: 0.5981For batch 1347, loss is    0.65.\n","1349/1531 [=========================>....] - ETA: 10s - loss: 0.6647 - accuracy: 0.5982For batch 1348, loss is    0.65.\n","1350/1531 [=========================>....] - ETA: 9s - loss: 0.6647 - accuracy: 0.5982 For batch 1349, loss is    0.65.\n","1351/1531 [=========================>....] - ETA: 9s - loss: 0.6646 - accuracy: 0.5982For batch 1350, loss is    0.65.\n","1352/1531 [=========================>....] - ETA: 9s - loss: 0.6646 - accuracy: 0.5982For batch 1351, loss is    0.65.\n","1353/1531 [=========================>....] - ETA: 9s - loss: 0.6646 - accuracy: 0.5982For batch 1352, loss is    0.65.\n","1354/1531 [=========================>....] - ETA: 9s - loss: 0.6646 - accuracy: 0.5982For batch 1353, loss is    0.65.\n","1355/1531 [=========================>....] - ETA: 9s - loss: 0.6646 - accuracy: 0.5983For batch 1354, loss is    0.65.\n","1356/1531 [=========================>....] - ETA: 9s - loss: 0.6646 - accuracy: 0.5983For batch 1355, loss is    0.65.\n","1357/1531 [=========================>....] - ETA: 9s - loss: 0.6646 - accuracy: 0.5983For batch 1356, loss is    0.65.\n","1358/1531 [=========================>....] - ETA: 9s - loss: 0.6645 - accuracy: 0.5983For batch 1357, loss is    0.65.\n","1359/1531 [=========================>....] - ETA: 9s - loss: 0.6645 - accuracy: 0.5983For batch 1358, loss is    0.65.\n","1360/1531 [=========================>....] - ETA: 9s - loss: 0.6645 - accuracy: 0.5984For batch 1359, loss is    0.65.\n","1361/1531 [=========================>....] - ETA: 9s - loss: 0.6645 - accuracy: 0.5984For batch 1360, loss is    0.65.\n","1362/1531 [=========================>....] - ETA: 9s - loss: 0.6645 - accuracy: 0.5984For batch 1361, loss is    0.65.\n","1363/1531 [=========================>....] - ETA: 9s - loss: 0.6645 - accuracy: 0.5984For batch 1362, loss is    0.65.\n","1364/1531 [=========================>....] - ETA: 9s - loss: 0.6645 - accuracy: 0.5984For batch 1363, loss is    0.65.\n","1365/1531 [=========================>....] - ETA: 9s - loss: 0.6644 - accuracy: 0.5985For batch 1364, loss is    0.65.\n","1366/1531 [=========================>....] - ETA: 9s - loss: 0.6644 - accuracy: 0.5985For batch 1365, loss is    0.65.\n","1367/1531 [=========================>....] - ETA: 9s - loss: 0.6644 - accuracy: 0.5985For batch 1366, loss is    0.65.\n","1368/1531 [=========================>....] - ETA: 8s - loss: 0.6644 - accuracy: 0.5985For batch 1367, loss is    0.65.\n","1369/1531 [=========================>....] - ETA: 8s - loss: 0.6644 - accuracy: 0.5985For batch 1368, loss is    0.65.\n","1370/1531 [=========================>....] - ETA: 8s - loss: 0.6644 - accuracy: 0.5986For batch 1369, loss is    0.65.\n","1371/1531 [=========================>....] - ETA: 8s - loss: 0.6644 - accuracy: 0.5986For batch 1370, loss is    0.65.\n","1372/1531 [=========================>....] - ETA: 8s - loss: 0.6643 - accuracy: 0.5986For batch 1371, loss is    0.65.\n","1373/1531 [=========================>....] - ETA: 8s - loss: 0.6643 - accuracy: 0.5986For batch 1372, loss is    0.65.\n","1374/1531 [=========================>....] - ETA: 8s - loss: 0.6643 - accuracy: 0.5986For batch 1373, loss is    0.65.\n","1375/1531 [=========================>....] - ETA: 8s - loss: 0.6643 - accuracy: 0.5986For batch 1374, loss is    0.65.\n","1376/1531 [=========================>....] - ETA: 8s - loss: 0.6643 - accuracy: 0.5987For batch 1375, loss is    0.65.\n","1377/1531 [=========================>....] - ETA: 8s - loss: 0.6643 - accuracy: 0.5987For batch 1376, loss is    0.65.\n","1378/1531 [=========================>....] - ETA: 8s - loss: 0.6643 - accuracy: 0.5987For batch 1377, loss is    0.65.\n","1379/1531 [==========================>...] - ETA: 8s - loss: 0.6643 - accuracy: 0.5987For batch 1378, loss is    0.65.\n","1380/1531 [==========================>...] - ETA: 8s - loss: 0.6642 - accuracy: 0.5987For batch 1379, loss is    0.65.\n","1381/1531 [==========================>...] - ETA: 8s - loss: 0.6642 - accuracy: 0.5988For batch 1380, loss is    0.65.\n","1382/1531 [==========================>...] - ETA: 8s - loss: 0.6642 - accuracy: 0.5988For batch 1381, loss is    0.65.\n","1383/1531 [==========================>...] - ETA: 8s - loss: 0.6642 - accuracy: 0.5988For batch 1382, loss is    0.65.\n","1384/1531 [==========================>...] - ETA: 8s - loss: 0.6642 - accuracy: 0.5988For batch 1383, loss is    0.65.\n","1385/1531 [==========================>...] - ETA: 8s - loss: 0.6642 - accuracy: 0.5988For batch 1384, loss is    0.65.\n","1386/1531 [==========================>...] - ETA: 8s - loss: 0.6642 - accuracy: 0.5989For batch 1385, loss is    0.65.\n","1387/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5989For batch 1386, loss is    0.65.\n","1388/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5989For batch 1387, loss is    0.65.\n","1389/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5989For batch 1388, loss is    0.65.\n","1390/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5989For batch 1389, loss is    0.65.\n","1391/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5989For batch 1390, loss is    0.65.\n","1392/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5990For batch 1391, loss is    0.65.\n","1393/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5990For batch 1392, loss is    0.65.\n","1394/1531 [==========================>...] - ETA: 7s - loss: 0.6641 - accuracy: 0.5990For batch 1393, loss is    0.65.\n","1395/1531 [==========================>...] - ETA: 7s - loss: 0.6640 - accuracy: 0.5990For batch 1394, loss is    0.65.\n","1396/1531 [==========================>...] - ETA: 7s - loss: 0.6640 - accuracy: 0.5990For batch 1395, loss is    0.65.\n","1397/1531 [==========================>...] - ETA: 7s - loss: 0.6640 - accuracy: 0.5991For batch 1396, loss is    0.65.\n","1398/1531 [==========================>...] - ETA: 7s - loss: 0.6640 - accuracy: 0.5991For batch 1397, loss is    0.65.\n","1399/1531 [==========================>...] - ETA: 7s - loss: 0.6640 - accuracy: 0.5991For batch 1398, loss is    0.65.\n","1400/1531 [==========================>...] - ETA: 7s - loss: 0.6640 - accuracy: 0.5991For batch 1399, loss is    0.65.\n","1401/1531 [==========================>...] - ETA: 7s - loss: 0.6640 - accuracy: 0.5991For batch 1400, loss is    0.65.\n","1402/1531 [==========================>...] - ETA: 7s - loss: 0.6639 - accuracy: 0.5991For batch 1401, loss is    0.65.\n","1403/1531 [==========================>...] - ETA: 7s - loss: 0.6639 - accuracy: 0.5992For batch 1402, loss is    0.65.\n","1404/1531 [==========================>...] - ETA: 7s - loss: 0.6639 - accuracy: 0.5992For batch 1403, loss is    0.65.\n","1405/1531 [==========================>...] - ETA: 6s - loss: 0.6639 - accuracy: 0.5992For batch 1404, loss is    0.65.\n","1406/1531 [==========================>...] - ETA: 6s - loss: 0.6639 - accuracy: 0.5992For batch 1405, loss is    0.65.\n","1407/1531 [==========================>...] - ETA: 6s - loss: 0.6639 - accuracy: 0.5992For batch 1406, loss is    0.65.\n","1408/1531 [==========================>...] - ETA: 6s - loss: 0.6639 - accuracy: 0.5993For batch 1407, loss is    0.65.\n","1409/1531 [==========================>...] - ETA: 6s - loss: 0.6639 - accuracy: 0.5993For batch 1408, loss is    0.65.\n","1410/1531 [==========================>...] - ETA: 6s - loss: 0.6638 - accuracy: 0.5993For batch 1409, loss is    0.65.\n","1411/1531 [==========================>...] - ETA: 6s - loss: 0.6638 - accuracy: 0.5993For batch 1410, loss is    0.65.\n","1412/1531 [==========================>...] - ETA: 6s - loss: 0.6638 - accuracy: 0.5993For batch 1411, loss is    0.65.\n","1413/1531 [==========================>...] - ETA: 6s - loss: 0.6638 - accuracy: 0.5993For batch 1412, loss is    0.65.\n","1414/1531 [==========================>...] - ETA: 6s - loss: 0.6638 - accuracy: 0.5994For batch 1413, loss is    0.65.\n","1415/1531 [==========================>...] - ETA: 6s - loss: 0.6638 - accuracy: 0.5994For batch 1414, loss is    0.65.\n","1416/1531 [==========================>...] - ETA: 6s - loss: 0.6638 - accuracy: 0.5994For batch 1415, loss is    0.65.\n","1417/1531 [==========================>...] - ETA: 6s - loss: 0.6637 - accuracy: 0.5994For batch 1416, loss is    0.65.\n","1418/1531 [==========================>...] - ETA: 6s - loss: 0.6637 - accuracy: 0.5994For batch 1417, loss is    0.65.\n","1419/1531 [==========================>...] - ETA: 6s - loss: 0.6637 - accuracy: 0.5994For batch 1418, loss is    0.65.\n","1420/1531 [==========================>...] - ETA: 6s - loss: 0.6637 - accuracy: 0.5995For batch 1419, loss is    0.65.\n","1421/1531 [==========================>...] - ETA: 6s - loss: 0.6637 - accuracy: 0.5995For batch 1420, loss is    0.65.\n","1422/1531 [==========================>...] - ETA: 6s - loss: 0.6637 - accuracy: 0.5995For batch 1421, loss is    0.64.\n","1423/1531 [==========================>...] - ETA: 5s - loss: 0.6637 - accuracy: 0.5995For batch 1422, loss is    0.64.\n","1424/1531 [==========================>...] - ETA: 5s - loss: 0.6637 - accuracy: 0.5995For batch 1423, loss is    0.64.\n","1425/1531 [==========================>...] - ETA: 5s - loss: 0.6636 - accuracy: 0.5996For batch 1424, loss is    0.64.\n","1426/1531 [==========================>...] - ETA: 5s - loss: 0.6636 - accuracy: 0.5996For batch 1425, loss is    0.64.\n","1427/1531 [==========================>...] - ETA: 5s - loss: 0.6636 - accuracy: 0.5996For batch 1426, loss is    0.64.\n","1428/1531 [==========================>...] - ETA: 5s - loss: 0.6636 - accuracy: 0.5996For batch 1427, loss is    0.64.\n","1429/1531 [==========================>...] - ETA: 5s - loss: 0.6636 - accuracy: 0.5996For batch 1428, loss is    0.64.\n","1430/1531 [===========================>..] - ETA: 5s - loss: 0.6636 - accuracy: 0.5996For batch 1429, loss is    0.64.\n","1431/1531 [===========================>..] - ETA: 5s - loss: 0.6636 - accuracy: 0.5997For batch 1430, loss is    0.64.\n","1432/1531 [===========================>..] - ETA: 5s - loss: 0.6636 - accuracy: 0.5997For batch 1431, loss is    0.64.\n","1433/1531 [===========================>..] - ETA: 5s - loss: 0.6635 - accuracy: 0.5997For batch 1432, loss is    0.64.\n","1434/1531 [===========================>..] - ETA: 5s - loss: 0.6635 - accuracy: 0.5997For batch 1433, loss is    0.64.\n","1435/1531 [===========================>..] - ETA: 5s - loss: 0.6635 - accuracy: 0.5997For batch 1434, loss is    0.64.\n","1436/1531 [===========================>..] - ETA: 5s - loss: 0.6635 - accuracy: 0.5997For batch 1435, loss is    0.64.\n","1437/1531 [===========================>..] - ETA: 5s - loss: 0.6635 - accuracy: 0.5998For batch 1436, loss is    0.64.\n","1438/1531 [===========================>..] - ETA: 5s - loss: 0.6635 - accuracy: 0.5998For batch 1437, loss is    0.64.\n","1439/1531 [===========================>..] - ETA: 5s - loss: 0.6635 - accuracy: 0.5998For batch 1438, loss is    0.64.\n","1440/1531 [===========================>..] - ETA: 5s - loss: 0.6634 - accuracy: 0.5998For batch 1439, loss is    0.64.\n","1441/1531 [===========================>..] - ETA: 5s - loss: 0.6634 - accuracy: 0.5998For batch 1440, loss is    0.64.\n","1442/1531 [===========================>..] - ETA: 4s - loss: 0.6634 - accuracy: 0.5998For batch 1441, loss is    0.64.\n","1443/1531 [===========================>..] - ETA: 4s - loss: 0.6634 - accuracy: 0.5999For batch 1442, loss is    0.64.\n","1444/1531 [===========================>..] - ETA: 4s - loss: 0.6634 - accuracy: 0.5999For batch 1443, loss is    0.64.\n","1445/1531 [===========================>..] - ETA: 4s - loss: 0.6634 - accuracy: 0.5999For batch 1444, loss is    0.64.\n","1446/1531 [===========================>..] - ETA: 4s - loss: 0.6634 - accuracy: 0.5999For batch 1445, loss is    0.64.\n","1447/1531 [===========================>..] - ETA: 4s - loss: 0.6634 - accuracy: 0.5999For batch 1446, loss is    0.64.\n","1448/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6000For batch 1447, loss is    0.64.\n","1449/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6000For batch 1448, loss is    0.64.\n","1450/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6000For batch 1449, loss is    0.64.\n","1451/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6000For batch 1450, loss is    0.64.\n","1452/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6000For batch 1451, loss is    0.64.\n","1453/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6000For batch 1452, loss is    0.64.\n","1454/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6001For batch 1453, loss is    0.64.\n","1455/1531 [===========================>..] - ETA: 4s - loss: 0.6633 - accuracy: 0.6001For batch 1454, loss is    0.64.\n","1456/1531 [===========================>..] - ETA: 4s - loss: 0.6632 - accuracy: 0.6001For batch 1455, loss is    0.64.\n","1457/1531 [===========================>..] - ETA: 4s - loss: 0.6632 - accuracy: 0.6001For batch 1456, loss is    0.64.\n","1458/1531 [===========================>..] - ETA: 4s - loss: 0.6632 - accuracy: 0.6001For batch 1457, loss is    0.64.\n","1459/1531 [===========================>..] - ETA: 4s - loss: 0.6632 - accuracy: 0.6001For batch 1458, loss is    0.64.\n","1460/1531 [===========================>..] - ETA: 3s - loss: 0.6632 - accuracy: 0.6002For batch 1459, loss is    0.64.\n","1461/1531 [===========================>..] - ETA: 3s - loss: 0.6632 - accuracy: 0.6002For batch 1460, loss is    0.64.\n","1462/1531 [===========================>..] - ETA: 3s - loss: 0.6632 - accuracy: 0.6002For batch 1461, loss is    0.64.\n","1463/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6002For batch 1462, loss is    0.64.\n","1464/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6002For batch 1463, loss is    0.64.\n","1465/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6002For batch 1464, loss is    0.64.\n","1466/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6003For batch 1465, loss is    0.64.\n","1467/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6003For batch 1466, loss is    0.64.\n","1468/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6003For batch 1467, loss is    0.64.\n","1469/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6003For batch 1468, loss is    0.64.\n","1470/1531 [===========================>..] - ETA: 3s - loss: 0.6631 - accuracy: 0.6003For batch 1469, loss is    0.64.\n","1471/1531 [===========================>..] - ETA: 3s - loss: 0.6630 - accuracy: 0.6003For batch 1470, loss is    0.64.\n","1472/1531 [===========================>..] - ETA: 3s - loss: 0.6630 - accuracy: 0.6004For batch 1471, loss is    0.64.\n","1473/1531 [===========================>..] - ETA: 3s - loss: 0.6630 - accuracy: 0.6004For batch 1472, loss is    0.64.\n","1474/1531 [===========================>..] - ETA: 3s - loss: 0.6630 - accuracy: 0.6004For batch 1473, loss is    0.64.\n","1475/1531 [===========================>..] - ETA: 3s - loss: 0.6630 - accuracy: 0.6004For batch 1474, loss is    0.64.\n","1476/1531 [===========================>..] - ETA: 3s - loss: 0.6630 - accuracy: 0.6004For batch 1475, loss is    0.64.\n","1477/1531 [===========================>..] - ETA: 3s - loss: 0.6630 - accuracy: 0.6004For batch 1476, loss is    0.64.\n","1478/1531 [===========================>..] - ETA: 2s - loss: 0.6630 - accuracy: 0.6005For batch 1477, loss is    0.64.\n","1479/1531 [===========================>..] - ETA: 2s - loss: 0.6629 - accuracy: 0.6005For batch 1478, loss is    0.64.\n","1480/1531 [===========================>..] - ETA: 2s - loss: 0.6629 - accuracy: 0.6005For batch 1479, loss is    0.64.\n","1481/1531 [============================>.] - ETA: 2s - loss: 0.6629 - accuracy: 0.6005For batch 1480, loss is    0.64.\n","1482/1531 [============================>.] - ETA: 2s - loss: 0.6629 - accuracy: 0.6005For batch 1481, loss is    0.64.\n","1483/1531 [============================>.] - ETA: 2s - loss: 0.6629 - accuracy: 0.6005For batch 1482, loss is    0.64.\n","1484/1531 [============================>.] - ETA: 2s - loss: 0.6629 - accuracy: 0.6006For batch 1483, loss is    0.64.\n","1485/1531 [============================>.] - ETA: 2s - loss: 0.6629 - accuracy: 0.6006For batch 1484, loss is    0.64.\n","1486/1531 [============================>.] - ETA: 2s - loss: 0.6629 - accuracy: 0.6006For batch 1485, loss is    0.64.\n","1487/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6006For batch 1486, loss is    0.64.\n","1488/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6006For batch 1487, loss is    0.64.\n","1489/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6006For batch 1488, loss is    0.64.\n","1490/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6007For batch 1489, loss is    0.64.\n","1491/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6007For batch 1490, loss is    0.64.\n","1492/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6007For batch 1491, loss is    0.64.\n","1493/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6007For batch 1492, loss is    0.64.\n","1494/1531 [============================>.] - ETA: 2s - loss: 0.6628 - accuracy: 0.6007For batch 1493, loss is    0.64.\n","1495/1531 [============================>.] - ETA: 2s - loss: 0.6627 - accuracy: 0.6007For batch 1494, loss is    0.64.\n","1496/1531 [============================>.] - ETA: 1s - loss: 0.6627 - accuracy: 0.6008For batch 1495, loss is    0.64.\n","1497/1531 [============================>.] - ETA: 1s - loss: 0.6627 - accuracy: 0.6008For batch 1496, loss is    0.64.\n","1498/1531 [============================>.] - ETA: 1s - loss: 0.6627 - accuracy: 0.6008For batch 1497, loss is    0.64.\n","1499/1531 [============================>.] - ETA: 1s - loss: 0.6627 - accuracy: 0.6008For batch 1498, loss is    0.64.\n","1500/1531 [============================>.] - ETA: 1s - loss: 0.6627 - accuracy: 0.6008For batch 1499, loss is    0.64.\n","1501/1531 [============================>.] - ETA: 1s - loss: 0.6627 - accuracy: 0.6008For batch 1500, loss is    0.64.\n","1502/1531 [============================>.] - ETA: 1s - loss: 0.6627 - accuracy: 0.6009For batch 1501, loss is    0.64.\n","1503/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6009For batch 1502, loss is    0.64.\n","1504/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6009For batch 1503, loss is    0.64.\n","1505/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6009For batch 1504, loss is    0.64.\n","1506/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6009For batch 1505, loss is    0.64.\n","1507/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6009For batch 1506, loss is    0.64.\n","1508/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6010For batch 1507, loss is    0.64.\n","1509/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6010For batch 1508, loss is    0.64.\n","1510/1531 [============================>.] - ETA: 1s - loss: 0.6626 - accuracy: 0.6010For batch 1509, loss is    0.64.\n","1511/1531 [============================>.] - ETA: 1s - loss: 0.6625 - accuracy: 0.6010For batch 1510, loss is    0.64.\n","1512/1531 [============================>.] - ETA: 1s - loss: 0.6625 - accuracy: 0.6010For batch 1511, loss is    0.64.\n","1513/1531 [============================>.] - ETA: 1s - loss: 0.6625 - accuracy: 0.6010For batch 1512, loss is    0.64.\n","1514/1531 [============================>.] - ETA: 0s - loss: 0.6625 - accuracy: 0.6011For batch 1513, loss is    0.64.\n","1515/1531 [============================>.] - ETA: 0s - loss: 0.6625 - accuracy: 0.6011For batch 1514, loss is    0.64.\n","1516/1531 [============================>.] - ETA: 0s - loss: 0.6625 - accuracy: 0.6011For batch 1515, loss is    0.64.\n","1517/1531 [============================>.] - ETA: 0s - loss: 0.6625 - accuracy: 0.6011For batch 1516, loss is    0.64.\n","1518/1531 [============================>.] - ETA: 0s - loss: 0.6625 - accuracy: 0.6011For batch 1517, loss is    0.64.\n","1519/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6011For batch 1518, loss is    0.64.\n","1520/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6012For batch 1519, loss is    0.64.\n","1521/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6012For batch 1520, loss is    0.64.\n","1522/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6012For batch 1521, loss is    0.64.\n","1523/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6012For batch 1522, loss is    0.64.\n","1524/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6012For batch 1523, loss is    0.64.\n","1525/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6012For batch 1524, loss is    0.64.\n","1526/1531 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.6012For batch 1525, loss is    0.64.\n","1527/1531 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.6013For batch 1526, loss is    0.64.\n","1528/1531 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.6013For batch 1527, loss is    0.64.\n","1529/1531 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.6013For batch 1528, loss is    0.64.\n","1530/1531 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.6013For batch 1529, loss is    0.64.\n","1531/1531 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.6013For batch 1530, loss is    0.64.\n","For batch 1531, loss is    0.64.\n","For batch 0, loss is    0.81.\n","For batch 1, loss is    0.77.\n","For batch 2, loss is    0.72.\n","For batch 3, loss is    0.73.\n","For batch 4, loss is    0.74.\n","For batch 5, loss is    0.73.\n","For batch 6, loss is    0.72.\n","For batch 7, loss is    0.71.\n","For batch 8, loss is    0.71.\n","For batch 9, loss is    0.70.\n","For batch 10, loss is    0.71.\n","For batch 11, loss is    0.72.\n","For batch 12, loss is    0.70.\n","For batch 13, loss is    0.69.\n","For batch 14, loss is    0.67.\n","For batch 15, loss is    0.68.\n","1531/1531 [==============================] - 87s 56ms/step - loss: 0.6623 - accuracy: 0.6014 - val_loss: 0.6035 - val_accuracy: 0.6780\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f27de808050>"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"MilNlwVOGWV8"},"source":["# Stopping the model at highest accuracy"]},{"cell_type":"code","metadata":{"id":"6vM5uzU5GWAq","executionInfo":{"status":"ok","timestamp":1618097213503,"user_tz":-60,"elapsed":614,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["import keras\n","class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n","    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n","\n","  Arguments:\n","      patience: Number of epochs to wait after min has been hit. After this\n","      number of no improvement, training stops.\n","  \"\"\"\n","\n","    def __init__(self, patience=0):\n","        super(EarlyStoppingAtMinLoss, self).__init__()\n","        self.patience = patience\n","        # best_weights to store the weights at which the minimum loss occurs.\n","        self.best_weights = None\n","\n","    def on_train_begin(self, logs=None):\n","        # The number of epoch it has waited when loss is no longer minimum.\n","        self.wait = 0\n","        # The epoch the training stops at.\n","        self.stopped_epoch = 0\n","        # Initialize the best as infinity.\n","        self.best = np.Inf\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        current = logs.get(\"loss\")\n","        if np.less(current, self.best):\n","            self.best = current\n","            self.wait = 0\n","            # Record the best weights if current results is better (less).\n","            self.best_weights = self.model.get_weights()\n","        else:\n","            self.wait += 1\n","            if self.wait >= self.patience:\n","                self.stopped_epoch = epoch\n","                self.model.stop_training = True\n","                print(\"Restoring model weights from the end of the best epoch.\")\n","                self.model.set_weights(self.best_weights)\n","\n","    def on_train_end(self, logs=None):\n","        if self.stopped_epoch > 0:\n","            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdvgvx31HESa","executionInfo":{"status":"ok","timestamp":1618097384199,"user_tz":-60,"elapsed":71643,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"c2c4553b-040a-4b4d-a203-538149ae0acb"},"source":["model1_best = Sequential()\n","model1_best.add(Dense(64, activation='relu', input_dim=74469))\n","model1_best.add(Dropout(0.2))\n","model1_best.add(Dense(1, activation='sigmoid'))\n","model1_best.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model1_best.fit_generator(generator=batch_generator(x_train_tfidf, y_train, 32),\n","                    epochs=1, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32,\n","                     callbacks=[EarlyStoppingAtMinLoss()])"],"execution_count":91,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1531/1531 [==============================] - 71s 46ms/step - loss: 0.6619 - accuracy: 0.6010 - val_loss: 0.6034 - val_accuracy: 0.6800\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f27ded4d990>"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"kb5ZI0VFIRg0","executionInfo":{"status":"ok","timestamp":1618097690611,"user_tz":-60,"elapsed":547,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["import keras\n","class CustomPredictCallback(keras.callbacks.Callback):\n","  def on_predict_batch_end(self, batch, logs=None):\n","        print(\"For batch {}, accuracy is {:7.2f}.\".format(batch, logs[\"accuracy\"]))"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YeBByW-IHtSa","executionInfo":{"status":"ok","timestamp":1618098111117,"user_tz":-60,"elapsed":835,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"416e119a-936d-47a0-901f-99a663658bff"},"source":["result = model1_best.evaluate(x_test_tfidf, y_test, verbose = 0)\n","print(result[1])"],"execution_count":96,"outputs":[{"output_type":"stream","text":["0.6539999842643738\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGazrpF_KpnC","executionInfo":{"status":"ok","timestamp":1618098194193,"user_tz":-60,"elapsed":811,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"584a667a-841b-4137-b209-3551d33e470d"},"source":["val_result = model1_best.evaluate(x_validation_tfidf, y_validation, verbose = 0)\n","print(val_result[1])"],"execution_count":97,"outputs":[{"output_type":"stream","text":["0.6800000071525574\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UG4PJq2vMA-b"},"source":["# Saving the above best model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQfOP07QMF8K","executionInfo":{"status":"ok","timestamp":1618098623637,"user_tz":-60,"elapsed":891,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"e9e1ecb1-2622-48da-fbdd-07b4cd27993d"},"source":["# save model and architecture to single file\n","model1_best.save(\"/content/drive/MyDrive/Data_Mining_Assignment/custom_model.h5\")\n","print(\"Saved model to disk\")"],"execution_count":98,"outputs":[{"output_type":"stream","text":["Saved model to disk\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mIVkK2UOMbvG","executionInfo":{"status":"ok","timestamp":1618098791341,"user_tz":-60,"elapsed":981,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["from keras.models import load_model\n","model1_best = load_model('/content/drive/MyDrive/Data_Mining_Assignment/custom_model.h5')"],"execution_count":103,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ySeAM88NFQ-","executionInfo":{"status":"ok","timestamp":1618098806924,"user_tz":-60,"elapsed":834,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"312badcb-577c-46a6-c154-ee37e50fad00"},"source":["result = model1_best.evaluate(x_test_tfidf, y_test, verbose = 0)\n","print(result[1])"],"execution_count":104,"outputs":[{"output_type":"stream","text":["0.6539999842643738\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KbIYr6WwHI13"},"source":["# Trying batches to train the NN"]},{"cell_type":"code","metadata":{"id":"kL3oPKp89J_G","executionInfo":{"status":"ok","timestamp":1618087786984,"user_tz":-60,"elapsed":693,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}}},"source":["def batch_generator_shuffle(X_data, y_data, batch_size):\n","    samples_per_epoch = X_data.shape[0]\n","    number_of_batches = samples_per_epoch/batch_size\n","    counter=0\n","    index = np.arange(np.shape(y_data)[0])\n","    np.random.shuffle(index)\n","    while 1:\n","        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n","        X_batch = X_data[index_batch,:].toarray()\n","        #X_batch = X_data[index_batch,:]\n","        y_batch = y_data[y_data.index[index_batch]]\n","        counter += 1\n","        yield np.array(X_batch),np.array(y_batch)\n","        if (counter > number_of_batches):\n","            np.random.shuffle(index)\n","            counter=0"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lDOSPWk9ZQX","executionInfo":{"status":"ok","timestamp":1618087928497,"user_tz":-60,"elapsed":139310,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"d3ca842f-ef72-4942-e671-9fcebfe3e581"},"source":["%%time\n","model_s = Sequential()\n","model_s.add(Dense(64, activation='relu', input_dim=74469))\n","model_s.add(Dense(1, activation='sigmoid'))\n","model_s.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/2\n","1531/1531 [==============================] - 70s 45ms/step - loss: 0.6610 - accuracy: 0.5976 - val_loss: 0.6087 - val_accuracy: 0.6580\n","Epoch 2/2\n","1531/1531 [==============================] - 68s 45ms/step - loss: 0.4245 - accuracy: 0.8229 - val_loss: 0.7342 - val_accuracy: 0.6520\n","CPU times: user 4min 2s, sys: 4.34 s, total: 4min 6s\n","Wall time: 2min 18s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r0MBgYVj9pxT"},"source":["# Trying to Tweak the Leaning Rate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lp_87DWt9s3U","executionInfo":{"status":"ok","timestamp":1617474972360,"user_tz":-60,"elapsed":505374,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"fc1f9f27-e4ed-4c6d-8e32-38a82e9df20e"},"source":["%%time\n","import keras\n","custom_adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n","model_testing_2 = Sequential()\n","model_testing_2.add(Dense(64, activation='relu', input_dim=74469))\n","model_testing_2.add(Dense(1, activation='sigmoid'))\n","model_testing_2.compile(optimizer=custom_adam,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_testing_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1531/1531 [==============================] - 73s 48ms/step - loss: 0.6538 - accuracy: 0.6052 - val_loss: 0.6115 - val_accuracy: 0.6700\n","Epoch 2/2\n","1531/1531 [==============================] - 74s 48ms/step - loss: 0.3648 - accuracy: 0.8391 - val_loss: 0.8442 - val_accuracy: 0.6460\n","CPU times: user 4min 16s, sys: 4.01 s, total: 4min 20s\n","Wall time: 2min 27s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ac28iMs-9ze2","executionInfo":{"status":"ok","timestamp":1617475122902,"user_tz":-60,"elapsed":654828,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"5e92600d-040a-478f-938a-8448c697c6af"},"source":["%%time\n","custom_adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n","model_testing_3 = Sequential()\n","model_testing_3.add(Dense(64, activation='relu', input_dim=74469))\n","model_testing_3.add(Dense(1, activation='sigmoid'))\n","model_testing_3.compile(optimizer=custom_adam,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_testing_3.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1531/1531 [==============================] - 75s 49ms/step - loss: 0.6534 - accuracy: 0.6044 - val_loss: 0.6287 - val_accuracy: 0.6540\n","Epoch 2/2\n","1531/1531 [==============================] - 75s 49ms/step - loss: 0.3530 - accuracy: 0.8403 - val_loss: 0.9010 - val_accuracy: 0.6280\n","CPU times: user 4min 21s, sys: 4.58 s, total: 4min 26s\n","Wall time: 2min 30s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAE69Ldf97AU","executionInfo":{"status":"ok","timestamp":1617475271281,"user_tz":-60,"elapsed":801398,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"4ae4c471-566b-4688-abf0-b28d7d964ae8"},"source":["%%time\n","custom_adam = keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n","model_testing_4 = Sequential()\n","model_testing_4.add(Dense(64, activation='relu', input_dim=74469))\n","model_testing_4.add(Dense(1, activation='sigmoid'))\n","model_testing_4.compile(optimizer=custom_adam,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_testing_4.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1531/1531 [==============================] - 73s 48ms/step - loss: 0.6742 - accuracy: 0.5755 - val_loss: 0.6771 - val_accuracy: 0.6420\n","Epoch 2/2\n","1531/1531 [==============================] - 75s 49ms/step - loss: 0.4920 - accuracy: 0.7391 - val_loss: 0.7487 - val_accuracy: 0.6460\n","CPU times: user 4min 19s, sys: 4.2 s, total: 4min 23s\n","Wall time: 2min 28s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBQnvZ_n98eZ","executionInfo":{"status":"ok","timestamp":1617475646743,"user_tz":-60,"elapsed":1174102,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"80f27c7d-fc5f-431f-e1e9-e25b1ed68da7"},"source":["%%time\n","custom_adam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n","model_testing_5 = Sequential()\n","model_testing_5.add(Dense(64, activation='relu', input_dim=74469))\n","model_testing_5.add(Dense(1, activation='sigmoid'))\n","model_testing_5.compile(optimizer=custom_adam,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_testing_5.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=5, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1531/1531 [==============================] - 76s 49ms/step - loss: 0.6698 - accuracy: 0.5916 - val_loss: 0.5981 - val_accuracy: 0.6760\n","Epoch 2/5\n","1531/1531 [==============================] - 75s 49ms/step - loss: 0.4746 - accuracy: 0.8001 - val_loss: 0.6654 - val_accuracy: 0.6700\n","Epoch 3/5\n","1531/1531 [==============================] - 75s 49ms/step - loss: 0.3032 - accuracy: 0.8940 - val_loss: 0.8039 - val_accuracy: 0.6380\n","Epoch 4/5\n","1531/1531 [==============================] - 75s 49ms/step - loss: 0.1804 - accuracy: 0.9437 - val_loss: 1.0007 - val_accuracy: 0.6120\n","Epoch 5/5\n","1531/1531 [==============================] - 74s 49ms/step - loss: 0.1086 - accuracy: 0.9676 - val_loss: 1.1794 - val_accuracy: 0.6080\n","CPU times: user 10min 54s, sys: 11.5 s, total: 11min 5s\n","Wall time: 6min 15s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGrY_Pje-AbC","executionInfo":{"status":"ok","timestamp":1617475876464,"user_tz":-60,"elapsed":1402855,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"3fe2946e-3798-4543-9fb8-caf8cb489602"},"source":["%%time\n","model_s_2 = Sequential()\n","model_s_2.add(Dense(128, activation='relu', input_dim=74469))\n","model_s_2.add(Dense(1, activation='sigmoid'))\n","model_s_2.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_s_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1531/1531 [==============================] - 115s 75ms/step - loss: 0.6589 - accuracy: 0.5952 - val_loss: 0.6080 - val_accuracy: 0.6740\n","Epoch 2/2\n","1531/1531 [==============================] - 114s 75ms/step - loss: 0.4089 - accuracy: 0.8271 - val_loss: 0.7634 - val_accuracy: 0.6440\n","CPU times: user 6min 42s, sys: 5.24 s, total: 6min 48s\n","Wall time: 3min 49s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WqC6OhBwA5L-"},"source":["![ANN Results.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABDYAAAFBCAYAAACMzC+SAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAALZeSURBVHhe7f3Pqts+FPcLa59rSfpA2VfgXEGyJ4UXOu0ZOcMEDoUz6HAPXigHnGHy8MD7m2440EmTK0iuoBSeOveSdy1LcmRbsiXHTuLk+wG3O4ksy1p/tCRL8sv/+l//6zSdTgW4DbvdTqD+b8d+vxf/43/8D/UJPDP/+3//bzGZTNQnMETgT0FXQJduC+ofXBPo22MBeT4vL//zf/7P09vbm/oIrs3v378F6v92TP6f/1P9BYAQ+//r/6f+AkME/hR0BXTptqD+wTWBvj0WkOfz8rJer0/qbwCejv/vv/9X/QWAEP/3p/+P+gsAAAAAAAAwFLKBjTiO1UdwbV5eXsTphLGlW4H6BxrowvCBDEFXQJduC+ofXBPo22MBeT4v/4f6HwAAAAAAAAAAAGBwYGADAAAAAAAAAAAAgwUDGwAAAAAAAAAAABgsGNgAAAAAAAAAAADAYMHABgAAAAAAAAAAAAYLBjYAAAAAAAAAAAAwWDCwcXccxWrykr2q6GWyok8AdAH0qlOOOzHnuqRjskJt3jPH1UTq/ctc7LoWVage3IXewBcA8BjAlgG4HR3YH2LJzsHABuiQopHv1LfD4hHuAfTPP/FH/QWGwh+SWhOh9h+qB9AbAAAAJl3EnYhd74O+Y4hQnk8vMLABOiQVfw/qz8OH+DfIwceh3MN5lPdljiZsmECG98cj+DAAQDtu7ZPRJtwX15JHF+0O2q774BZyqNPT59MLDGyADhmLz5H6M/oq3kbq70HxCPcAAGgH7B8AAMA16aLdQdt1H9ybHJ5PLzCwATpkJBb7kzid6Ngv6NMQeYR7AAC0A/YPAADgmnTR7qDtug/uTQ7PpxcY2BgUR7GbT8RETzmiYzKxb4h33K3EfKI3zXOnzTfW4+lLRz5HpV3J6UzF33diRdfX+b1QfsW9boprufRPYXlIjrs5lVelqRx0Hypd93RzD43pS4LYzdVv5XsjmWh5642FZNqZ2GSfiM1MnTux1mUTzbpi1Eml7u31xd/76GqT/t0FvLmToYvZfaTqNws+tucrQ187Bp6k7XxYRqAeBKdvZTN+/tQX+AJw/5A+rcrxAenXfFXRqa7bVbTrz2PL4fpgv0+/Ntxe32G+vos8JLeLv/siXNfbx14u2yECYwKfMjTraU156JOvL22jRzdjvV6fwO0gEai/NOkpiUT2vYgS+qTZnmL+znpEp3irkhHb2JZGH9EpOWd6SpNIfh9Fp8hMpzLMf6fzCr/nh5mfvexheZjpXUdMtdENnF+Rju+hXK/GYZdZ6d7SJD8/Uhdwy7dYhiL2+/LVFVMmZrlt5QvRVWc9FS5yHfi6FYz7cx3n+/avTx8Z+uYFznDdlGn2J2ZdOvxvoB4Ep29jM/S9/RpNugFf4ANfG9wOe/2TPmndtR5F3T/rdDftqlNHjcNU166vXwW23BVcPpOL9EHdp68cXHI8y8fH13eRh5nedXQXf/cJl9XEvK8mXb9MbnY5mNdxHWdb8y9Ds546yhPoS0P16JZgxsZA2M31iFwkkm3KFitO6VZQQ0McxObdHImLyM/GYpuqdHSkpP2Sg/j4bRlaOxzoF8o7VenXU/WDhn/nPNXvZCn6e2t+Vnzy2ImfS7XTTbwVZCeUNhV5UvrudFqLcumuR2A9lOs10XIQJZn5M11zXlr2RFYn/N1eLILnmfnpyujtK6WUbH6dx7uPvz8oFROJr2rxXpiuKhr17zbsfi7V/XE163tJ8rqo4leffjJsYceggQDbNQjVg+D0bWyGvr/cJ5vAF4D7JtMnZVhRouODop4uv12hXUW7LvN9YFu+SB/y++yqDe/C1/vkce/xd3tCdL07uZ25bSxZpb0v7UIXewYzNm4LiUD9pbGNrhkj5aURb1Iqz9Eyex7n80ujmAp3/rb8mkeMm/M4f2eWJ8+jMOJ4OZxnkS7uoS69kb8x8k2+pPJdhvXJCWO/rhvXqK0Ne97nMp7vKf+upa6ev7Pr3zXhMhQx7qVcZ0652HDVSagMmTbnPA9cL2XCbLfB/3rpwQXpSzKttxmf+7EBX+ADlwPcjmr919gVYWtDbd9ltGxX3baHdv2ebdkHLmsVX30IuU9bnnY5uvWtrzzO35n3k+fRqGP3A5e3SKCNVvCtc9t3xrnlOuw9lgwsD2GrkzA9ui2YsTEEjsZ7jvP1U/IY69HVAke1Rs5cm6VH211E4vNY/Xkn8KiqHP87it8ftvscIiPx6VX9SVK9/auX/HVl+oXcV4Yemd2JXyph9PVNbkoUrKua+9O/wr28fvLcdKmN7bnoMi/QmlA9uCR9kM10DXwBuGMa7Gqcb/1/i3YV7bqdZ7Fl130Otw1/vPh7JN6+ah+xEXLSxlnXRfzFmInSsdxCY4KMHnXnrn3p5WBg4+HgjWLGYjzbiMNhiM5oKr4nyqioAR1nxjwWuu2Mf2C35+4I1JXpl3y62+HjtzgazvH1E6TSre0N3Y7BsIAvAOAxgC3fB0Nswx87/q4sRzF0Pf6ihzXuQW6I/y4BAxtDYPRJ5A8D8vVT5UOtpzr+Fvngapzk66ZO5hqsu+Y8OhxF2gURUZyt6xz+cuuj+HceKhXVuOGKI6TBumI0eoel+PZNr02MRd4mhOjqvWPey59/JLkGurS9wdvxAxGqB5ekv5XNwBeAe6fWrsynyrdoV9Guw5ZLDLINf/D4e/QmzpM2folVvr+Goet9yC00Juhbdy7ypfcPBjYGwVjkM4M278VXih3V67cqr/EhX/RZTzHiV/q8D2L6m9j9VKPDkfj6314ZMx37tVhMh9hqHsRybL4e7Fs++m1OfTtP/TI24OFXwo3PGw458XGUDfjqijninY8kF6bwtdPV+8S4Fwr4fup78ZBLsO3VyHCQdvxQhOrBBenvwGbgC8B9MhXnVRPUATfeL3jM4wbiKu0q2vWcZ7HlC+QxmDb84eLvMsXlKEtttAVdP9Od3EJjgjPBZfDS03BfOiQwsDEIRmLxn949lxrU2fi85mo8FrPNQRz0OiljRPKw1GshKU2uqXdOPi2SAwd1j+rgtWbzu3pZsi/nezmvR41E8v3sMgqBxVLJN3N4vDOy+qFA0THJKYOB77tvoyvmiLfiPIWPCdDVu4fu5cd5fHyj78Ull+D6rJHh0O34oQjUgzbpb20z8AVgAEy/n98ikLeTdPCU7Qx+i4DxWLnfdhXturyfR7blC+QxxDb8IePvIqZNagq63ovcAmOC4DKE62moLx0SGNgYCqOF2KdbkcRkBOqrDLKIONkar5ciA9qnWboznCYV27Nd3THjs4GW4CcJGzLAyaCcK78ajeRmeq7sFU6laZuZfBNhii2Kkyzdj3zOWJHpupxv6LSxNrpSdNDUJJyn8Gm8dXUATNfZK7n85BJen24ZDt2OH4wgPSBC09/cZuALwABo0qf9mkJ8g97aVbTrObruH9SW28tjiG34o8XfFkiPC6pOdlvU9Z7kdrNY0kGoLx0QL/y61zi+Wyt7eHiEjKd6AclxNVFPP/id4EaQcNyJ+VjtCBwlZHTdbGLUV/077+MRMKfP8XrbBwloYIvDBzK8Mg/qCxjo0m25x/pHu/64wN5r9Lun+LtPIM/nBTM2wF2R/s2a1QrH1Hg9Ebgpx3zDJYp/Ko91AADPAnwBAI8BbBkg/gaPAAY2wF1hbrZlrvEbz9STBB5J/g+vfL0dO/EzX+dnma4KAHgS4AsAeAxgywDxN3gMMLAB7orRYi/SLa9DK6z6IqJ8PdpDTf8cGrtf+a7MUTLMHZMBAB0AXwDAYwBbBgTib/AIYI+NG4N1YLcF9Q800IXhAxmCroAu3RbUP7gm0LfHAvJ8XjBjAwAAAAAAAAAAAIMFAxsAAAAAAAAAAAAYLBjYAAAAAAAAAAAAwGDBwAYAAAAAAAAAAAAGCwY2AAAAAAAAAAAAMFgwsAEAAAAAAAAAAIDBgoENAAAAAAAAAAAADBYMbAAAAAAAAAAAAGCwYGADAAAAAAAAAAAAg2UYAxvHnZi/vIgXOiaro/oyhKNYTeT5L5MVfQKgmd38jnTmYhvwwbCT+U59B5o4riayzl7mYtebohxJH/V16JjQtdQv9t/g88rAnkFvXEWeQANbBgAAYGMgMzb+iT/qLwCuwnEl3jfyz+jrmxjJP2/INWxgJN6+RvLPzbtAfB7KH5JSP+zmYzHbHNQn4kDXUvKp+61IcbDjqcJj2DPsuVcQo1wN2DJsGQAAHGApysNzfpqAkX5/jr8/hOwqRuLr2+1Dp2sxWvwQcfbXQXz8RvR0H+zELxXIiygR6ekkTqe9WGRqWfdbmVT81eMfhw/H4MdjAnuGPYM+uH58AVuGLQMAgAsMbABQ4Sh+f6geYPRVPFHsREzFFxk9Ud/39+2n+QJSR+OJ4Oun4hPKut8qjMVn9dDvufQa9szAnsHwgS0zsGUAALCDgQ0Ayhx/i3PsdA9TXa/LWPd+Dx8CD4bui+jzWP1Vpe43yUgs9jyjg4794nn0GvYs/4A9g6EDW5Z/wJYBAMCKdWAj35iJpxbyxkiT0sZ0mUOVG9ZN8u8nYmXdOY/SreZiotd2ZwedN1/ZN9rLrndOO+Hrpeo3K6Vy6HMG7PTzzQiz+l/l9TFZnad6Hnf8vSEXy31LOc6EnqkuNjOVlmRVqJ/Hq8OLSP+qqa78ELwYOhVlsxOr0qaN9rWv92IDqhwN6UafXtVfmPLqW2c5abNO5P6VNxtV32WQrevr6E3osrTjZa6Ph+VYnkv6V/ebHfuGom10+rgr67N5lO7r1sCe1V+w504IkKdPW57L0UOfGvXVroDe+Qf7Jq/4okNgy+ov2PK18Im1c5TeneXIusRbeRs0pAmxQcbHx3RzD0b8UGnj7bHFPQJ56vI9hjytrNfrU5ltLE70Ex3RKcr+Lx/0fWT/PklVJhnbU2xNp49S+jRxXO98RIUTKH9LGnlEp3irkp3SU6LLESX06X7gMpVJk0iVtVT/6obO8rEd5zp1pzPr3bcOHxO+zzJ5/Zf1kzB/c9lG8Zx7sYEafSjbhFmGR1cAA77fMj51dtYJ11GU8TnPmKRnYNS7lrHz+iSXut8oM4vPs/vBUJ1uvt/SfV0Rvn4Z2PNz2vOlcH1VCJRnrl+OtjxUn5z5GUdRxGH5d+KbKjbTDs6rDGwZttwXXKdl3DrOh79+5HrhkSbEBpkmH9PlPZxtrORnHGW7JVyWMpDn+RiaPENoWIpyEIcoFtuUpy+ngipUQd8fIqrnlDVHUOXk35ujyLv5TOjN+qNkqza2o/R5Rgex/HYeDdr9PD991HlTBQude5ks/+yvSCR5+m2+wdLmfYAjTSYHqme+t6z+qd7WU/VDRDrPclH3XKpTLYPpmn/T9UHEW5X+vLngw9dhC9J8h8VXUXooZMCy0bYxABs4GjvJJ1pvUip3XM179InuHATVWU6zToSQ2bAh/7wc5AvqfmuHj07vxM+l0lDyJ1KfU5EnzXzMWrQtQR/AnmHPXREqzxxHWx6qTznl/EjuGrPNbp2/Bz7xRdfAlmHL18Uv1mas+kFy3MZnKfqkac2F/QWmqXyjt6+5Tm5+GTMIBrOhL+T5WPJ0UD9jozTCtI3z0R1ywOpLxhiZzod9jO/Ko85E9Ro16a2jR7ZrSqghU3nr0auBztigo3RrNbjqw11PYXX4mPA9lnHqP+GuF1tdGt/d3AbMvOl65cIUqC/3o8L3W8SvzsJ0gr516ZdrlLxu9Nz5W7sZG83lP39nqlyex431hctQBvZcX25gh+urSKg8TbnZ2vJ6udj01q2vhm376B9hyz/YN5nXqN7gRXCeZWDL9eUG7eE69cMmW7e8z/ikoVSBNljvY1y0vQezfGc7y7+7I53k8vgBecrr3bc8Q+hv89CG3frzTZAoVfbawaDd/Qkzfb62Ux5j/URx8ETCvh/gUcg17uYaK/10IICnqMNQjuJfXikXclc2MBXf9ZOrw0bMxpxuIuar0nrBDOPtGU9NSJ09HzzCL+vBeFPB3QF7hj13RKg8C1ja8lB9qmUk8u0XdPpO878HYMuw5WvjGWsb8nZu4u2T5iIu7C94lm+qX82Tzw44v3L+/jf0hTzLDFuedvBWlMFxFKvJWIxnG3E4POvgQ5+YAeJjMVrsRcpTYSMdGR3EZjkT48rmQKnIZ/w+Of519iwYQTgF7OOsUR0LHavHP+7tbSuwZ9gzeAxgy7Dla/IIsXYP9zD9ki8/y147bHSgyxv63heQp5XBytNNfwMb5lrAP/9Kztl8wqfWStamt2Cmz9d2lo/+1nreDON1ZyJO8nWhJ3Otqy/PWofX4g5tYDSaivV+T9/zOjv15WEpfha3RD4T/GTy8QiuM2+G8qTU5Ky3UR6EE1GcrSNvvb3HEIA9Pzeh8mwiVJ9qMWcz+OhfU/5D9E0BhNZNqOzN9LDlYdIy1j78rX1NToZPmk5ssJd7MGcYLcW3b3o/hlh8uef2H/J0MFB51tDjjI2pOM9wocoyX2ez+5k/4RPxF0rJGFPsMmeu0h9XYmK80vCMkX7zXnzN2VG9YuvBn6pGn3XDxq8Ke29eilJpkFGHNrqbontHNsB5TMzXAY6M+yxNeTNGbJ+akDoL4JyHsdGTU8Z3Rq63kfj6HwfhqmHdr8Viep9hNuwZ9twNofJsIlSfTA5iOT6/VvW4+mZJH57/Rb7Jp8N/IbBl2PItaIy1R2/iay7vmZjnslSyZv3ySUNcZIM1dHIPCnPTyXzmgNVP3SeQ52PJs0J/m4cSxuYo1oM3SlJJM4z8i0eUv162cN2m/POyDHXz0PImWIy5SZjjKG0Uc5anPox8vevwMeF7rGDoYfn2wzYoI+7FBurSlW3CaeePDd9vAc86604nmmVckYfzt/43Dy0fURSd4nL5rgyXowLsOf/tmez5Uri+KgTKs74tJwL16Zyf6yhdJ1Rfnekd+krUxhcXwHlVgC3nv8GWu4XrtEhgrE2ycckxl5VPmkAbrPcxPdxDRjXfsj3eGi5TEcjTPIYmzxD63WNjtBD7dCuSOMpHgzJImnGyFel+XRwRmq6z12eZb8qJ4kRs0734kc/rM2jK/yHnRY/EYp9m93yG79eYvlhiuqY6ojrJiYypp09Zhw2MP+d18efSuWP3YgOU7j9SEFMNKBHlzWUo7otwzOc0D/RVT10RUGdBZDILkPFdMT4/6SzBI/2b5bjwJOAugD2rv57cnrsgVJ5NhOpTDr9ClM4zFSB7BWBp6Who/ln6sPurjS+6Bras/oIt909grE36wfI+75PCSDn+p43SJ00LG3TTwz1kUL4/zAyGsGwB8pQ8ijxrsM3YANeDRKD+ArfAXv/G6GX5icnDY47clp5YPTiwxWacTxNS46noDW3GLkPYs5TZc9nzpdh16bbUP817LGDLZWDLfXKP9n7XmDMR7vDxPuQZyJ3LMwS8FQWACiPxphenHT6EXhb3FBibE0XJd8fTQvCspI4t+Y/pPa/9hj0zsGcwfGDLDGwZ3Jrjb73JpBDxoB/vA+aR5ImBDQAsnDfTMTb8eQLOzg1TXUEVcyOs5Vi/Q/1FjGd6I6xIJP/d2ytfYc+wZ/AowJZhy+DW7MTP8w67w162AIjHkicGNgCwMVoIveQse7ez/PPBMV5zF//Aa35BhdFiL9ItrxfVAxwaXrcp143epd7AnmHP4DGALcOWwW3Z/crfwhElA357BpA8mDxfeI+NODY3DQHXhJ92nrLlYOAWoP6BBrowfCBD0BXQpduC+gfXBPr2WECezwtmbAAAAAAAAAAAAGCwZDM25vO5+ggAAAAAAAAAAAAwHLAU5cZgutRtQf0DDXRh+ECGoCugS7cF9Q+uCfTtsYA8nxcsRQEAAAAAAAAAAMBgwcAGAAAAAAAAAAAABgsGNgAAAAAAAAAAADBYMLABAAAAAAAAAACAwYKBDQAAAAAAAAAAAAwWDGwAAAAAAAAAAABgsGBgA4C+2M2zV07Nd+rzg7Obv9D9zsWT3C54Nq5ozze1JXWfk9VRfQHAg3GhLaOta8NRrCaoNwBAv3gMbGhnREddK3BciQmn4YDIlc7WmLQ9ry10vTmc64UcSSSTXG6Z7CZUn0FxcEgeLa7XlZxb50N2874RIkrE96n6yrQl5zERzv4E2UFuK02dDq+01Xp9mUyqdnbciRWly9PQwbZaznX6PRGR2Ij3R+oQXeyfjlR9JIuS3Fl/V2EGA7qgU3s28bQlkxobfUhbegTgD+6H3myZaMi7D/s8rmQbO6QBxSGW+fHxbYtaxNUljhQbzinv/DoUv85X1diQ8U/L6XSaej/LZa9tY5+SJvlf2A8pEKJDvmk7kP96vT7Vso1PlEwd8Wmrvq6QJqcoTxedklR9b6Lyis1M2p7XgjSJ1HX4qLmXK8JlGRbpKYlUHUbRKY7jU0z/yzp1yK9CSB7h1wuRM6dxcZG+KL2OCgVMT9uEys/3YDmkHbiuY9RDJd8yPmm3pzj7PTpFcXLabrd0JFQO+pyYJaimSyhNlneU0JVM1HUr3w8DvqcKF/knqrtcDlx/Sta5/tJBJw2xru4VrlMX3duzxteWTJps9Ia2pPS53sc8PlwHFeAPrgbXh4v+bNk37+7tU193SHYXVmbt8wLldSX4PoaPb1tktD+ecXWZs51oX2bYTcn5+ac929U2TU+psy1S91l1sjl83vPhI/9L+iEmITrkm7Yb+TcObGxjeWGtiE490o2Frhibw7c1+G3PC+JcqSxc+fd9OFcu05DIHVRJGFoB6xyNJiSPsOuFy5nTVrlcX2S5/RqIjIb6y/MjhyTL5c64Oa2+v+bySfuv2p7+vpx38H3fEXw/FVr7J0OH6IfKOanWq3pZgjCsMuzVnv1tycTHnm9mS0qfn10vrboEf3A1rPV/FVv2y3vIbd1t0PUbJq9rYde3IeHfFkndpbSlwC40jmc7KVxK+cdyGbzTqu/Ol1f3VCqPjD/r9Wj48gzFX/5OAuXvq0PeaTuSf8PAhjEqopWwdIEc43dXZ8ja4Lc9L4TsGlrY9+Vc+b6GQ53h+NZrSB6B12shZ2v9X6wv6hxb0OtA6r7tPhnDDpUtuINfj7TK5poDaJWX7T4Muy3gnff94dYFeZ9B/kl9V6sDOu8g3QJ1uGXYkz230ndPe76VLTX6mOfArUtSdvAH/eKu/z5tOSDvW9nnYGkjr+th1bch4a2PWg62ePNSGenzffprlrTqHsxzMz9r2KvsJLti5TODl2coHfij+n6ISYgOBaTtSP71e2zsfokN/Rd/mQoxehNf6Ypi86txPaNcf0hJ31eW9VNu2p7XyGgh9qe9WIzUZ9CSVPw90H/RV/FWqcuR+PTK//8R/2qFF5JH4PW6kvOl+Rx/iw8u9+snKqUHx5XgJb/2++SlZDOyw1hs164FwWd80h5/f4gDWdpX28VMjv+odh2MPglZ/f+Ktqq+P3z87taG74AQ/7T7xQIl3/lj4dYB0rMf1MOlHMUvrBPtjx7t2duWDLztOcSWjH0deB0r/52tky3o1QVrqp37Ruj1uva9CB4Z+IMb0GfbHJp3UFtHtrc676kjbW9VtBllY5X9KvL9PiyHNkjDPrM9XPI0vLZe5VfIh/c3cJW6Wla5Rt+S3lnm0r5cgXs3gHD826Iu4vgmIvF5rP5spC7tUfzjIFTbK+nwtyXdZfLf5XH+g9EmFinQ0A8p0mM/rkA7+dcObMjGOBY8rsEFeJMjG80bJukG+rAU3/x2IJG0PQ9cB93RdXTYx59ZPw7ibyo/WwnJo4vr3YL0L5WK7NjTs+9+LrP01qBXOZso+S4ahzU806aZlyFvknKg47fxU5WxkNX/l9yWiev7B8DbPyln7NG4Sx3m8SH4u7ulxp6DbSnEnlvY0mb2ImacfxyLWKqWggcgxtlvIk7EdrsV24SU+bARs7HvRmGgAPzB8Ahsm+vxt8/dnGxvScZn2p74aO488sDBeJkNhCbqPKkhEWVFn0u7n27eJ2LMRs72nzmAA/mEsZjwIIPKJ6bfIv5+Sd9X9Fb5CSrrIaJrqHyig87HZ8RtJ+bjmVhuqKYjfT32M2NBfRLQE95tUa9xterEUjk+2TIvYEmrHqDzYPHuyBst/8x0Jnu4nukVxcrxVuwxqlHh0ri+th9Spq9+XEfyrxnY2Ak5rvElD8BGix/kFqkIHiPU0/VWpl3+LI5KN9D2PADuhaOMYsmOfZyvsjPSejmAaEJBxjcy5CgR/zU6ct+0OsjeiOVsKf68fpUB0zahjhAHOzMx1sGLnqV1+BC/mww+p6sR//vEzz+FNO7g3nHbc4AtZYTYM9PGliKRpHuxX6/Fer8XC+VTjqtvMkDY8m8LMZ1OxXSxFvuUZx0cxPInWts2wB8Mi7C2uQlf+1R+gu3etD22z4Zi6IeL2/2abLlos3/EWEzL55OeJelJ2v96L9KEG3D6esNPOVNxonzW9JvMg5v2Yiyv/QQ5Ckq7z9JyPvuT0vPNe+Mg6HH1Th6Rb9e43v6UlwX0QWhb1A9a9ma/0YU97Ugs/qMyi2U2EDae/REx6RFPbgyZufx8XCr/un7INelG/s6BDa10cqREMxVfMu/m09GZiu+ZIwt9JVbb8wAYHucgwPIEV49W+oyghqTNiFUAtJAB03RBHaFUSNPTy830LC3q+IzlqO9utxOrbJrqsz59gX8CZXxsiQi20XDsUzSP4jfPv48oaBkfxZGfhOhDfBKvXM7ykjLgCfwB8ITj5qD1GHpA5LMozC/RS2Asj9TL9j96+5oNYPBAReEpZ/7QwpxtovyEtfOg9fwgPmqD/3MeP0qOaLT4T/pE0COebVEfqGUCdv0pUZd2xGU+8QYZdOzFmvVoN89mG8bbNWnisbDMaTLveOuCQdNO/rX9kGvTgfwdAxtn5/SZqoI7M/qgW6ejyblJ8hkey29BU13bngfAsNiJn8q5l4MA/m0uLTkbrawnJO2Z6kMr/QTqvMZ7tNiLdBuLSI36zmYzsfz4I14THgmmBOWg6wlo9k9qinLAk/ZuniCCW9FsS+1stBvUjIFs2clYjAvHTPCMcdAe+ANQj3oKSW3ocjaW+1V4LflUPqS81EXtfee1nEbvheWF8hOONn0kHZp1QOVMfR6gf3ziuu5RywSof5ik3PmsIyQtcVyJCbWdPAOI205e1rXcvNK5KcWmCd3W8iqzUYZCuPzr+iF3QAv52wc29AZLVBFL6shwZyY/VBTkt2HSVKypU0SpA6e6tj0P9IpuJB1P9+Qar4Z1xCF5dHG9e0YFKLZpe/kI6p9fYj6fn493OZX28PEz+0zxUVDas5PzD7JHUzmVVI6g0sFTVBef2Dw5Aqccn40m/6TruGmdqh5AHrAOPzX+thRmoz0RJWLLwYDt2Pc3i+TxgT8ADainkByIx6QH2dTwl+a9babZFOmNmE3YN+zkpp48QEo60nqTQPCABMR1ncfVR7Ga8DIBHrdvWl4VkpYpL9+USyZ4ZsFiNKLYVO1z9PQzDsPj+pyafoiTq/Xj2snfOrChd1flKS15Z8Y4ZBu+FF5jDtPvahrMu1j9k1950fY80CN1m2XpNVpN64hD8ujievcKGazcglgkpQ3ATA6HjdhsjOPAToB/kJ8/DC/mm3YsK9USZOt1eh6NmvnGpGekwT/JOqaf6x6R6AFkr12owT0Saksh9twdZz8qOBiwHTJhI9jU0gH8AfCAA3E5wFE3EKZRMQJvwEmtbfaQkTcgpc/bNODtLd7UxVtUGunQGmaK1OcB+sO/LeoyruaBCrksOd6eGmYjhqSVZE/nD6TvNQPv2X1D34JjEYlfP6RKiA6117e28rcMbOinBm7FlqPIDY10zkgs5NbhYslO2Zu254H+cL8ZRz+RLIz68RQiXgc1N6ddhuQReL07QU/ZrO0EqPX2riCWl4DYBhX5pc5MtjEXfeZ1syFpGV2+ymsKG8qUo6aG8RPgqi/UTnSoA06+1PsnPT1dbGaWneeJo56OSSrc454L4HLq7NnXlkJtVNKVLTW80Szbb0P93UB5Gnq+2eDTA38wBLzaZm/a2+do+kXqQy1yWUf0+sVYc04+Yr+ubhraCfqpr8VPkH7K6epNM0VcefCrpuEr+sQ/rusijmfMgQq5TMBNSFpFYV8FN9kTfyx9ahfX+8T8t+rHXSL/9XpNvtJgG5/o6xMPqbnZnsgpU7qY/lKkyYmK7jyP4jeZLx2FJG3PCyKlfOJTrI7seiI6Rfq7ZEspbgPf17DQsqcjUvUXaRkZ+sBoXSp/H5JHUNpwOXM+VS7UlwadZrReB+u0qlPqCKkvanCmTU9JXoeRvKcoyj+byWU5VZos3fk8exGUvKKkvo7uEL6vCpf4J31udhj6k9ehpxyBN1ynVfq0Z39bslJrzwG21OgXzn40ipNTst2etlv6P5ZlLZxnzcu4T+WHo+wz1WP2f9kXDx++1wrwB1eD66NKv7YclrenfWbX5HwSsjlpd1rmhWJU7M70LcZB/iUiG0y2xlWd9q/KWFVGlXddvKXbfe3PLPnbrmvoOZezUJd37Cu4vMMmpC0y5az0Oz+3JB8l4/L3ua/T55cPshdNSFqJS291XqT/aXpKSdc4nc1v8vWei/BYRMvF6g41DvnnMuKjSYeC0jKXyb8ysOF1o0QlXW2DQRjOrpCk7XlBGJVqPWwVex34+oMj3eYBsTxko12xG0O21d8882C804bLmb+vcqm+KAfjCnh0vbTp/DsDGAu1aTmIo3rM74nSceBVSppSHjIY0QfXfU3wqO7Nq3x3Bt9fhUv9EznfrJ4rdVita3A5Vhn2bc+etmSlzkZDbMnLL1TLyUFGoYPEuPJiP5wHSvQ7ncun6kCjvg6HB99jBfiDq2Gt/15tOTBvb/tsb3dpwvZGnRTq+MlBEY6F9ECBoWNO+1f3VFFG3Qmy1Rf9VvZntvIyNb6iMGCn9PuefQWXc/gEtEUXxfFaf2qO3MZC0jJ1NspI/dTnuuJR/u35CJG/kquznhVW+St66cddLv8XHtiIY3J94Cbw62pIDuoTuDZ91f9u/iJmG96npo+1sPfLcTUR4+WBfaDfdMM7ArY4fB7JnodsS48A/MFtuXdb7t0+d3Pxot4GUFyiRvD0cF66FG/FCc6hE2DvjwXk+bw4XvcKALiE6fdERMLvtciPg9qfx7r3BgDD5fr2DFsCoA+6seX+7VNv1ml99W/6N9uPxeuVrwAA8ERgYAOAPhi9Cd4vx++1yA+C2tU/+vomnmiSCngGrm3PsCUA+qELW76CfY7evgoqptjMJmIyX4kdv+6VjtWcPmevfI3Fj2eaDgoAAB4McGBjJ+YvL9k0o+ZjTqkBuAVql3zf1yI/ALufvKs/gi3wiFzXnmFLAPTF5bZ8FfscLcQ+3YqYB2E2SzHj173Skb10J96K9LSufVsAAAA8I9hj48bwAAzWgd0O1D/QQBeGD2QIugK6dFtQ/+CaQN8eC8jzecFSFAAAAAAAAAAAAAyWbMbGfD5XHwEAAAAAAAAAAACGA5ai3BhMl7otqH+ggS4MH8gQdAV06bag/sE1gb49FpDn84KlKAAAAAAAAAAAABgsGNgAAAAAAAAAAADAYMHABgAAAAAAAAAAAAYLBjYAAAAAAAAAAAAwWDCwAQAAAAAAAAAAgMGCgQ0AAAAAAAAAAAAMFgxsANAXu3n2yqn5Tn1+cHbzF7rfuXiS2wXPxhXt+aa2pO5zsjqqLwB4MC60ZbR1bTiK1QT1BgDoF4+BDe2M6KhrBY4rMeE0HBC50tkak7bnBXI87sR8MpH3kR0TMV/t6O5AOEcSySSXWya7CTVWQZUZkkeL65FezbtoRFvnQ3bzvhEiSsT3qfqKv9O25DwmwtmfIDvIbaWp0+GVtlqvL2QjFTsj21lRujwNHWyr5Vyn3xMRiY14f6QO0cX+6UjVR7IoyZ31dxVmMKALOrVnE09bMqmx0Ye0pUcA/uB+6M2WiYa8+7DP40q2sUMaUBximR8f37aoRVxdIqhfpeLI8vWqfo/zNNLU+FnOq7aNfUqa5H9hP6RAiA75pu1A/uv1+lTLNj5RMnXEp636ukKanKI8XXRKUvW9icorNjNpe14AaRLl+UdxfIpj/ZmOtpl2BJdhWKSnJFJ1F0VUl1Sf9L+uX6v8KoTkEX69s7z5qNFZgtO4CMmngtLrqFDA9LRNWP/sh7QD13WMeqjkW8Yn7fYUZ7+zTSSn7XZLR0LloM+JWYJqukTbT5TQlUzUdSvfDwO+pwoX+Sequ1wO2vfQkesvHXTSEOvqXuE6ddG9PWt8bcmkyUZvaEtKn+t9zOPDdVAB/uBqcH246M+WffPu3j71dYdkd2Fl1j4vUF5Xgu9j+Pi2RUb74xlXV1B+7uzLDLupOD9LuSgW1r70nPxsV9s0PaXOtkjlV3WyOXze8+Ej/0v6ISYhOuSbthv5Nw5sbGN5Ya20Tj3SjYWuGJvDtzX4bc8LgJ0vC7VYzzpA8TTinuD7GhJ5o18ShlbAOkejCckj7Hpn42F5y7/rDZTTVgnPp4wsd4BuNdRfnh85JFkud8bNafX9NZdP2n/V9vT35byD7/uO4Pup0No/GTpEP1TOSbVe1csShGGVYa/27G9LJj72fDNbUvr87Hpp1SX4g6thrf+r2LJf3kNu626Drt8weV0Lu74NCf+2SOoupS0FdqFxPNtJ4VJ5v6ooY329SrY6vf5BfT6nU/dUOlHGn/V6NHx5huIvfyeB8vfVIe+0Hcm/YWDDGBUpK2AZ43dXZ8ja4Lc972K0EnSdbxh8/eFQZzj6t6ZGKySPwOtluqTT+pXHWv8t8imizrEFvQ6k7tvukzHsUNmCO/j1SKtsrjmAVnnZ7sOw2wLeed8fbl2Q9xnkn9R3tTqg8w7SLVCHW4Y92XMrffe051vZUqOPeQ7cuiRlB3/QL+7679OWA/K+lX0Oljbyuh5WfRsS3vqo5WCLNy+VkT1vZ3yrfZ52luqz6Tuzcw17lZ1kW9mLDF6eoXTgj+r7ISYhOhSQtiP51++xsfslNvRf/GUqxOhNfKUris2vxvWMcv0hJX1f2ddaOWh73mVE4vNY/QkaSMXfA/0XfRVvI/nNmZH49Mr//xH/aoUXkkfg9UYLsT/txaKSNpBL8zn+Fh9c7tdPVEoPjivBS37t98lLyWZkh7HYrl0Lgs/4pD3+/hAH0vuvtouZHP9R7ToYfRKy+v8VbVV9f/j4fUUbvg4h/mn3iwVKvvPHwq0DpGc/qIdLOYpfWCfaHz3as7ctGXjbc4gtGfs68DpW/jtbJ1vQqwvWVDv3jdDrde17ETwy8Ac3oM+2OTTvoLaObG913lNH2t6qaDPKxir7VeT7fVgObZCGfWZ7uORpeG29yq+QD++F4Cp1taxyjb4lvbPMpX25AvduAOH4t0VdxPFhjD+zpzyIj9/FTGWZqSjOTthR/OMgVNsr6fC3Jd1l8t/lcf6D0SYWKdDQDynSYz+uQDv51w5syMY4FjyuwQV4kyMbzRsm6Qb6sBTf/HYgkbQ9rxWqsqlp+tQoRJChO7qODrt2Xn9T+dlKSB5dXO8WpH8bnHWR3c9llt4a9CpnEyXfReOwhmfaNFN88iYpBzqeGz9VGAtZ/X/Jkkxc3z8A3v5JOWNqZJpUQOowjw/17e9Aa2rsOdiWQuy5hS1tZi9ixvnHsYilail4AGKc/SbiRGy3W7FNSJkPGzEb+24UBgrAHwyPwLa5Hn/73M3J9pZkfKbtiY/mziMPHIyX2UBoos6TGhJRVvS5tPvp5n0ixmzkbP+ZAziQTxiLCQ8yqHxi+i3i75f0fUVvlZ+gsh4iuobKJzrofHxG3HZiPp6J5YZqOtLXYz8zFtQnAT3h3Rb1GVfvfkoZxz8KHc/R4j+RULYH1rn5SuyOPHhGusqJ4634TydWD9B5sJjTHFV+2cP1TK8oVqb0e4xqVLg0rq/th5Tpqx/XkfxrBjZ2Qo5rfMkDsNHiB7lFKoLHCPV0vZVplz+Lo9INtD0vlOPqnVwtYdwfAF1wlFEs2bGP81V2RlovBxBNKMj4RoYcJWfH78Q3rQ6yN2I5W4o/r19lwLRNqCPEwc5MjHXwomdpHT5EaaC9hn5G/O8FP/+EQdNHwm3PAbaUEWLPTBtbikSS7sV+vRbr/V4slE85rr7JAGHLvy3EdDoV08Va7FOedXAQy599traPC/zBsAhrm5vwtU/lJ9juTdtj+2wohn64uN2vyZaLNvtHjMW0fD7pWZKepP2v9yLl3iR/veGnnKk4UT5r+k3mwU17MZbXfoI7mycqH6flfPYnpeeb98ZBUB1bF663P+VlAX0Q2hZ1xHEndjs+VnKGjhxVF9XZiCOx2KdiG5PmbpbZINeMFC3TEUp7VmNK9x+VWcg049kfEVMazi5k5vLzcan86/oh16Qb+TsHNrRzkiMlmqn4knk3n47OVHzPHFnoK7HanheAms6SNRgwEnBDzkGA5QmuHq30GUENSZsRqwBoIQOm6YI6Qmk2qn5ebqZnaVHHZyxHfbkRW2XTVJ/16csV/BMYGD62RATbaDj2KZpH8Zvn31PA+WV8FEd+EqIP8Um8cjnLS8qAJ/AHwBOOm4PWY+gBkc+iML9EL4GxPFIv2//o7Ws2gMEDFYWnnPlDC3O2ifIT1rhY63l1OUGRcx4/So5IP7UHfeLZFnXE8fe7mM1mdCzlDB0iev3MalDl+Fv8+sNpIhFFUhEOy28UV5YSj7jMJ94gg469WLMe7ebZbMN4uyZNPBaWOfEMEHheTTv51/ZDrk0H8ncMbJyd02eqCjkiJw9WymbnJslneJDyhrT5bc/zQ01noftIUq4kAG7FTvxUA2zlICDTU2nJ2WhlPSFpz1QfWuknUOc13qPFXqTbmBoiOerLjdjy4494TXgkmBKUg64noNk/qSnKAU/au3mCCG5Fsy21s9FuUDMGsmUnYzEuHDOh4lHQEvgDUI96Cklt6HI2lvtVeC35VD6kvNRF7X3ntZxG74XlhV4Lb2/TR9KhWQdUztTnAfrHJ67rCo4PZQeUjzSLFSlQpHamtLwxX1KViC3PKNzTeemWOtwcV5JN1M0mOK7EhNpOnt3BbScv61puXqn/xtdLsuv1MhtloITLv64fcge0kL99YENvsEQVscxG44xDRUF+GyZNxZoVnZ/4Bk11bXteE0exmvB0Fo4vL9iA6lnRjaTj6Z5c49Wwjjgkjy6ud8+oAMW2HCofQf3zS8zn8/PxLqfSHj5+Zp8pPgpKe3Zy/kH2aCqnkuYNGE9RXXxi8+QInHJ8Npr8k67jpnWqegB5wDr81PjbUpiN9kTEQSUFA7Zj398skscH/gA0oJ5CciAekx5kU8Nfmve2mWZTpDdiNmHfsJObevIAKelI600CwQMSENf1FlePZKxY8YVqUJ80f0vtTL58ajTNlqfI2QSuJU7l5ZtyyQTPLFiM+Hpqn6Onn3EYHtfn1PRDnFytH9dO/taBDb27Kk9pOY/GnQ+pt0vhNeYw/X5W3H/yKy/anueEBzXk9Pl4e7rBU7NHoG6zLL1Gq2kdcUgeXVzvXiF9lFsQi6S0AZjJ4bARm41xHNgJ8A/y84fhxXzTjmWlWoJsvU7Po1Ez35j0jDT4J1nH9HPdIxI9gOy1CzW4R0JtKcSeu+PsRwUHA7ZDJmwEm1o6gD8AHnAgLgc4yp0/GypG4A04qbXNHjLyBqS8h0Hax4O5uniLSiMdWsNMkfo8QH/4t0VXiqt1Z1NvHmmdxaM75Hayp/MHOSDiKk5239C34FhE4tcPqRKiQ+31ra38LQMb+qmBW7HlKHJDI50zEgu5dbhYslP2pu15NsxBDTmdBbTB/WYc/USyMOrHU4h4HdTcnHYZkkfg9e4EPWWzthOg1tu7gtjiFD/jyAIiOo03XaLPvG42JC2jy1d5TWFDmXLU1DB+Alz1hdqJDnXAyZd6/6Snp4vNzLLzPHHUS+JIhXvccwFcTp09+9pSqI1KurKlhjeaZfttqL8bKE9DzzcbfHrgD4aAV9vsTXv7HE2/SH2oRS7riF6/GGvOyUfs19VNQztBdzItfoL0U05Xb5op4sqDXzUNX9En/nFdF3E896n4deIlPWE5y54qmYWazauf2Fs7tjU2VNhXwU32xB9LnwLkb+AT89+qH3eJ/NfrNflKg218oq9PPK3BzfZETpnSxfSXIk1OVHTneRS/yXzpKCRpe14AeR5RfIpjy5G0zLgDuFzDQsveqM9IfTb1gdG6VP4+JI+gtCldUqWhI9MrEZ0i/R3JOVUpNZxPlfB8CjToNKN1MlinVZ1SR0h9UYMzbXpK8jqM5D1FUf7ZTC7LqdJk6c7n2Yug5BUl9XV0h/B9VbjEP+lzs8PQn7wOPeUIvOE6rdKnPfvbkpVaew6wpUa/cPajUZycku32tN3S/7Esa+E8a17GfSo/HGWfqR6z/8u+ePjwvVaAP7gaXB9V+rXlsLw97TO7JueTkM1Ju9MyLxSjYnembzEO8i8R2WCyNa7qtH9Vxqoyqrzr4i3d7mt/Zsnfdl1Dz7mchbq8Y1/B5R02IW2RKWel3/m5JfkoGRe/t1wrl3M1jzRR5WCdIjvK7CDRbUiI3nJxZP5Jmp5S0jVOVz3/EeQZSoj8JV79EKv8mQAdCkrLXCb/ysCG140SlXS1DQZhOLtCkrbneeNoHMzjhp0wvv7gSLd5QCwP2WhX6tCQbfU3zzwY77SG8ViPqgHx91XC8ymidM6lV7pe2uidM4CxUJuWgziqx/yeKB0HXqWkKeWhGx95cN3XBI/q3rzKd2fw/VW41D+R883quVKH1boGl2OVYd/27GlLVupsNMSWvPxCtZwcZBQ6SIwrL/bDeaBEv9O5fKoONOrrcHjwPVaAP7ga1vrv1ZYD8/a2z/Z2JzuERmeQjsToQOY65rR/dU8VZdRxsa2+6LeyP7OVl6nxFYUBO6Xf9+wruJzDJ6AtujiOp2sZgxPZoQYu7JfjAT3zei69am5vzXK74lH+7fkIkb+Sa1M/xCl/opd+3OXyf+GBjTgm1wduAr+uhuSgPoFr01f97+YvYrbhfWqea5Pa42oixssD+8DBLfmCLQ6fR7LnIdvSIwB/cFvu3ZZ7t09+k8RMvg2guESN4OnhvHQp3ooTnEMnwN4fC8jzeXG87hUAcAnT74mIhN9rkR8HtT+Pde8NAIbL9e0ZtgRAH3Rjy/3bp96s0/rq3/Rvth+L1ytfAQDgicDABgB9MHoTvF+O32uRHwS1q3/09U1uGgXAo3Bte4YtAdAPXdjyFexz9PZVUDHFZjYRk/lK7Ph1r3Ss5vRZvT7zxzNNBwUAAA8GOLCxE/OXl2yaUfMxp9QA3AK1S77va5EfgN1P3tUfwRZ4RK5rz7AlAPriclu+in2OFmKfbkXMgzCbpZjx617pyF66E29FelrXvi0AAACeEeyxcWN4AAbrwG4H6h9ooAvDBzIEXQFdui2of3BNoG+PBeT5vGApCgAAAAAAAAAAAAZLNmNjPp+rjwAAAAAAAAAAAADDAUtRbgymS90W1D/QQBeGD2QIugK6dFtQ/+CaQN8eC8jzecFSFAAAAAAAAAAAAAwWDGwAAAAAAAAAAABgsGBgAwAAAAAAAAAAAIMFAxsAAAAAAAAAAAAYLBjYAAAAAAAAAAAAwGDBwAYAAAAAAAAAAAAGCwY2AOiL3Tx75dR8pz4/OLv5C93vXDzJ7YJn44r2fFNbUvc5WR3VFwA8GBfaMtq6NhzFaoJ6AwD0i8fAhnZGdNS1AseVmHAaDohc6WyNSdvzAjked2I+mcj7yI6JmK92dHcgnCOJZJLLLZPdhBqroMoMyaPF9Uiv5l00oq3zIbt53wgRJeL7VH3F32lbch4T4exPkB3kttLU6fBKW63XF7KRip2R7awoXZ6GDrbVcq7T74mIxEa8P0iH6LiS9+yqPxncGn7J8GW2o5JP2Sdldf8YdXe3dGrPJp62ZFJjo49mS2DoXN7mdx6D9WbLREPefdhnU3tzjwyxzI+Pb1t0uU2H5OFv/5zOyM/ViKr2s7aNfUqa5H9hP8RE9Q3K8l9ZlchXVzqQ/3q9PtWyjU+UTB3xaau+rpAmpyhPF52SVH1vovKKzUzanhdCfg/RKYrjUxxH6jMdrTPtBi7DsEhPSaTqLoqoLqk+6X9dv1b5VQjJI/x6aWLIt05nCU7jIiSfCkqvo0IB09M2Yf2zH9IOXNcx6qGSbxmftNtTnP3ONpGcttstHQmVgz4nZgmq6RJtP1FCVzJR1618Pwz4ngrkvskiE/2bea/5d0pPS0exWrW8ST7q9/xzrWxBHRUZGnRvzxpfWzJpstEb2pJqL59dD7kOAGPoqmcbXOZse2wj7O8MW3TEYPybi/5s2Tfv7u1TX3dIdhdWZq1HgfK6Enwfw8e3LbrcpoPy8O6Dne1qm6an1NkWqfus6b/xec+Hj/wv6YeYWK5F+eo4tigaX13pRv6NAxvbWF5YK6JTj3RjoSvG5vBVIYt63PK8ANj5slAL+epOyI2dLN/XkMgb/ZIwtALWORpNSB5h1zsbD8tb/l0vX05bJTyfMrLcvg0E0VB/eX7kOGS53Bk3p9X311w+af9V29Pfl/MOvu87gu+njOs+rfWifYpDhibar5brieuvTragHpsMz/rehz3725KJjz3fzJaUL3p2PbTr0vMh9bDq10LbfHcMZtdxe/1fw5b98r6ZfQ4WXb9h8roWdn0bEv5tUVc27ZtHvf0b+qC+O5+q7ql0DRk/1evR8OUZir/8nbSQfyWpJQ721pWO5N8wsGGMilgKW8D43Rr0M+omCt+3Pe9iOlCCDuD7Gg51daZ/a2q0QvIIvF6mSzqtX3ms9d8inyLqnICnOa6OrsSwQ2UL7k6HR1plc80dF5WX7T4Muy3gnff94daFUh3YvmNcdVKhjU4BH65uz6303dOeW+XdAY0+5jmw6tLToe3Fsw0OQp9vj+1uY8sBed/KPgfLpfrSL4O3d2991HK4xKZ7ykPdg+kPstjYsFfZSbZdt8jT+e8O/FF9P6SIM60qxzkODtCVjuRfv8fG7pfY0H/xl6kQozfxla4oNr8a1zPK9YeU9H1lWT/lpu154Fqk4u+B/ou+ireR/ObMSHx65f//iH+1wgvJI/B6o4XYn/ZiUUkbyKX5HH+LDy736ycqpQfHleAlv/b75KVkM7LDWGzXrgXBZ3zSHn9/iANZ2lfbxUyO/6h2HYw+CVn9/4q2qr4/fPx+DBsmXfhBvVBxWIqfyvHtfi6p/sgv/lj4ydfJRvxqcqbgcnq0Z29bMvC25xBb2p33oeJ1rPx3tk62oF8XrKk28i+i1+va9yIAQ6eLNr+JSHweqz+b6LNtDs07qK0j21ud99SRtrcq2oyysepeTHq/D8uhDdKwz6Naey7T8Np6lV8hH97fwFXqalmd+z85y1zalyt47wYQin9bdO04/hKO4h8HodpeSYe/Lekuk/8uj/MfjDaxSIGGfkiZ8WfurR/Ex++ikGU5KJvcqV+iK+3kXzuwsfuVDWsIHtfgArzJkY3mDZOMzsA3rx1IFG3Pa8Pup6D64d4JDMQX3dF1dNi1ov9N5WcrIXl0cb1bkP4tGXY9tR1l5Wyi5LtoHNbwTJtmXoa8ScqBjs9mTjbGQlb/X3JbJq7vh0txwHUnMrdYt/FcIyOxyBwd5TmTmyP17O3AJdTYc7AthdhzC1tifZpx/nEsYj43hwcgxtlvIk7EdrsV24R08LARs7HnRmHg+ei1DVYBL9nPJ1vmfRDYNtfjb5+7OdnekozPtD3x0dzx44GD8TIbCE3UedKsI8qKPpcaoc37RIzZyNn+MwdwIJ8wpjaGfJPKJ6bfIv5+Sd9XDF/5CSrrIaJrqHyig87HZ/hyJ+bjmVhuqKYjfT32M2MZc4Ne8G6Lrh3Hu7D1wdQDdI61dsejOKo02cP1TK8oVo63Yo9OW4VL4/rQB3ajxX8iIVkd2I/Mpbx2K/I/mcC24j8toxBd6Ur+7qUoxnTZnJCp6SqtOR3JtqSk7XmhpLyJitxI5bz5YdNUqf7hcgyGiqyKONdcmYTkcdH1/KbDNde/77S6M171kGPR9xx1bdPenNPEfdPq+5EHb/pDAZPcYEh/nxdcp/WYQmYQMp3tnuB7dyHviWRat9eQ1le9OZJ5bKuVwesLs/TqyDZgGlid3Rt1MpS49daF255DbIlR6b3sWeJtSyofV1rnPdh8rK1M6jt3HfjX51Dg+316umjzHehzXXk313+XtlzGL28/+7TYvQ2L3cn8S2VQMrHZZ7kseR2X0+dtVbFMbpnoOKV0r5Yy6zwK1yPOZblPX8FlGy5aX+VR2xZ1YdNt8vDtg1HeeZlJ32KlR1ZbqIHPfx4C5G+lrh9SR0py0Xatrl2y+2Bd6UD+zhkbx9W7yB7uyOkaiqn4QncvDh+iNPvEwlR8p5oWPjM8CrQ9r57j73cxm83oWMqRZCJ6/Uw/ZH8CcBO0nVmf4OrRSp8R1JC0GbEgfyH264VYTKdiOl2I9T7NRmDPy830LK2DWI7lqO9utxOrbJrqcz19yWdtZE+hGmZr8NOtzaZ4/Ko+uhhN12J/SvOncIfNMnt67vVQDNwRPrZEBNtoOPYpmkfxm+ffR7H4Mj6KIz8J0Yf4JF65nOUlZQD0iZpSzLbjs8TyIeC4OWg9hpqGHX0WhfklegmM5XF42f5Hb1+ztoU6FcWnnHppeWG2ifITVpnouLw69bzIOY8fJUekn/CCPvFsi26Adx9sxGU+8agEHXuxZj3azbPZhvF2TZp4LCxz4tkCaLs07eRf2w+p4/hb/PrDsoxEFEnjPiy/UV/hAol0IH/HwMbZOX2mquDOjD74Bpqdm2S0+EE5yBsNuc+259UxWuxVRfGRinRLV1AdiQ7HTwAIYCd+quCuHATwb3NpyaI57gtJe6Y6/Vevdzvv/cB2w7YSRTx1lRulmVh+/BGvSSKnupeDrkdFL5MjGjumJIezr1GHUzAjMV3oAQ7pWzcz7FcwNJptqZ2NdoOa8p8tOxmLceGYCRVjAnAl1JRiiiWTlAPVR2ckFv9xe3kQy9lY7lfhteRT+ZDyUhe1953Xchq9F5YXei28vU0fSYdmHVA5U58H6B+fuO4WtO6DHVdiQm1nlKRZ28nLupabV/IdnEeS5THG06CccPnX9UNqyJfJJWKb7sV+T/JNtyLJ+grk57qSSQv52wc29AZLVORlNsJmHCoK8tswaSrWrLz8xFfvvOdF2/N8Gcmnpb1e4wHRjaTj6Z5c49WwEVhIHl1c755RAYqIv1SCu3wE9c8vMZ/Pz8d7tlqN7O9n9pnio6C0Zyfnv7FTZiv5CCod5MDWi09sOs51c4+IXAvYFzzAsReZSyJpYlPRIeBvS2E22hMRByAUDNiOfX+zSMCA6bwN5j0cePNcHuPrYKPvoaCeQnIgHlPDyQ8Jxi/ND9Wm2RTpjZhN2Dfs5KaePEBKdd56k0DwgATEddeO46349sHIX3xbigO1XXLPBrnHGc8sWIw4D/XAyVGO5yE8rs+p6Ye4UQ9qyJttKXaYalc0mopFPkPkXfq3i3SlnfytAxt6d1We0nIeYTsfUhfPbwmoZfr9fJP/5FdetD2vDU9vFL7UbZalNlUkFa7fCCwkjy6ud6+QwcotiEVSs67hcCgtaTiwE+Af5OcPw4v5pnVv7KSmvjY1aoz5xiQAnpRQWwqx5+44+1HBwYDtkAkb+dNL+cD90mUbzIMacgljvD3dYObS7eFAXA5wNHXoGBUj8Aac1NpmDxl5A1L6zE9Iux8UqpM1lUY6tIaZIvV5gP7wb4u6sOku/QLh6INlT+cPsvPsyiq7b+hby7jerx9SQW8Iap2ZpQdZNO11pa38LQMbehmKWynlKLIQG6/HivoNAOTE2Sl70/a8MtyY8mvvymZzVG99IZ7oqfNluN+Mo59IFkb9eAoRr4Oam9MuQ/IIvN6doKds1nYC1Hp716uVitP2jEM+0s+mZfFnXjcbkpbR5au8VrmhTDlqahg/Aa76Qu1EAxq1Z4Prz/b6u6N28vpNVOAeqLNnX1sKtVFJV7bU8EazbL8N9XcD5Wnox9U3eZ/gQemizWfMQQ05pfgWeLXN3rS3z9H0C3n5JuSyjuj1i7HmnHzEfn1+QtopukNi8RPUNsnp6k0zRVx5ULw9h6/oE/+47tpxfMs+WGFfBTfZE38sfWoX1/vE/Db561kY1sGKsl9s2Y+7RP6Vt6Lo3ZVDd1Bt2PmU4je1y2l5p9yW53lj7hZ7fltBds3sCN0Ftlu4DMNCy56OSL3xIa/fUl3mO3WX6zggj6C0vEOvSkOHlHF0ivR3yZZSFOF8qoTnU6BBpxmt18E6bdmF3IkzrcUmIr2zcXHXc1lO4y0fxnn2Iih5Ne0Af4fwfdXRuFu4lrvtrSh0JPqVJzpdllb/ft5Z2ku2wIpdhn3as78tWam15wBbavQLZz9q7piud6cvnGfNy7hPpbNR9pnqMfv/tu1oH/C9AiagDVa6U/4+j+Nyf1c6yAbL2Ou/X1sOy9vTPrNrcj5J/lYIXX+FYlTsrvimg/wg/xJRPebtCeO0f1XGyv3qvOviLd2O1bRNtusa7RuXs1CXd+wruLzDJqQtutym/fOwlCvXiXJajUtvuTjynCRNTynpGqer6v0z+u8Q+Uu8+iEO+edvOWI/Qb4x822JjgvKMgnQt4zL5F8Z2PC6UaKSrrbBIAxnV0jS9rwgqMEyKjw7lDAssr4qXJbBkW7Pr2vKDtloV+rSkG31N888GO+0hvFYj6oB8fdVwvMpohyMK+DR9dKm8+8MYCzUpuUgjuoxvydKx4FXKWlKeRTsJqv7GrtR9+ZVvjuD768O74ENx1GoE6XThfonh78lZw3aw/VYpWd79rQlK3U2GmJLXn6hWk4OMgodJMaVF+tsHijR75m+cnL+3FSHw4PvESguavPNgNtxWGyLv6/Spy0H5u1tn+3tTrY5RseBjsToFOZtkdP+1T1VGi0tE1t90W+WtqlSXqbGV5w7LvS78of37Cu4nMMnoC26yKYVvnlwubz7YM3trXlNVzzKvz0fIfJXcm3qh9TIP80GaU350/VcviJAVy6V/wsPbMQxuT5wE/h1NSQH9Qlcm77qfzd/EbMN71PzRBukEcfVRIyXB/aBg1tDDVscPo9kz0O2pUcA/uC23Lst926f/NaBmXwbQHGJGsHTw/nNMvwGLjiHToC9PxaQ5/PieN0rAOASpt8TEQm/1yI/Dmp/HuveGwAMl+vbM2wJgD7oxpb7t0+9WeerbQOP9C/dAV2+cZdvAAB4LjCwAUAfjN4E75fj91rkB0G9Jjr6+lbdCAqAIXNte4YtAdAPXdjyFexz9PZVUDHFZjYRk/lK7Ph1r3Ss5vRZvWrxxzNNBwUAAA8GOLCxE/OXl2yaUfMxp9QA3AL1Vh/f1yI/ALufS3FAsAUekuvaM2wJgL643JavYp+jhdinWxHzIMxmKWb8ulc6spcExluRnta1bwsAAIBnBHts3BgegME6sNuB+gca6MLwgQxBV0CXbgvqH1wT6NtjAXk+L1iKAgAAAAAAAAAAgMGSzdiYz+fqIwAAAAAAAAAAAMBwwFKUG4PpUrcF9Q800IXhAxmCroAu3RbUP7gm0LfHAvJ8XrAUBQAAAAAAAAAAAIMFAxsAAAAAAAAAAAAYLBjYAAAAAAAAAAAAwGDBwAYAAAAAAAAAAAAGCwY2AAAAAAAAAAAAMFgwsAEAAAAAAAAAAIDBgoENAPpiN89eOTXfqc8Pzm7+Qvc7F09yu+DZuKI939SW1H1OVkf1BQAPxoW2jLauDUexmqDeAAD94jGwoZ0RHXWtwHElJpyGAyJXOltj0va8S6D88msieGvBkapwktdhVo8TaqyCqjIkjxbXI72ad9GIts6H7OZ9I0SUiO9T9ZVpS85jIpwqGaK3Xmmr9foymVTt7LgTK0qXp6GDbbWc6/R7IiKxEe8PYlPHlbxnV/3J4NbwS4Yvsx2VfKhe51TfeZqs7h+j7u6WTu3ZxNOWTGps9NFs6WG4OF45ktmT3EvtALdnK9h+GL3ZMtGQdx/22dTe3CNDLPPDUht/WPTYM66rxTePoLJxXHT+vc7Pcp6un5+XsLjeTBfeDvXRj+tA/uv1+lTLNj5RMnXEp636ukKanKI8XXRKUvW9icorNjNpe15r0lMS6euJU2S94PXgMgwLo/6i6BTH8Smm/2V9OuRXISSP8Oulif6djxqdJTiNi5B8Kii9LupXetomVH6+B8sh7cB1nRC99Um7PcXZ79EpipPTdrulI6Fy0OfELEE1XUJpsryjhK5koq5b+X4Y8D0VyH2TRSb6N/Ne8++UnpaOYrVqeZN81O/551rZgjoqMjTo3p41vrZk0mSjN7Ql1d4+ux5yHVTIfYKUt10dXPEK6Ukuc9YV5Rvy9owOOum5a/0M14eL/mzZN+/u7VNfd0h2F1Zm7fMC5XUl+D4Gjdbr6BxznI+yXwmJ61wE5OFdtrNdbdP0lDrbInXtmk7h4OXZCt9YxJKO+ia6baupVgMjhmnsm/mm7Ub+jQMb21hemCuGL+C8Ya24VOCscmzGYWvw257XEumIqSJJiPYKuy5chiGRN/olYWgFrHM0mpA8wq53Nh42Yvl3fSPKaauE51Mm1zNf9WqovxC9bU6r76+5fNL+q7anvy/nHXzfdwTfTxnXfVrrRfkylwxNtF8t1xPX36190pCxyfCs733Ys78tmfjY881sSfmiZ9dDqy61jlcMHaQfKuekWi9R7xpr/V/Flv3yvpl9DhZdv2HyuhZ2fRsQyjd5hB/2+IVwxTs2gvLwLVslndKZ0onyGvV6NHh5BqPtq9knSd9lkUdADKvzKKe9qB/XkfwblqLsxK8N/Rf/EOvvXwVdUGx+1c/7OYgv4kfMfyzFz4ApQm3PC2Mnfi4P2f0sPqmvQABH8fuD6o80ISnN4RxNvwtSQFaQ0rSyMiF5BF7v+Ft8HChtehL7xVh92YKL81Hljr6Kt5H6qoFdZmjV+5SE6K1H2uz+6GrJf2JRWz5l/5Ypu3IqLpnr31R+oRi9sZ84iI/fQRMa75b8Pj9+k1QVx5Xgmcy2evHjKP794f9fxadS/Y8We9I5T6UBfvRpz962ZOJnz49mS49EcLyy+ylY5Owz/ltPRUVVRlOx+E/5muXPc5sGivRuy/55wz7BMAmP66p0kYcPI/Hplf778y+Pv3j502zDdroWrcKvRyUgFkn/Zo2R+Ny6m9RjP65AO/nXD2zsfgnW3fgLnT56E19rC3BGK/bmfZUXxoe25/mym8/ofmKxpcACtCEV0h5sHXalgOKP+FcrvJA8Aq83Woj9aR/QwXBwaT7KwYjXT1RKD/KOsu0+w/TWJ+3x9weFY5H4aruYyfEf1a6D0SfqlhOGw8lQ3xcGAoYM6UK5A7P7uaT6437pwk++TjaiYZwYdEGP9uxtSwbe9hxiS8a+DryOlf/O9usp6FfIetgSzn0j9L5BljXcD05IvCIHrht8hvY18Atu+mybQ/MOauvI9lbnPXWk7a2KNqNsrLoXk97vw3JogzTsM9vDJU9j7NtUyIe+d+6LUS2rc/8nZ5lLey/4+hnQP23iujJd5OGFegik7ZV0+NuSWtygBwnPQUgsMv7MLVd1UFbmQV2RxhGPHvtxBdrJv3ZgQzbGseBxDS7AmxzZaN4wyegMfAvZVKjteT6ozmOUfMcoX1u0M3N02LWx1A7UhuTRxfVuQfrX0zlIajvKIXrrmVaO1pI3STnQMTd+4mDHd/OosZDV/5fclonr++FS7MC4n1T4MxIL2YMRm5ncHKljbwe6pMaeg20pqB0KtyXWpxnnH8ci5nNzeABinP0m4kRst1uxTUgHDxsxG9dsWAzceMcreoZW8xMy2aZxnwAC6YXAtrkef/vczcn2lmR8pu2Jj4aHQAQPHIyX2UBoos6TGhJRVvS51Aht3idizEbO9p85gAP5hDG1MeSbVD4x/cYzTTZL+r6it8pPUFkPEV1D5RMddD4+I247MR/PxHJDNR3p67GfGctZS6BX/vzbid1OHcdQP9JF/ObOo7Fs6gE6x1r8+1HNdMsermd6RbFyvMWsVgshscho8V82U+LAPmAu63q3It+RVfZW/NdUv3314zqSf83Ahl6G8iUPwEaLH+QWqQgeI9TT9VamDZxW2fa8eshZf6MK4WmgMAjQM0cZxZId++iasjPSejmAaBKit75pdZC9EcvZUvx5/SoDpm1CHSEOdmZirIMXPUvr8CH8Z9v6ztwZEEYH5uf8nWqOpFX35PXPu5jP58Wj/LhquhbpVgaphw3VOTVAsoGRP4P7wW3PAbaUEdoOtbElnqK5F/v1Wqz3e7FQPuW4+iYDhC3/thDT6VRMF2uxT3nQ7iCW/a3/fGj84hX1xIqCTq8mAfRGWNvchK99Kj/Bdm/aHttnQzH0w8Xtfk22XLTZP9SBnJbPJz3LltKw/a/3Is3meXMbw085U3GifNb0m8yDvi/F8tpPcOfmROXjtJzP/qT0fPPeOAh6XMk2snC9/SkvC+iXA7U7s5k6xmPVsTWE1iquK9Eyj8aykU3xkrxYLLPfx7M/IiY94smNmHFfR2gsQvW8T8U2Jk+ykXU9I8PPbJbq93bNVDfydw5saOckR0o0U/El824+yjwV3+XimcBXYrU9rwY96nPx1HEAuuUcBFie4IbobbCOxyoAWsiAabqgjlBaWu+mZ2lRx2csR315pH2VTVN9rqcv+ayN7ClUw2wNfrq12RSPX9XnH6MpBXwnalzUUzjZwFA9o485MHxsibhCO2Sfoqn3FYjFl/FRHPlJiD7EJ/HK5exk2vAz0kO8Ah4TjpuDRq71gMhnUZhfopfAWKaqlu1f7gNClJ9y5h1T86m6Xgtv6zxoPW/aU+Scx4+SI9JPiUFPZEupTrxjZn6k1LHVs3POcUUXcV1gHt5lIyjteq/T7cWa9Wg3z2YbxlveV+FYWObED4TgeTWesQhz/C1+/WFBRSKKpGEelt9KA003oAP5OwY2zs7pM1VFPnWIDq6EZucmyWd4UGWF1FXb8+zsxFzWSDbqA8D9oDYRtAQBYXrbTserD630E6jzGm/ezDKbWaBGfXmkffnxR7wmPBJMCcpB16OiZ20QjR1TftplNODZ4RTMSD6FywY4pG/dzJ5vv4Kh02xLt2yH1IyBbNnJWIwLx0zwWB1oT3O8oqZmB8y86WZGAbgP1FNIakOXs7Hcr8K2TK2C8iHlKf1q7zuv5TR6vwMv9Fp4e5s+kg7NOqBypj4PcF1G1LHdU/zGmHsBdRHXXZqHq2wVjisxobaTZxNw28nLupabV+rAp9ngCF28NBvhufGJ63mgQC5NS8SWZ3ju9+KUbkWSyZJ81D3VZwv52wc29AZLdNtLPW1IHyoK8tswaSrWmeKGTnVte16V/In4n1/FqeHvckri4eNn9pnaGdBEw6ZAXjvthuTRxfXuGRWgmMu9NCF6G6bjvtNnz2QzC/IRVDp4iiq/zoGr37Fu7hGRawH7ggc49kK289g8cBj429JdtEMRBzEUDNiOPWYztqcpXtF60rQf1Hn3+MG2acCOegrJgXhMepBNDX9p3ttmmk2R3ojZhH3DTm7qyQOkpCMhGxaDJ2b8mbSlShdx3cV5OMp2prx8Uy7d5hnOi9EoGxzJHjg9/YzDkLhePWQhT7Sldj9fzsZv5spndzQsObtaP66d/K0DG3p3VZ7Skiusccg23PM1Z/nrXKii/smvvGh7noPDoTQ1/MCVyT/Izx++vbynpm5jIb1XRNM64pA8urjevUIGK1+F4njFqyREb33Tyg66LcjW6/Q8AmvzjUkAPCmhtnSbdujsRwUHA7ZDJmwEm1o6aIhX9KBo7evy9QMlx9uxwPDhQFwOcNQNhGlUjMAbcFJrmz1k5A1I6TM/Za1M8ryYuniLSiMdWsNMkfo8wJ3TRVzXYWyYPZ0/yA64S90z3wp9849F9Gae1hk1eoCkiev049rK3zKwoZ8auDuMchS5oZHO0W8AICfOTtmbtucV4elStsGZk3w0mk1v4c/YZdcH95tx9BPJwuwDnkLE66Dm5rTLkDwCr3cn6CmbtZ0Atd7eFcSG6G2ojuvyVab/NZQpR00No4wte01oJzrUAacrwPVne/3dUTt520ay4FbU2bOvLbVrh7qyJbcfzcj221B/N1Cehp5vNvj01McrermK2Mwsb6IgyPazHd/pT+wF1h9ebbM37e1zNP0i9aEWuawjev1irDknH7FfVzcN7QTdqbH4CdJPuWy2aaaIKw9+1TR8xfVRT7zpr+jrW71fccV11jjeQW1sWKahbIV9FdxkT/yx9Mk/rtczKKwDDRafZpX/Ffpxl8h/vV6TrzTYxif6+kS5qS9sbE/klCldTH8p0uRERXeeR/GbzJeOQpK2512Kuk8KKNUXt4HLMCy07OmI4lMc0xFpGRn6wGhdKn8fkkdQ2pQuqdLQkemViE6R/i7ZUooinE+V8HwKNOg0o/U6WKdD9NaZNj0leR1G8p6iKP9sJpflVGmydOfz7EVQ8oqS+jq6Q/i+6kgTWUdOmWm5U13m9WUcyVbViE6XpdW/6/q/vU8aMnYZ9mnP/rZkpdaeA2yp0S+c/WgUJ6SL29N2S/8rvSucZ83LuE+ls1H2meox+7/si4cP32uFS+IV0+5N/cv1p05+zwfXR5V+bTksb0/7zK7J+SRkc9LutMwLxajYnelbjIP8S0Q2mLcnjNP+VRmryqjyrou3dDtW0zbZrmvoOZezUJd37Cu4vMOF5EZ1GxVij7PcyjoaFNcpGZfl5p9HWNkkLr3V1yX9T9NTSrrG6Wx+k/N+LvxjER3LZjZOfi3zS4lu00v16ZB/LqMsn7IOXJKWuUz+lYEN3TBb/b5BJV1tg0EYzq6QpO15l+JsCK4Ll2FwpNs8IJaHbLQrNWnItvqbZx6Md1rDeKxH1YD4+yrh+RRRDsYV8Oh68emwlAnR29q0HMRRPeb3ROk48ColTSkP7ezkwXVfEzyqe7u1XbWB768O74ENx1GoE6XThfonh78lZw3aw/VYpWd79rQlK3U2GmJLXn6hWk4OMgodJMaVF+tsHijR75m+cnL+3FSHw4PvsYLRplkxfIA1Cdl3JoOKT/XUlyfCWv+92nJg3t722d7uZJtjdD7oSKhTUNExp/2re6ooo2tgg6HfLG1TpbxMja84d1zod6Xf9+wruJxDJs0GzMw4mQ7uuFrkFhTXGT7P/D0kj5CyUerG9tbsD7iuyb89H/6xiE0mVjt3yD+jl37c5fJ/4YGNOCbXB24Cv66G5KA+gWvTV/3v5i9ituF9avpYC3u/HFcTMV4e2Afe4O0PlwFbHD6PZM9DtqVHAP7gtty7Lfdun/zmgpl8G0BlqTRPD+elS/wGLjiHToC9PxaQ5/PieN0rAOASpt/lO7p9Xov8OKj9ebzWVwIwHK5vz7AlAPqgG1vu3z71Zp3WV/+mf+XeBI27fAMAwHOBgQ0A+mD0Jni/HL/XIj8Ialf/xk2qABga17Zn2BIA/dCFLV/BPkdvXwUVU2xmEzGZr8SOX/dKx2pOn3lXPRGLH880HRQAADwY4MDGTsxfXrJpRs3HnFIDcAvULvm+r0V+AHY/eYdrBFvgEbmuPcOWAOiLy235KvY5Woh9uhUxD8JslmLGr3ulI3vpTrwV6Wl9d2+EAwCAW4M9Nm4MD8BgHdjtQP0DDXRh+ECGoCugS7cF9Q+uCfTtsYA8nxcsRQEAAAAAAAAAAMBgyWZszOdz9REAAAAAAAAAAABgOGApyo3BdKnbgvoHGujC8IEMQVdAl24L6h9cE+jbYwF5Pi9YigIAAAAAAAAAAIDBgoENAAAAAAAAAAAADBYMbAAAAAAAAAAAAGCwYGADAAAAAAAAAAAAgwUDGwAAAAAAAAAAABgsGNgAAAAAAAAAAADAYMHABgB9sZtnr5ya79TnB2c3f6H7nYsnuV3wbFzRnm9qS+o+J6uj+gKAB+NCW0Zb14ajWE1QbwCAfvEY2NDOiI66VuC4EhNOwwGRK52tMWl7XgjGNaoHnGw4RxLJpFCnkwnVY1AcHJKHZ9o+5Ex5zls1xmQ37xshokR8n6qvTFtyHhPh7E+QHej7a+x0eKWt1uvLZFK1s+NOrChdnoYOttVyrtPviYjERrw/SIfouJL37Ko/GdwafqlW/yz5UL3Oqb7zNFndP0bd3S2d2rOJpy2Z1Njoo9nSw2DYeLt45UhmT3IvtQPcnq1g+2H0ZstEQ9592GdTe3OPDLHMj49vWxQSg1uojXeqdnMsxzsU685X1TiSHKiyPXnU+Vm+fm0b+5Q0yf/CfkiBEB3yTduB/Nfr9amWbXyiZOqIT1v1dYU0OUV5uuiUpOp7E5VXbGbS9rwQ1DWiKD7FcfnYnmyXvBZ8X8MiPSWRklcUyTqk/2vlVyEkj4C0LeTM+bhIE30dPmp034YuS7GAp21iK5s8pB24rmPUQyXfMj5pt6c4+z06RXFy2m63dCRUDvqcmCWopksoTZZ3lJTqVF238v0w4HsqkPsmi0z0b+a95t8pPS0dxWrV8ib5qN/zz7WyBXVUZGjQvT1rfG3JpMlGb2hLqr19dj3kOqiQ+wQpb7s6uOIV0pNc5qwryjfk7RkdN45H7gmuDxf92bJv3t3bp77ukOwurMza5wXK60rwfQwf37bIaH+a4moX2o484u2zTWm/Z9hYwVGe7WqbpqfU2Rap+6w62Rw+7/nwkf8l/RCTEB3yTduN/BsHNraxvLBWRKceaSXXFWNz+LYGv+15IahrtD6/R/i+hkTuoEqVqRWwztFoQvIIul4LOXMeVc5GyM5A/h3WGMtyezYQTEP95fmRQ5LlcmfcnFbfX3P5pP1X61R/X847+L7vCL6fMq77tNaL0j+XDE20Xy3XE9dfnWxBPTYZnvW9D3v2tyUTH3u+mS0pX/TsemjVpdbxiqGD9EPlnFTrJepdY63/q9iyX943s8/Bous3TF7Xwq5vQ8K/LZK6S2lLsUpIHK99oV9S2bEuFEvHS2Z5K3mqeypdRMZP9Xo0fHmG4i9/JwHyD9Eh77Qdyb9hKcpO/NrQf/EPsf7+VdAFxeZX/byfg/gifsT8x1L8DJgi1PY8cE2O4vfHgf6PRFKawzmafhekgKwg1qmbZ0Ly6OJ6LTj+Fh8HumZ6EvvFWH0Zgip39FW8jdRXDewyQ6vep2Qnfi4pP7LDxSf1lROPtNn90dWS/8SitnzK/i1TduVUXDLXv6n8QjF6Yz9xEB+/fecz3jf5fX78JqkqjivBM5lt9eLHUfz7w/+/ik+l+h8t9qRznkoD/OjTnr1tycTPnh/Nlh6J4Hhl91OwyNln/LeeioqqjKZi8Z/yNcuf3bdpj0LvtuyfN+wT3BXebdH142oZ15T83uhNfOVrkQ2VwkiDkfj0Sv/9+ZfHX7z8abZhO12LVuHXo9IqFilS3w8xCdGhS/StnfzrBzZ2v0Q2rvGFTtdK6KHwujOweV/lhfGh7XngWqTib6aftg67UkDxR/yrFV5IHl1crwWjhdif9q2dg3Yw4vUTldKDvKNsu08yw/mM7DAWWwqIm/BJe/z9QU1JJL7aLmZy/Ee162D0ibrlhOFwMtT3hYGAIUO6UO7A7H4uqf64X7rwk6+TjWgYJwZd0KM9e9uSgbc9h9iSsa8Dr2Plv7N1sgX9umBNtXPfCL1et7qm+tEJiVdkwNjgM7SvgV9w02fbHJp3UFtHtrc676kjbW9VtBllY9W9mPR+H5ZDG6Rhn9keLnkaY9+mQj68v4Gr1NWyOvd/cpa5tC9XyN4NoBX+bdGN4monkfjsHEdUD4G0vZIOf1vSXV7QeX9U2sQiBRr6IUWu1Y9rJ//agQ3ZGMeCxzW4AG9yZKN5wySjM/DNbwcSSdvzPPnzbyd2O3Ucu8//4dEdXUeHffyZ9aNu9JUIyaPl9W4u5/QvlYrs2O2tC9R2lJWziZLvzaPTnmnTzMuQN0k50PHZzMnGWMjq/0tuy8T1/XApdmDcs1j8GYmF7MGIzUxujgRvdMfU2HOwLYXYcwtbYn2acf5xLGI+N4cHIMbZbyJOxHa7FduEdPCwEbOx70ZhoIB3vKJnaNUF8BLZpnEbBoH0QmDbXI+/fe7mZHtLMj7T9sRHc+eRBw7Gy2wgNFHnSQ2JKCv6XGqENu8TMWYjZ/vPHMCBfMKY2hjyTSqfmH7jmSabJX1f0VvlJ6ish4iuofKJDjofnxG3nZiPZ2K5oZqO9PXYz4zlrCXQC95tURdxvEH7eFt1eKnM+cxV9QCdYy3O66hmumUP1zO9olg53mJWq4VL4/qgB3Z99eO6kv/auceGbXMO9Z1tXaleL5Wn15uYGOtgbGtP254XQr6Wq3xEp7j1YqRu4HIMhoqsiuh1VLVyCskj9Hot5My/1xO+LtSrHnIs+p6jrm3am7KF6jps37T6fuTBGwxRwCQ3GNLfnytUpa1bj1wtt2v/iHuH791FvqdG3V5DWv/05kjmsa1WBq8vNPU12+xpYHV2b9TJUOLWWxduew6xJUal97JnibctqXxcaZ33YPOxtjKp79x14F+fQ4Hvt0KlvnzilTofX0TLCfts3Lpt9svbzz4tdm/DYnfWteRKB232WS6Lvt9yvrkel8qUp69UkNbh0r1ayuzS4XNZ/OV1Tbhsw0Xrqzxq2yKbzzfwthGdT+Xw61c5dY3yzcts5GW1hRr4/OchQP5W/NuojBAdCtW3DuTvnLFxXL2L7OGOnK6hmIovdPfi8CGalxVOxXeqaeEzw6NA2/NqyKYYnljL8yPd8lNYOWrtNQgN7p8BylnbmfUJrh6t9BlBDUmbEcu1xOuFWEynYjpdiPU+La1307O0DmI5lqO+PCq/yqapPtfTl3zWRvYUqmG2Bj/d2myKx6/q44/RdE36muZP4Q6bZfb0HP5oaPjYEhFso+HYp2jqfQVi8WV8FEd+EqIP8Um8cjnLS8qAJz3EK+Ax4bg5aD2GmukTfRaF+SV6CYzlkXrZ/uU+IET5KadeWl6YbaLXwtuWyWk9b9pT5JzHj5IjGi3+kz4R9IhnW9QFl8TbakmBVdco3/Ve57kXa9aj3TybbRhveV+FY2GZ02SOrQvOtJN/bT/k2nQgf8fAxtk5faaqyKcZ0UG3ToffhkmjxQ/KgVIvvwVNdW17XggjEvh+y1cheWNPj4flvuWsNhEkbS8HAfzbXFqyaN5aIyTtmfLGlVRbar3beY03b/qUUv1FETdWMzGbzcTy4494TRI51b0cdD0q5GzV6pHmjinJwWzss8MpmJGYLvQAh/Stm9nz7VcwdJptqZ2NdoOa8pstOxmLceGYCR6rA+1pjlfUkgXhv279tapQYLCMso1hY2pDl7Ox3K/Ca8mn8iHlpS5q7zuv5TR6Lywv9Fp4e5s+kg7NOqBypj4P0D8+cV1f+MXbakkB9SW9NgA9rsSE2s4oSbO2k5d1LTevdG6aDaRQYCrGeBqUEy7/un7IHdBC/vaBDb3BElXEkjoy3JnJDxUF+W2YNBXrTMnJoQe96qTteYGMP8vRbOCHbiQdT/fkGq+GdcQheXRxPeZe5awCFBF/qTj3fAT1zy8xn8/Px3u2Wo3s72f2meKjoLRnJ+cfZGczC/IRVDr2PIr6ic3TuW7uEdHr3/uBBzj2QsYE2DxwGPjbUpiN9kSUiC0HA7Zj398sksenKV7RetK0bv28e3wnW0CA+0E9heRAPCY94IcE45fmvW2m2RTpjZhN2Dfs5KaePEBKOtJ6k0DwgATEdV3F1S5q423ew4U3z+Yxfp+Nein9t6U48NukssRyjzOeWbAYjbKBlOyB09PPOAyP63Nq+iFOrtaPayd/68CG3l2Vp7QUnjqqQ7bhnq85y1/n8i5W/+RXXrQ9D/RI3WZZalNFUuH6h00heXRxvXuFDFZuQVz7aqXDobSk4cBOgH+Qnz8ML+ab1r05lP8md9oZFpeqAfBchNpSiD13x9mPCg4GbIdM2Ag2tXTQEK/oQdHa1+XrB0peu9KDIcKBuBzgqBsI06gYgTfgpNY2e8jIG5DS5216wZthnNTFW1Qa6dAaZorU5wH6w78tulVczYMacglzvD15zVzMns4fSN9rBt6z+4a+tYzr/fohVUJ0qL2+tZW/ZWBDPzVwK7YcRW5opHP0GwDIibNT9qbteb6okSD6K/r65h3YPTfuN+PoJ5KFUT+eQsTroObmtMuQPAKvZ+X6ctZTNms7AWq9vSuI5SUgtkFF3kGH4WlZ/JnXzYakZXT5KlMFG8qUo6aG8RPgqi/UTnSoA05XgOvP9vq7o3by+k1U4B6os2dfWwq1UUlXttTwRrNsvw31dwPlaejH1Td5n09Pfbyil6uIzczyJgqCbF9Ozybr73EPlmfHq232pr19jqZfpD7UIpd1RK9fjDXn5CP2azHtRUH0U1+LnyD9lNPVm2aKuPLgV03DV/SJf1zXRRzvwhVvm4MacklBI4V9FdxkT/yx9KldXO8T89+qH3eJ/CtvRdG7Kzt2MJVYdlBt2PmU4je1y2lpt92253lDZY3EKSq8qUDvzkxH007VPcNlGBZa9lx3qj7zHWxLO9bmO3WXd7INyMM7bTs5829VUiq6zkO/uSI6Rfq7ZFuvMw06zWi9DtZpyy7kTpxpzR2UVX1RvenPZnJZTqNOjfPsRVDyurFdtYHvq47G3cK13G1vRaEj0a880emytPr3s656yRZYscuwT3v2tyUrtfYcYEuNfuHsR80d0xOld4XzrHkZ96l0Nso+Uz1m/5f99vDhe61wSbxi2r2pf7n+1Mnv+eD6qNKvLYfl7Wmf2TU5n4RsrvimgkIxKnZXfNNBfpB/icgG8/aEcdq/KmNVGVXedfGWbsdq2ibbdQ0953IW6vKOfQWXd9iEtEWmnJV+5+eW5KNkfEm8nfvFPN4pHWRbRVx6q/Mi/U/TU0q6xulsfpOv91yExyJe/RCr/JkAHQpKy1wm/8rAhteNEpV0tQ0GYTi7QpK25wWQZg2JYXR8sEHewbsVuSyDI93mAbE8ZKNdqU1DttXfPPNgPNO2kTOnqWIYofWwGaKJcjCugEfXS5vOvzOAsVCbloM4qsf8nigdB16lpNkrSXMHxAfXfU3wqO7Nq3x3Bt9fHd4DG46jUCdKpwv1Tw5/S84atIfrsUrP9uxpS1bqbDTElrz8QrWcHGQUOkiMKy/WWcO/Sn3l5Py5qQ6HB99jBaNNs2L4AGsSsu9MBhWfilc9l7HWf6+2HJi3t322tzvZ5lAMQx0/OSjC7YYeKDB0zGn/rg6C7gTZ6ot+s7RNlfIyNb7i3HGh35V+37Ov4HIOn4C2yDcGN3ye+Zt/vK11reYo2GNze2uW2xWP8m/PR4j8lVyb+iEO+Wf46hDjnfZy+b/wwEYck+sDN4FfV0NyUJ/Atemr/nfzFzHb8D41fayFvV+Oq4kYLw/sA2/w9ofLgC0On0ey5yHb0iMAf3Bb7t2We7fP3Vy8qLcBFJeoETw9nJcu8Ru44Bw6Afb+WECez4vjda8AgEuYfpfv8/Z5LfLjoPbnse69AcBwub49w5YA6INubLl/+9SbdVpf/Zv+lfsYNO7yDQAAzwUGNgDog9Gb4P1y/F6L/CCoXf2xGS94OK5tz7AlAPqhC1u+gn2O3r4KKqbYzCZiMl+JHb/ulY7VnD5nr3yNxY9nmg4KAAAeDHBgYyfmLy/ZNKPmY06pAbgFapd839ciPwC7n7wbNoIt8Ihc155hSwD0xeW2fBX7HC3EPt2KmAdhNksx49e90pG9dCfeivS0rn1bAAAAPCPYY+PG8AAM1oHdDtQ/0EAXhg9kCLoCunRbUP/gmkDfHgvI83nBUhQAAAAAAAAAAAAMlmzGxnw+Vx8BAAAAAAAAAAAAhgOWotwYTJe6Lah/oIEuDB/IEHQFdOm2oP7BNYG+PRaQ5/OCpSgAAAAAAAAAAAAYLBjYAAAAAAAAAAAAwGDBwAYAAAAAAAAAAAAGCwY2AAAAAAAAAAAAMFgwsAEAAAAAAAAAAIDBgoENAAAAAAAAAAAADBYMbADQF7t59sqp+U59fnB28xe637l4ktsFz8YV7fmmtqTuc7I6qi8AeDAutGW0dW04itUE9QYA6BePgQ3tjOioawWOKzHhNBwQudLZGpO257XiSFlN8utlx2TyNB3P21Ct88mEGragmLlFHqRX8y4a0db5kN28b4SIEvF9qr4ybcl5TISzP0F2kNtKU6fDK62nPRx3YkXp8jR0sK2Wc51+T0QkNuL9QTpEx5W8Z1f9yeDW8EuGL7MdlXyoXudU33marO4fo+7ulk7t2aRF21Jjo49mSw/DxfHKkcye5F5qB7g9W8H2w+jNlomGvPuwz6b25h4ZYpkfH9+2qIvY3KCz+JTjIqNMNX6W86htY58SD/nXxqqh/lTRKH9ffetA/uv1+lTLNj5RMnXEp636ukKanKI8XXRKUvW9icorNjNpe14w21OsrhHFyWm73dKRnOKYPicXZXwRfF+PS3pKIiXbKKK6jk8x/V8r6wrheaSJ/p2PGp0lOI2LkHwqKL2OCgVMT9uEys/3YDmkHbiuY9RDJd8yPml97aGaLqE0Wd5RQlcyUdetfD8M+J4K5L7JIhP9m3mv+XdKT0tHsVq1vEk+6vf8s59hAAsVGRp0b8+aNm1Lk43e0JZUe/vsesh1UCH3CVLednVwxSukJ7nMWVeUb8jbMzropOeu9TNcHy76s2XfvLu3T33dIdldWJm1zwuU15Xg+xg+vm2R0f54xtX1NLVnJnVpz3a1TdNT6myL1H3WdAr5vOfDU/7aB0aqDSocbdqgJvn76ls38m8c2NjG8sJcMXwBpx7piqICZw2/zeHbGvy25wWhK7WNwfYL39ejkgcIJcFpZa1zSpqwPM7Gw0Ys/65vRDltlfB8yshyB+hbQ53k+SUyXV3D0ZzW3x6k/VdtT39fzjv4vu8Ivp8yrvu01ovu9DhkaKL9armeuP7qZAvqscnwrO992HO7tsXHnm9mS8oXPbseWnWpdbxi6CD9UDkn1XqJetdY6/8qtuyX983sc7Do+g2T17Ww69uQ8G+LpO5S2lKsEhKbm/i0Z5ratMq/ni+v7qlUHhk/1evR8OUZir/8q/V8GU3y99a3juTfsBRlJ35t6L/4h1h//yrogmLzq36SykF8ET9i/mMpfgbMZ2l7nhfH3+LjQNWe/CcWI/Ud6Jmj+M2VTlqTlOZ7jqbfBSkrK1PDlKfAPDI5U9r0JPaLsfqyBRfno8odfRVvnvq2ywytep+Snfi5pPzIDhef1FdOPNJ624Oyf8uUXTkVl8z1byq/UIze2E8cxMfvtvMZ74v8Pj9+k1QVx5Xgmcy2evHjKP794f9fxadS/Y8We9I5OKlO6dOeW7Utfvb8aLb0SATHK7ufgkXOPuO/9VRUVGU0FYv/lK9Z/jy3aaBI77bsnzfsE9wV3m1RF7G5ScfxaYGR+PRK//35l8dfvPxptmE7XYtW4dej0ioW6YImmV6ib+3kXz+wsfslsnGNL3T66E189VR43RnYvK/ywvjQ9rwmjr8/qPmJxFffXibogFT8zXTZ1rlXyir+iH+1gg7MY7QQ+9P+cqO+NB/lYMTrp2oAayPvKNsHQnbzGdlhLLYUEDfhk9bbHo7/qHYdjD5Rt5wwHE6G+r4wEDBkSBfKHZjdzyXVH/vxhZ98nWxEwzgx6IIe7blN2+JtzyG2tDvv68DrWPnvbL+egn5dsKbayL+I3jeo5brcARMSr8iB6wafoX0N/IKbPtvm0LyD2jqyvdV5Dbq0vVXRZpSNVdan5/t9WA5tkIZ9Znu45GmMfZsK+dD3zn0QqmV17v/kLHNpX65L9m4AXvi3RV3E5me6jk+LqIdA2l5Jh79RJxoPqavcqp/bLNNL9K2d/GsHNmRjHAse1+ACvMmRjeYNk4zOwLemtCZtz2sgzWqVai9lx24428y5VzdBBB2gO8WOzv34M+vSQZQe+BfpIo9bkP6lUpEdf/Z7olTbUVaDHlHyvXl02jNtN/YwFrL6/5LbMnF9P1yKHRj3LBZ/RmIhezBiM5ObI8EH3TE19hxsSyH23MKWWJ9mnH8ci5jPzeEBiHH2m4gTsd1uxTYhHTxsxGxcs2ExcOMdr+gZWpFoahJkm8bjxRBILwS2zfX42+duTra3JOMzbU98NHceeeBgvMw6Dok6T2pIRFnR51IjtHmfiDEbOdt/5gAO5BPG1MaQb1L5xPQbzzTZLOn7it4qP0FlPUR0DZVPdND5+Iy47cR8PBPLDdV0pK/HfmYsZy2BXvBui7qMq7uOT9UDdI61dkfeaFnOdMsermd6RbFyvMWsVgtt4vo//3Zit1MH1XcwPjIN0beu5L927rFh25xDfWdbV1pZX643MTHWwdjWnrY9zxu97kgevKEKNRByQxX9fbuMO4Gv/5BU5FpEr7mqrfqL8tByv3Qdnl8+Jl73lmPR9xx1bdPelC1U1zD6pg2xB53WtmbPXS+u/SPuHb53F/meGnV7DWl91Zsjmce2Whm8vjBLr45ss6eB1dm9USdDiVtvXbjtOcSWGJXey54l3rak8nGldd6DzcfayqS+c9eBf30OBb7fCq3ilTofX0TLyaUPz4S1/gt0actl/PL2s0+L3duw2J11LbnSQZt9lsui77ecb67HpTLl6SsVpHW4dK+WMrt0+FwWf3ldEy7bcNH6Ko/atsjm8w2CbaTT+JSg8uVlJn2L1e9WW6iBz38eAuTPaB2oHOf6bsZTpqH61oH8nTM2jqt3saH/5UiJZiq+kHcThw/RvKxwKr7TXQufGR4F2p7XRCzXTq4XYjGdiul0Idb7tMV6MgC6Q9uZdcRTj1b6LHkISZvhYw96ltZBLMdy1JdHdlfZNNXnevqSz9rInkI1zNbgp1ubTfH4VX38MZquxf6U5k/hDptl9vTc66EYuCM825ZgGw3HPkVT7ysQiy/jozjykxB9iE/ilctZXlIGPOkrXgEPB8fNQesx1Eyf6LMozC/RS2Asj9TL9i/3ASHKTzn10vLCbBO9Ft42rVzredOeIuc8fpQc0Wjxn/SJoEeu2M/pKz4dcZlPPCpBx16sWY9282y2YbzlfRWOhWVOk3m3WxcMG0/5Z8vudB3LI91yjCtncvlNzOopnulA/o6BjbNz+kxVkU9VoYNcJx1+GyaNFj8oB0q9/BY01bXteU2UN+qjK6n1PVjTCm6B2nSHtL0cBPBvc2nJonk5YkjaMz72wJtZplvqeEfs8GZiNpuJ5ccf8Zokcqp7Oeh6VPS0c6LRkZMczAYjO5yCGYnpQg9wSN+6mT3ffgVDp9mW2tloN6g1rtmyk7EYF46Z4LE60J7meEUtWXCuI67yWlUoMFhG2cawMbWhy9lY7lfhteRT+ZDyUhe1953Xchq9F5YXei28vU0fSYdmHVA5U58H6J/r9HP6j09zjisxofOjJM3O52Vdy80rdeDTrDNOgakY42lQTlv5j6YLsadYn2neN+qK8UwL+dsHNvQGS1QRS+rIcGcmP1QU5Ldh0lSss4oihx70qpO259nQQvUPKkAH6AbV8SRQrgdrWHPcRR73jApQRPyFNL5IPpPjzy8xn8/Px3u2Wo3s72f2meKjoLRt7CGbWZCPoNKx51HUT2yeznVzj4he/94PPMCxF7JdwUDrMPC3pTAb7YkoEVsOBmzHvr9ZJI9PU7yi9aRp3fp59/jBtmnAjnoKyYF4THrADwnGL81720yzKdIbMZuwb9jJTT25Q0E6gs3wwZmAuK6DuLq/+LTMUay+LcWB3yaVPfyTe5zxDOfFaJR1xrMHTk8/47Cjfu74M0m+mSCZXqRv7eRvHdjQu6vylJbCU0d1yDbc8zVn+etc3sXqn/zKi7bnWXBvhuO/qRcIpW5jLbUBI6l7/YOpLvK4V8hgedcdtrOadQ2HQ2lJw4GdAP8gP38YXsw3bSf2YL4xCYAnJdSWQuy5O85+VHAwYDtkwkawqaWDhnhFD4rWvi5fP1ByvB0LDB8OxOUAR91AmEbFCLwBJ7W22UNG3oCUPm/TDt7+VqF+M9SjdGgNM0Xq8wD94d8WdRdX9xGfmmRP5w+k7zUD79l9Q9+6iesD8ZNpe31rK3/LwIZ+auBWbDmK3NBI5+g3AJATZ6fsTdvzqugpdJXpNWqNEAKJPnC/RUeP9hVmKvB0I14zNTenaAbmcSdofavtBDToHi8BsQ0q8g46DE/L4s+8bjYkLXOxPaipYfwEuDomo53oUAecrgDXn+31d0ft5PWbqMA9UGfPvrYUaqOSrmyp4Y1m2X4b6u8GytPQj6tv8j6fnvp4RS9XEZuZ5U0UBNl+tuM7/dnnHizPjlfb7E17+xxNv0h9qEUu64hevxhrzslH7Ndi2ouC6Ke+Fj9B+imXzTbNFHHlwa+ahq/oE/+47vLYvM/4NKewr4Kb7Ik/lj5dHtfTWdnsCPor+vp2boMulH+wvmkukX/lrSh6d+XaLXEtu3w37HxK96t2OS3tttv2vCDMHWPVGwsivUOzx47zPcJleFy0ntARqbdD5HIo7W6r9a6y621AHiTnrX4LBR2ZXpF8I/1dsj3v3qvgfKqE51OgQacZrdfBOu3cddqCM62/PchyqjRZuvN59iIoeTXtAH+H8H3V0bhbuJa77a0odCT6lSc6XZZW/67r31O2wIpdhn3a84VtS609B9hSo184+1Fzx/RE6V3hPGtexn0qnY2yz1SP2f9+u5UPCb7XCpfEK6bdm/qX60+d/J4Pro8q/dpyWN6e9pldk/NJyOaKbyooFKNid6ZvMQ7yLxHZYN6eME77V2WsKqPKuy7e0u1YTdtku66h51zOQl3esa/g8g6bkLbIlLPS7/zcknyUjL3k5tRDC7VpXXrLp8myJGl6SknXOJ0tD877ufCVP9UtpYty++bjbOMVf9aJ/AP0LeMy+VcGNnw7XJV0tQ0GYTi7QpK25wXDjRY5VpUXH1lDU7WHq8LleGjSbR48y0M28JVqN/Sg+ptnHqbxWI+qAfH3VcLzKaIcjCvg0ffapvPfWcPhZw/ZK0lzB8QH131N8Kjuzat8dwbfXx3eAxuOo1AnSqcL9U8Of0vOGrSH67FKz/bsaUtW6mw0xJa8/EK1nBxkFDpIjCsv1tk8UKLfM33l5Py5qQ6HB99jBaOdsmL4AGsSsu9MBhWfilc9l7HWf6+2HJi3t322tzvZ5lAHJOEBEXkk1Cmo6JjT/tU9VZRRd4Js9UW/WdqmSnmZGl9RGLBT+n3PvoLLOXwC2qIuYvMyXu2Qwpm2ub01y+2KR/m358NP/mk2uGrKng4e6LApSlfy9+7HXS7/Fx7YiGNyfeAm8OtqSA7qE7g2fdX/bv4iZhvep6aPtbD3y3E1EePlgX1g/7sldwxscfg8kj0P2ZYeAfiD23Lvtty7fe7m4kW9DaAyTZ+nh/PSJX4DF5xDJ8DeHwvI83lxvO4VAHAJ0+/yndA+r0V+HNT+PNa9NwAYLte3Z9gSAH3QjS33b596s07rq3/Tv3ItfNe7AQIAwMDBwAYAfTB6E7xfjt9rkR8Etat/YeMhAB6Ba9szbAmAfujClq9gn6O3r4KKKTaziZjMV2LHr3ulYzWnz9krX2Px45mmgwIAgAcDHNjYifnLSzbNqPmYU2oAboHaJd/3tcgPwO4n76iMYAs8Ite1Z9gSAH1xuS1fxT5HC7FPtyLmQZjNUsz4da90ZC/dibciPa3v7o1wAABwa7DHxo3hARisA7sdqH+ggS4MH8gQdAV06bag/sE1gb49FpDn84KlKAAAAAAAAAAAABgs2YyN+XyuPgIAAAAAAAAAAAAMByxFuTGYLnVbUP9AA10YPpAh6Aro0m1B/YNrAn17LCDP5wVLUQAAAAAAAAAAADBYMLABAAAAAAAAAACAwYKBDQAAAAAAAAAAAAwWDGwAAAAAAAAAAABgsGBgAwAAAAAAAAAAAIMFAxsAAAAAAAAAAAAYLBjYAKAvdvPslVPznfr84OzmL3S/c/EktwuejSva801tSd3nZHVUXwDwYFxoy2jr2nAUqwnqDQDQLx4DG9oZ0VHXChxXYsJpOCBypbM1Jm3P88Yov/OYCMRw98CRRD3J9YGPyYQawTayIb2ad9GIts6H9O59I0SUiO9T9dWlukh2kNtKk8J6pa3W98tkUrWz406sKF2ehg621XKu0++JiMRGvD+IMR1X8p5d9SeDW8MvGb7MdlTyoXqdU33nabK6f4y6u1s6tWcTT1syqbHRR7Olh+HieOVIZk9yL7UD3M6tYPth9GbLREPefdhnU3tzjwyxzA9Lbfxh6nFHfSLv6xEhaemTtD151PlZzrO2jX1Kuo1F6jiWY1jSm/mq2jeQ+PbvLpd/88DG7qdYHtTfm18lBbRz2Ly3Gihoe14Tn77GIo7tR5SleBWfRtkf4Gawsx2L2eYgDlEk5UP/Hw4bMRuHDTxlje14KSiri7gon+Nv8UHnRV/fhKla7XVRBmN+RfFJS87jheub/owTsd1u6UhE/CrEn3+mlVO68UwsOQ5U6ZKY5LKZifFkVXRgozfxlW7i8PG7+P1AGb19zWRyWP6s+j1qqDk2tgbHWn9Lx1dTsOyUqV43hwPVq5I//b2ZjREk9kQf9izxtSWTBht9MFt6RMLjFQ7YxmJMinI4RLnd63ZuSbbPD48g72b6s2XPvGGf4E6JonPMcT6+iLH6nemyT+RzPU1zWu4HUFwkqB1NU5Fu4yzWrMZE5EvZj8ZbsXYNTj4lPcQiDthPjrMYluSayTGLlsVmSX2DymgDy9Wnf9eR/Nfr9akOyvdExT5Roel/cYq36ocyaXKi2zrRDWb/U8B/StVPOdu4mkfb87pA5Ss6z9gfvj5gNZD6VZYFKXaAjNJTElFaSh8lW/V3fKo7017/4fmUkfcTnZKKMjtouM88v0Smi2oybk6r76+5fNL+q7anvy/nHXzfdwTfTxnXfVrrRfkyH13VfrVcT1x/dbIF9dhkeNb3PuzZ35ZMfOz5ZrakfNGz66FVl1rHK4YO0g+Vc1Ktl6h3jbX+r2LLfnnfzD4Hi67fMHldC7u+DQjlmzzCDzch8XbI9XzTVtIpnSmdKOOnej0avDyD0fbVfSxig89jP1lIreRXLoO8RlWOlf5dR/JvmLGxE7+ykZ8fYv1dPr3c/HKN+kgO4ov4EfMfS/GzPmmBtuddwi67uUgkzvmI4DocxW9+hGKRxWj6XZBi+80Wyp7EUB7pSewXtvFiTy7OR91P9FW8eY561+viTvzkaVNkh4tP6isnHmn1E6vkP7GoLZ+yf8usBDkVl8z1byq/UMhZDgfx8fsxnmPl92k+maubreHFUfz7w/9Xn4qMFnvSOU+lAX70ac/etmTiZ8+PZkuPRHC8ome+ks/4bz2tzhQYTcXiP+VrbDPEgKR3W/bPG/YJHo376xONxKdX+u/Pvzz+4pkCsw3b6VrcSynvgh5jERsyVi21ZWomG7eQ567BJf27dvKvH9jY/RKs5vEXOl0X2KODqTsDm/fSVPUG2p7Xirxz4t/5BH2Rir+Z3ttkoRRb/BH/mpRitBD70z7AqB1cmo9yMOL1UzWAtdGgi7s5T82KxdZjzp1P2uPvD3I7kfjapPjHf1TrDkafqFtOGA4nQ33/MFN0SRfKHZjdzyXVH7cFCz/5OtmIhnFi0AU92rO3LRl423OILe3O+zrwOlb+O1snXdCvC/YwMvIvwlNHOa/yOunHJyRekR2GBp+hfQ38gps+2+bQvIPaOrK91Xkdu7S9VdFmlI1Vpl1TfGCuOS8c2iAN+8z2cMnTGPs2FfLhtfCuUlfL6tz/yVnm0r5cbfdKA9fjLvtE6iGQtlcq4zfqiId13p+DXmORYCLxOR8bvqR/107+tQMbsjGOBY9rcAHe5MhG84ZJRmfgW1Nak7bntaC7zgm4GN2BdgwEjD+z3pkjgHdO+jfTrehs2bXU6qJqbKLke/PotGfaNPMy5E1SDnSM4KN2458yYyHF8pfclonr++FS7MC4Z7H4MxIL2YMRm5ncHKlfbwcuosaeg20pxJ5b2BLrE6+vjbI1rOrLDL3Glf7Ua28T0sEWexgBhXe8omdomcGeHdnW8XgxBNILgW1zPf72uZuT7fFGVabtiY/mhzU8cMD7fVAcnqjzpIZElBV9LjVCm/dJtoeLMNe8875NPMig8uH17TzTZLO07eek/ASVNV8LT/nk+z9VRzYt7NS+XFTTak8F3mh1Nh6f9+sDvcF7Kex26jj6+5G2faKQ6zWmVQ/QOdbi349qplv2cD3TKypjvMWsVgv9xiK+qEEMKkc+Gzmkf9eV/NfOPTa2J3K9pbUt6jvbulK9tiZPr9Ka62Bsa0/bnncRlmvcCL6vp6eiA0X0+qww+fut52yu//B1oWHlrdNFdW3T3pQtVNfB+abV9yOPKE5OFDCdttvkRPGL/D4vuE5btx65Wm7X/hH3Dt+7i3xPjbq9hrQeR7wnUVw8ttXK4PWFWXp1sCwsyUAAdTKUuPXWhdueQ2yJUem97FnibUsqH1da5z3YfK+tTOo7dx341+dQ4Put0Cpe8Y83tJx81zk/Mtb6L9ClLZfxy9vPPi12b8Nid9a15EoHbfZZLou+33K+uR6XypSnr1SQ1uHSvVrK7NLhc1n85XVNuGyDRsu0clA8UpJFlRZ9opDrBabN20/jd6st1MDnPw/aX8mjj1jEB6v/0LJ3OF19Tv5zB/J3ztg4rt5F9nBHTtdQTMUX0n5x+BDNywqn4jvVnPCZ4VGg7Xn+6HvrdqQKgHBqdVGPVvqMoIekzYjlWuL1QiymUzGdLsR6n5bWu+lZWgexHMtRXx5pX2XTVJ/r6Us+ayN7CtUwW4Ofbm02xeNX9ZneaLoW+1OaP4U7bJbZ03Ovh2LgjvCxJSLYRsOxT9HU+wrE4sv4KI78JEQf4pN45XKWl5QBT/qPV8CDwHFz0HoMNdMn+my8NYLQS2AsU1jL9q/f7EWdiuJTTr20vDDbRK+Ft01N13retKfIOY8fJUc0WvwnfSLoh2wp1Yl78/mRbjlukbNz6uKKVn2ikOsFpl3vdbq9WLMe7ebZbMN4y/sqHAvLnCbzK2xdMBhuGIuoZSJ2/xFAB/J3DGycndNnqop86hAdpPp0+G2YNFr8oBwo9fJb0FTXtuf5oTZLoSuUHS8A16VOF+XrjDggafYRIWnPVF/npde7ndd48wZB/MqlKOIGaCZms5lYfvwRr0kip7qXg65HhZytWj3S3BiQHMwGPDucghmJ6UIPcEjfupk9334FQ6fZltrZaDeo6aHZspOxGBeOWbtXZoKc5nhFLVnw2SdK8er7rkUwAEbZxrAxtaHZK315vwqvJZ/Kh5SXuqi977yW0+i9sLzQa+HtbfpIOjTrgMqZ+jzAdRlRx3ZP8Rvj3guouz6R3/Uk3mmpwzyhtjNK0qzt5GVdy80rdeD5daAJnby0vF70eblNLKKWiYgeNnVtIX/7wIbeYIkqYkkdGe7M5IeKgvw2TJqKdaa45NCDXnXS9jwPVKMg4i/dVj5oj258HU8N5dqx5vXJg6NGF/MR9D+/xHw+Px/v2Wo1sr+f2WeKj4LSnp2cf5CdzSzIR1Dp2PMo6ic2T+e6uUdEr3/vBx7g2AvZzmPzwGHgb0thNtoTkXo3vO3Y9zeL5PFpile0njTtE3XePf7h2rpnRz2F5EA8Jj3ghwTjl+a9babZFOmNmE3YN+zkpp7cKSEdCdkkEDwx48/Z42gnXfeJmq5n0pj2KFbfqMPMb5PKBl3kHmc8s2QxGmWDI9kDp6efcXjLWIT35eFNSHmsxLL58kX9u3bytw5s6N1VeUpL4amjOmQb7vmas/x1Lu9i9U9+5UXb82qhSpLb/uIVr3dF3SZcarNGMo3Heojlp4uHQ2lJw0EOLPLTV/78YXgx37TuzVj9N7nTjWFxqRoAz0WoLYXYc3ec/avgYMB2yISNYFNLBw3xih4UrX1dvn6ghDe1PSwciMsBjrqBMI2KEXgDTmpts4eMvAEpfd6mF7wZxkn9ZqhH6dAaZor4b6gK7oH77hNlT+cPpO81A++Zb4W+3SgWIf2ZyGXp8fbkmAHSvn/XVv6WgQ391MDdkZSjyA2NdI5+AwA5cXbK3rQ9rwa1rgiBw73hfuOOHl0sjCbz1CReXzW/z7dJ6CmbtZ2ABl3kJSC2QUXeQYfhaVn8mdfNhqRldPkq0/987UNNDeMnwNW2UDvRRxuI6hCuP9vr747ayes3UYF7oM6efW0p1EYlXdlSwxvNsv021N8NlKehH1ff5H0+PfXxil6uIjYzy5soCLJ9OZWXrL/HPVieHa+22Zv29jmafpH6UItc1hG9fjHWnJOP2K/FtBcFOU9Zr/gJ0k+5XKFppogrD37VNHzF9aGOJz/xpr+ir29Vv+IT8wXF2w3XK9BUNnNfBTfZE38sfeovFnHK3xzUkMtE7AT27zSXyH9dfiuK3l3ZsYOpxLKDbsPOp1RnapfT0o7Ubc9rgc7r0ny6hMsDGK1TdETqTRL5zrilnXDzHcDLO+Sm9NP5TRSZXonoFOnvkm1hB3DGXv/h+RRo0GmmtS6G7FzsTGvuoBzJe4r0juXFXc9lOVWaLN35PHsRlBybdoC/Q/i+6qjs3lxGy932VhQ6Ev3KE50uS6t/1/V/2a7Uz45dhn3as78tWam15wBbavQLZ/9q7pieKL0rnGfNy7hPpbNR9pnqMfvff7f6ocD3WuGSeMW0e1P/cv2pk9/zwfVRpV9bDsvb0z6za3I+Cdlc8U0FhWJU7K74poP8IP8SkQ3m7QnjtH9VxqoyqrzLdnv2E+d2rKZtsl3X0HMuZ6Eu79hXcHmHC8mN6jYqxB5nubl01CsOVTIuyi3kem3K5tJbXWbS/zQ9paRrnM7mNznv56KnWMQqfy0HOvIYtnSQvzxj+hX1e15Wmz+4TP6VgQ0vRScq6WobDMJwdoUkbc8LRefjEyReEb4voEi3eaAtDxkMVORl6EzxN8N4rEfVgPj7KuH5FFEOxqVrl+iiM4CxUJuWgziq3/yeKB0HXqWk2StJcwfEB8ukJnhU9+ZVvjuD768O74ENx1GoE6Xrhfonh78lZw3aw/VYpWd79rQlK3U2GmJLXn6hWk4OMgodJMaVF+tsHijR75m+cnL+3FSHw4PvsYLR9lgxfIA1Cdl3JoOKT8WrnstY679XWw7M29s+29udbHOok0KdBDkowu2GHigwdMxp/+qeKsroGthg6DdL21QpL1PjK84dF/pd6fc9+wou55BJswEzM36mgwcTXI5F+6qmONTweWa6kOuFla25vTX7Ca54lH97PnqIRazy1/6j5ijLz7d/p/O+QP4vPLARx+T6wE3g19WQHNQncG36qv/d/EXMNrxPTR9rYe+X42oixssD+8AOd1y+DrDF4fNI9jxkW3oE4A9uy73bcu/2uZuLF/U2gOISNYKnh/PSJX4DF5xDJ8DeHwvI83lxvO4VAHAJ0+/yHd0+r0V+HNT+PNa9NwAYLte3Z9gSAH3QjS33b596s07rq3/Tv3JvgsZdvgEA4LnAwAYAfTB6E7xfjt9rkR8Etat/86ZRAAyMa9szbAmAfujClq9gn6O3r4KKKTaziZjMV2LHr3ulYzWnz9krX2Px45mmgwIAgAcDHNjYifnLSzbNqPmYU2oAboHaJd/3tcgPwO4n73CNYAs8Ite1Z9gSAH1xuS1fxT5HC7FPtyLmQZjNUsz4da90ZC/dibciPa1r3xYAAADPCPbYuDE8AIN1YLcD9Q800IXhAxmCroAu3RbUP7gm0LfHAvJ8XrAUBQAAAAAAAAAAAIMlm7Exn8/VRwAAAAAAAAAAAIDhgKUoNwbTpW4L6h9ooAvDBzIEXQFdui2of3BNoG+PBeT5vGApCgAAAAAAAAAAAAYLBjYAAAAAAAAAAAAwWDCwAQAAAAAAAAAAgMGCgQ0AAAAAAAAAAAAMFgxsAAAAAAAAAAAAYLBgYAMAAAAAAAAAAACDBQMbAPTFbp69cmq+U58fnN38he53Lp7kdsGzcUV7vqktqfucrI7qCwAejAttGW1dG45iNUG9AQD6xWNgQzsjOupageNKTDgNB0SudLbGpO15oRx3YjWf5NfKrjeZi9UOwZvkSNVcrZ+w6gnJo4e0hi5Vj5aNKeU5b9UYk928b4SIEvF9qr7i77QtOY+JcPYnyA70/TV2OrzSVuv1ZTKp2pmynTwNHWyr5Vyn3xMRiY14f5AO0XEl79lVfzK4NfxSrf5Z8qF6nVN952myun+MurtbOrVnE09bMqmx0UezpYfh4njlSGZPci+1A4hFWtCbLRMNefdhn03tzT0yxDI/LLXxh0WPPeM6L7xt0aed5Ljo/Hudn+V8XD8/L011fGE/pIJv7FNNZ+/zdSD/9Xp9qmUbnyiZOuLTVn1dIU1OUZ4uOiWp+t5E5RWbmbQ9L4jtKVb5R3Fy2m63p20S59dtn+/l8PVvT3pKIiWDKDrFcXyK6f9amVQIyaOntEqXoojScLrCsaWcqnA+LtJEX4ePGt23octSLGCmd9WyyUPqo+s6Rj1U8i3jk9ZiE9uEykGfE7ME1XQJpcnyjpJSnarrVr4fBnxPBXLfZJGJ/s281/w7paelo1itZ/8Tqd/zz7WyBXVUZGjQvT1rfG3JpMlGb2hLqr19dj3kOqiQ+wQpb7s6uOIV0pNc5qwryjfk7RkdjnbqGeH6cNGfLfvm3b196usOye7Cyqx9XqC8rgTfx6DReu0V/4bEdfX426JPO3m2q22anlJnW6Tyqum8DV6erfCr4/b9kDI+12OMeKe2H9eN/BsHNraxvDAXlC/g1CNtVLpibMZha/DbnheANrzK+TpIaZtxB3C5bk3umEr1oJXKp35C8ugrrZZnKWktnEeVsxGycbZpjGW5HYGvjYa6zvMjhyTL5c64Oa2+v+bySfuv1qn+vpx38H3fEXw/ZVz3aa2XAH+i/Wq5nrj+6mQL6rHJ8Kzvfdizvy2Z+NjzzWxJ+aJn10OrLrWOVwwdpB8q56RaL1HvGmv9X8WW/fK+mX0OFl2/YfK6FnZ9GxAB8W9oXGcnxF502gZ7qdyDOq9UUFnOej0avDyD8azjOhr6IUX8ryd9ZTXfSj+uI/k3LEXZiV8b+i/+Idbfvwq6oNj8qp/3cxBfxI+Y/1iKn/VJC7Q9z4f074H+jcTnsfwMTI7i94esn6Q0L3M0/S5IqVjojdPL/PPoK22HHH+LjwNdMz2J/aKN0qhyR1/F20h91cAuM7TqfUp24ueS8iM7XHxSXznxSJvdH10t+U8sasun7N8yZVdOxSVz/ZvKLxSjN/YTB/Hx+zGmpub3+fGbpKo4rgTPZLbVix9H8e8P//8qPpXqf7TYk855Kg3wo0979rYlEz97fjRbeiSC45XdT8EiZ5/x33oqKqoymorFf8rXLH9236Y9Cr3bsn/esE8wTMLjOish9tKqnWRG4tMr/ffnXx5/8fKn2Yavuxatwq9HpXUdn6nvh5Twvt4l/bh28q8f2Nj9Enyb8Rc6ffQmvtYW4Iw2js37Ki+MD23Pa2L8OTPVSgN0/P1B31J1P/WIRyrkuI+tE66USvwR/2oFEpJHX2k7ZLQQ+9O+tXPQBi9eP1EpPcg7yrb7JDOcz8gOY7GlgLgJn7RS7yPx1XYxk+M/ql0Ho0/ULScMh5Ohvi8MBAwZ0oVyB2b3c5n5jfjHwk++TjaiYZwYdEGP9uxtSwbe9hxiS8a+DryOlf/O1skW9Mt3jasF574Rer1u09rqxyMkXpEBY4PP0L4GfsFNn21zaN5BbR3Z3uq8p460vVXRZpSNVfdi0vsXWA5tkIZ9Znu45GmMfZsK+dD3zkX01bI6939ylrm0f4OvnwH90yausxFgL23aSYl6CKTtlXT425JyuqDz/qi0r2NFQz+kjP/1LunHtZN/7cCGbIxjweMaXIA3ObLRvGGSbqCpM/DNfweS9uc1MFr8l40KHZZjMZlTY3Jkxz0R4+yp2Vb898wWop2coxOuB4VqB3BD8ugrrcGffzux26mDZH110r9UKrJjzwGz2o6ycjZR8r15dNozrZzBRN4k5UDH3DyKgx3fzaPGQlb/X3JbJq7vh0uxA+N+2uHPSCxkD0ZsZnJzpBtoKfClxp6DbSnEnlvYEuvTjPOPYxHzuTk8ADHOfhNxIrbbrdgmpIOHjZiNQzYKAzne8YqeodU8a1S2adyGQSC9ENg21+Nvn7s52d6SjM+0PfHR/FCGBw7Gy2wgNFHnSQ2JKCv6XGqENu8U17KRs/1nDuBAPoHjXvJNKp+YfuOZJhuOhyt6q/wElfUQ0TVUPtFB5+Mz4rYT8/FMLDdU05G+HvuZsZy1BHrlsvi3n/jNu51UD9A51uKyH9VMt+zheqZXFCtTnw2zWqtcGteHPrDzvl5IP64r+a+de2zYNudQ39nWlVbWl+tNRYx1MLa1p23PCyalbNQ6H3Xcw1pWLsdNqdR/Eb02qrbuQ/LoK638QqavHNEpdsiaf69HryOrX89n4lVnORZ9z1HXNu1N2UJVd33T6vuRB2/4QwGT3PBHf3+uUJW2bj1ytdyu/SPuHb53F/ma1Lq9hrT+2TYP3VYrg9cXmvqabb40sDq7N+pkKHHrrQu3PYfYEqPSe9mzxNuWVD6utM57sPlYW5nUd+468K/PocD3W6FSXz7xSp2PL6LldA+xya2x1n+BLm25jF/efvZpsXsbFruzriVXOmizz3JZ9P2W8831uFSmPH2lgrQOl+7VUmaXDp/L4i+va8JlGzRappWjHP9q3bbpbVt/Xnee/k0eje0k3Uf+vVF2qy3UwOc/D4F1XMG/jZIEXK/SZhap+OQO5O+csXFcvYvs4Y6crqGYii909+LwIZqXFU7Fd7pz4TPDo0Db8xo4/ha//vAIU0T9Ds6fbmP5Tbin44HBkU2NO7E3y490y0/b5dMJr4cNV0bbmfUJrh6t9BlBDUmbEcu1keuFWEynYjpdiPU+La1307O0DmI5lqOw/BRglU1Tfa6nL/msjewpVMNsDX66tdkUj1/V5x+j6Zr0Nc2fwh02y+zp+T3qKajDx5aIYBsNxz5FU+8rEIsv46M48pMQfYhP4pXL6TP1GFjoKV4BjwfHzUHrMdRMn+izKMwv0UtgLNNoy/Yv9wEhyk859dLywpN5vRbetkxO63nTniLnPH6UHJGeOQ16wjv+vVVc59lO0n2s9/oe9mLNerSbZ7MN4y3vq3AsLHPiWfjwvBrPOi5R2w+ppd31aulA/o6BjbNz+kxFy6c10UG3ToffhkmjxQ/KgVIvvwVNdW17npN8Ol8itule7Pd7cUq3VPnS4PN1iuDhGJGh7besTWRnHe/dcjlqE0HS9nIQwL/NpSWL5q01QtKeKW9cSbWl1rud13jzZpYp1V+U2cpMzGYzsfz4I16TRE51Lwddj4qedk40dkxJDmZwkR1OwYzEdKEHOKRv3cyeb7+CodNsS+1stBvUGtds2clYjAvHTPBYHWhPc7yipncL//2gXqsKBQbLKNsYNqY2dDmjeHPiNzU89yHlZQFq7zuv5TR6zwQv9Fp4e5s+kg7NOqBypj4PcF1c8e+t4jqfmLPCcSUm1HZGSZq1nbysa7l5pQ51mg3c0A2IMfpwOeF1XNcPaaaVTENoIX/7wIbeYIkKtiSFZ6XPDxUF+W2YNBXrzKjIoQe96qTteTZUQElC2+4XYqqFwLuQ5yNL78+7xrhhsyC5jqphbXBIHn2lrWP8ORuOuztUgCLiL5VR0nwE9c8vMZ/Pz8d7tlqN7O9n9pnio6C0VKnK6fgH2dnMgnwElY49j6J+YvN0rpt7RPT6937gAY69kDFIRw0C6Bl/Wwqz0Z6IeGCfggHbQW3js9hx9zTFK1pPGvaqolZO7x7v02cFA0I9heRAPCY94M7k+KV5b5tpNkV6I2YT9g07ualnFs9esEkgeC4c8e/14rrwmPPMUay+LcWB3yaVdbrlHmc8s2AxGmUDN9kDp6efcXhBHdf0Q9wEXO+iflw7+VsHNvRupzzFJFd645BtuOdrzvLXubyL1T/5lRdtzyujNy6xjkBq4TwzdZsFqY0SSS3rHyCF5NFX2qFBBiu3IK59tdLhUFrScGAnwD/Izx+GV/FN69p0lcvku8mddobFpWoAPBehthRiz91x9qOCgwHbIRM2gk0tHTTEK3pQtPZ1+fqBkueu9GB4cCAuBzjqBsI0KkbgDTiptc0eMvIGpPSZZx63eLjaQF28RaWRDq1hpkh9HuDO6SmuaxtzZk/nD/KhtEvds7yhby3r2K8fYsP/eu37cW3lbxnY0E8N3B1GOYrc0Ejn6DcAkBNnp+xN2/NK6NEia6VqAQy1c9wF7rfd6KeMhZE8nhbEa5vm5lTKkDz6SutCjfjRX9HXN8qxf/SUzdpOgFpv7wpieaqgbVCRd9BheFoWf+Z1syFpGV2+ytKchjLlqKlh/AS46gthU41w/dlef3fUTl6/iQrcA3X27GtLoTYq6cqW3H40I9tvQ/3dQHka+nH1Td7n01Mfr+jlKmIzs7yJgiDbz3Z8pz/73IPl2fFqm71pb5+j6RepD7XIZR3R6xdjzTn5iP36PPO4U/SDPoufIP2U09WbZoq48uBXTcNXXJ+A+NcV11lj/jBaxZyFfRXcZE/8sfSpZR17xPwO+ftfr2U/7hL5r8tvRdG7K4fuoNqw8ynFb2qX09KO1G3PCyDfjZnfVpDwzq10JPEpUjuvUlCpUl4fvv7t0fLkOlJvcch3pS3tQpvvvl3enTYgj17SUjr6Piq8kULvws3n2nck59+q8Bt0dB76zRXRKdLfkQ7VakyDTjNar4N12rILuRNnWnNHY1VfVG/6s5lcltOoU+M8exGUvJp2gL9D+L7qqOzeXEbL3fZWFDoS/coTnS5Lq38/6+ot/dHQscuwT3v2tyUrtfYcYEuNfuHsR80dzBOld4XzrHkZ96l0VrafVI/Z/2W/PXz4XitcEq+Ydm/qX64/dfJ7Prg+qvRry2F5e9pndk3OJ5Gxp/HmgEIxKnZXfPNAfpB/icgG8/aEcdq/KmNVGVXedfGWbsdq2ibbdQ0953IW6vKOfQWXd7iQ3KhufePfoLhOybgqtxB7CW0nXXqry076n6anlHSN09n8Juf9XITHIl79kBr5+1/P9CtKP/Jzbf7gMvlXBja8bpSopKttMAjD2RWStD0vkDRrTAxDp6PSONwALsddkG7zIFcesiGu1I4hr+pvnnkwPaS1yThrnGtkzGmqGEZoPZoaZmXwroBH12Gbzr8zgLFQm5YbJarH/J4oHQdepaTZK0lzB8QH131N8Kjuzat8dwbfXx3eAxuOo1AnSqcL9U/+aEvOGrSH67FKz/bsaUtW6mw0xJa8/EK1nBxkVNpAV16ss4Z/lfrKyflzUx0OD77HCkb7Z8XwAdYkZN+ZDCo+Fa96LmOt/15tOTBvb/tsb3eyzaEYhjqJclCE2w3diTR0zGn/rg6C7pTY6ot+s7RN1li5xlcUBuyUft+zr+ByDpmQ+DcorjN8XvH3UFv0bSeb21uzP+AqN//2fATEIlquTf0Qp/yZkOv59vkul/8LD2zEMaknuAn8uhqSg/oErk1f9b+bv4jZhvep6WMt7P1yXE3EeHlgH3iDtz9cBmxx+DySPQ/Zlh4B+IPbcu+23Lt98tv8ZvJtAMUlagRPD+elS/wGLjiHToC9PxaQ5/PieN0rAOASpt/l+8N9Xov8OKj9eax7bwAwXK5vz7AlAPqgG1vu3z71Zp3WV/+mf+W+CbUbeQIAwPOBgQ0A+mD0Jni/HL/XIj8Ialf/a23SCsDVuLY9w5YA6IcubPkK9jl6+yqomGIzm4jJfCV2/LpXOlZz+sy76olY/Him6aAAAODBAAc2dmL+8pJNM2o+5pQagFugdsn3fS3yA7D7ybtvI9gCj8h17Rm2BEBfXG7LV7HP0ULs062IeRBmsxQzft0rHdlLd+KtSE/rhjfCAQDA84E9Nm4MD8BgHdjtQP0DDXRh+ECGoCugS7cF9Q+uCfTtsYA8nxcsRQEAAAAAAAAAAMBgyWZszOdz9REAAAAAAAAAAABgOGApyo3BdKnbgvoHGujC8IEMQVdAl24L6h9cE+jbYwF5Pi9YigIAAAAAAAAAAIDBgoENAAAAAAAAAAAADBYMbAAAAAAAAAAAAGCwYGADAAAAAAAAAAAAgwUDGwAAAAAAAAAAABgsGNgAAAAAAAAAAADAYMHABgB9sZtnr5ya79TnB2c3f6H7nYsnuV3wbNypPd/U7lSdTFZH9QUAA+BCW0Zb14ajWE1QbwCAfvEY2NDOiI66VuC4EhNOw0GOK52tMWl7XijHnVjNJ/I+jOshHGvDkUQyyeWW1eWEGqugygzJo8X1SK/mXTSirfMhu3nfCBEl4vtUfcXfaVtyHhPh7COQHeS20tSR8EpbrdeXyaRqZ562M/2eiEhsxPuDdHKOK3nPrvqTwa3hlwxfZjsq+VC9zqm+8zRZ3T9G3d0tndpzCY+8j2WZk73PV452SNld2e+tSjryaHZ31xg23i5eOZJYyTeX2gGbXEEDN7TlPmyuqb25R4ZY5ofF8E3Vw6LHl/aJAq/n3/ZxunNedX6Wr+/6+Xlpiusv7Ie4aOxz+PbjOpD/er0+1bKNT5RMHfFpq76ukCanKE8XnZJUfW+i8orNTNqeF8T2FKv8ozg5bbfbUxJH8ppRcrJd8lpwGYZFekoiJa8oOsVxfIrp/1r5VQjJI/x6aaJ/56NGZwlO4yIknwpKr6NCAdPTNqHy8z1YDmkHrusY9VDJt4xP2qpNbLcJlYM+J2YJQmxHXffGNtUWvqcCuW+yyET/Zt5r/p3S09JRrFYtb5KP+j3/XCtbUEdFhgbd2/MZn7zPadiWWObGOZXGzWKf5Du0jhST39DuVNv8iDrL91Uh9wlSNnbX6opXSKa5X9Y6QEfentFBJ8H6JVwfLm5ty33YnL7ukGwprMw6LgmU15Xg+xg0Wq8j5VcKR9mvdNAnCrief9t3tqttmp5SZ/uiyl91sjl83vNhiRsqcf0l/RAX2rblUZWX8XttP64b+TcObGxjeWGtiE490kquK8ZmHLYGv+15Ach7qJ6vv79lQ8LXHxK5gypVplbAOkejCckj7Hpn42Ejln/XGyinrRKeTxlZbt+BHqKh/vL8yCHJcrkzbk6r76+5fKG2E3zfdwTfTxnXfVrrRfkylwxN5PnVeuL6u6U/Gjo2GZ71vQ979s9byrYUYGqdKeUtr2dp8xw6djO7U37rEXXWqkut4xVDT+iHyjmp1p3HrMs2WOs/wN5cdGHLzM1sbrDo+g2T17Ww69uAUL7JI/ywxy+EK96xEnA977avkqfSmdJFZDnr9Wjw8gxG29cFPqmhH+Ii94WOPof8vZpvpR/XkfwblqLsxK8N/Rf/EOvvXwVdUGx+1c/7OYgv4kfMfyzFz4ApQm3Pa0bdg2XaoZxOSJf8m8ovQANH8fvjQP9HIilV5mj6XZACsoJUpqAVCckj8HrH3+LjQGnTk9gvxurLFlycjyp39FW8jdRXDeykklbuU7ITP5eUH9nh4pP6yolH2uz+6GrJf2JRW75w2xm9sZ84iI/fofPY7pP8Pj9+k1QVx5Xgmcy2evHjKP794f9fxadS/Y8We9I5T6UBfvRpzwF5S9lORSGL0Zv4ygpGNmOaUvqXrkea99mzuI9md/dOcLyy+ynYLbPP+G9d0gFmNBWL/5SvWf48t2mgyJ3YMgObA8Pk+n2ikLavyEh8eqX//vzL4y9e/jTbsJ2uRavw61Hxjuvd1PdDXDT1OQL7cQXayb9+YGP3S/Btxl/odK2EjR3Xs3Fs3ld5YXxoe14tx38i60PYGH2irgVhVBqoIxUy3rZ12JUCUm3/q63MkDwCrzdaiP1p39qocy7NRzkY8fqJSulB3lG23SeZ4XxGdhiLLQXETfikPf7+oKYkEl9tFzNpYzvq+8JAwJAhXSh3YHY/l1R/7McXfvJ1shEN48SgC/q05658DtmjOYgx/swtYbXTJG2XUpdHPELsztgDgte8yrWsE7Eq6GKLfY00Rv5F9Npe+74FQyMkXpEBY4PP0L4GfsHNPdlyUFtH9rQ6r0GX9rQq2oGym+peTHq/D8uhjcywuWwPlzyNsW9TIR/e38BV6mpZnfs/Octc2r/B13eA/rm7PlHdAL56CKTtlXT4G3WiL+m8Pyrecb2Lhn6Ii+Y+xyX9xnbyrx3YkI1xLHhcgwvwJkc2mjdM0g00dQa+NaU1aXtea8ZCxo9/qepBI9ohOjrsOhivHewNyaOL692C9K+98+GgtqOsnE2UfG8enfZMK58GkzdJOdDx2czJhst2Hs+mih0Y99MOf0ZiIXswYjOTmyNdw9uBlgTacxiq0Sd7NGfvjBb/ZU8yDssx6Qd1gI7c2ZiIcfZkZCv+q7Tq4XbHujdjfxHHIuZzc3gAYpz9JuJEbLdbsU1IXw8bMRu32FTsUfGOV/QMreYZOLJN434FKrkXOrVlf5vbzcmelmRQpj2Jj4aHQAQPHIyXWcchUedJDYkoK/pcaoQ27+Qj2HDZpjOjPpCdsw+hdl7lE9NvPNNkw76lorfK9qmsh4iuofKJDjofnxG3nZiPZ2K5oZqO9PXYd4zlrCXQK3/+7cRupw5qN8IIb0faX8/S9qkH6BxrcV5HNdMte7ie6RXFytT+YVZrlUvj+lYP7Hz6HCH9uK7kv3busWHbnEN9Z1tXqtdL5en1JibGOhjb2tO253lTt+7o9mv++L4GQ0VWRfQ6qlo5heRx0fX8ZNtc/+E64lUPORZ9z1HXNu1N2UJ1DaRvWn0/8uANhihgkhsM6e/zguu0Ybbj2j/i3uF7d5GvSa3ba0jrq23z0G21Mnh9YZZeHdlmTwOrs3ujToYSt9668Lfn9nnbfVxKeqd+V0fd2mdvu1N+wZXWeb82f2zzMeq76i2F188t4XuoUKkDn3ilzscX0XVfJ+dnwVr/BW5vy342p/Kyxc0mFluyriVXOmizuXJZcv9STp+3VcUyuf2R1uHSvVrK7NLhc1n85XVNuGyDRsu0clA8UpCF1m2b3gbYlPf17Dh1jfI1N1nWeVltoQY+/3nQcpNHfVxvw7+NOmPxaxZ/kOuJ4/paD/KfO5C/c8bGcfUusgc2crqGYiq+0N2Lw4doXlY4Fd/proXPDI8Cbc9zoWeaHMRyLEeueGRxlU21wwgyuC3azqwjnnq00mcENSRtRizXEq8XYjGdiul0Idb7tLTeDbajyWdtZE+hGmZr8NOtzaZ4/Ko+/xhN12J/SvOncIfNMnsi7vVQDAwfNa2SbdE6jfP4W/z6w79HIopYQ0hHlt9qppCHYZ/OqfcgiMWX8VEc+amJPsQn8crFwNJNg67jFfCwcNwctB5DzfSJPovC/BK9BMYyVbVs03IfEKL8lFMvLS88mddr4W3+SOt5054i5zx+lJyLnoUGeiJbSnXi3nx+pFuOW+TsnHNc0VFc5309C3VtH+W73us892LNerSbZzMI4y3vq3AsLHPiGY3wvBqfuL5KbT/ERXCfw5MO5O8Y2Dg7p89UFfk0Izro1unw2zBptPhBOVBqCsZC2vy257ngjWvSLXUeIja4mZjNZmL58Ue8JomcgltuOAC4CmrTHdL2chDAv82lJYua7TIUIWnPlDeuJEtR693Oa7xhOwo97ZxodOQkB7Oxzw6nYEZiutADHNK3bmaPsQcBqENNq6T21LoJVj4FPRHbdC/2+704pVsKUGTQmK+t7xw1PThbdjIW48IxEzyuB4o0xytqerdzHXGV16pzBoNllG0MG5PtLmdku7xfhdeST9Uel5cFqL3vvJbT6D0TvNBr4e1t+kgGB9YBlTP1eYDrMqKO7Z7iN8bcC6ivuM51vSINbV+Z40pMKL6NkjSLb3lZ13LzSuem2UAK3YAY42lQjk9cX6SuH+KiXZ+jFS3kbx/Y0BssUUUsSeFZ6fNDRTZ+GyZNxTpTcnLoQa86aXuem+zpaD4KRAcFimvewpVvx7H2B5Ro2FjIayf/kDy6uN49owIUEX+pOPd8BPXPLzGfz8/He7ZajezvZ/aZ4qOgtGcn5x9kw3Ykev17P/AAx17ImACbBz42vI6dN9ziuMC2WaEKGijQ2O4XYqp/5zdn5E9f3vvd6yLiARUKHGwHlQntpUlTvKJ9btN+UOfd4wfbpgE76ikkB+Ix6QF3JscvzfvVTLMp0hsxm3D7vZObema+4YJNAsFzMf5M2lKlt7jOcT1JU9tXhtJ/W4oDv00qSyz3OOOZBYvRKBtIyR44Pf0swvC4PqemH+IiqM9xUT+unfytAxt6d1We0pIrvXHINtzzNWf561woEPsnv/Ki7XkhmG99AR7UbSykNlUkFa5/2BSSRxfXu1fIYOUWxLWvVjocSksaDuwE+Af5+cPwYr5p3Zuu+m9yB9sBoA0c2MnpvvH2ZH/aoTfbsj410wFMX5x9ruDAwXbIhI081QaYDfGKHhStfV2+fqAUuCs9GA4ciMsBjrqBMI2KEXgDTmpts4eMvAEpfeZZXN4PV72pi7fYLclOS/1Mkfo8wJ3Ta1zn0faVyJ7OH+QAv0vdM98KfWsZ1/v1Q1z49Tna9+Payt8ysKGfGrg7jHIUuaGRztFvACAnzk7Zm7bneaKmt/CTqRbyfFLcb8bRI3iFUT+uY14HNTenXYbkEXi9O0FP2awN7NX6NFcQy1MFbYOKvIMOw9Oy+DOvmw1Jy+jyVaYKNpQpp9Z2tBMd6oDTFeD6s73+7qidvH4TFbgHvOzZCzOwk9MqregnHNZAwGVfXdmd2+dmZPttqL8bKE9ZP66+Sf/ykNTHK3q5itjMLG+iIMj25fRs0o2u1yyDnO5smWlvc6PpF6kPtchlHdHrF2PNObXj+/V5FlennKesV2yf9FNOV2+aKeLKg18f/cj2f69Qm8NPvOmv6OtbvV9xxXXWON6F63qebZ9JYV8FN9kTfyx9ahfX+8T8FvmH9Tla9uMukf+6/FYUvbty6A6qDTuf0v2qXU5LO1K3PS8AmYfxpgJjx1Vz89ZbwOUYFlr2dETl+iztWJvv1F3eyTYgj6C0/BYBlYaOTK9IxpH+Ltmed+9VcD5VwvMp0KDTjNbrYJ227TrswpnW3EFZ2UWkdywv2kS47Sh5Ne0Af4fwfdVR2b25jJa77a0odCT6lSc6XZZW/67r31O2wIpdhn3as3/eeVuWy7x0UFpNvmM86xJ9v+UdzhPKX9lfVUcC7K7Rh5x9rrm7eqJ0tHCeNS/Dv6h7leWmesn+L/vt+4TLX+GSeMW0e1NHcp9aJ5Png+ujyn3YssTT5rJrcj6JtGPjTQWFYlRsqfimg/wgnxCRXeXtCeO0aVXGqjKqvOviLd2O1bRNtusaes7lLNTlHds/l3e4kNyobqNC7HGWW1lHg+I6JeOi3Npcj7/XaUuH0fZJXHqr8yL9T9NTSrrG6Wx+k6/3XPjH9RotF6s71Fjl76DJD/GhdSAvqy3fy+RfGdjwulGikq62wSAMZ1dI0va8AFKqbB0QyoMbmYYG8EpweQZHus2D3HN9Whp3Q7bV3zzzYLzTGsZjPaoGxN9XCc+niHIwroBH14tPJ6SM03FYqE3LQRzVY35PlI4Dr1LSYNtR9+ZVvjuD768O74ENx1GoE6XThfonh78lZw3aw/VYpU979s3b0Ukxj1L+adYBMv2e1JFCh0YTYndePqTqHzggqVzblRfrt1F2qducnD831fd9wOWuYLRpVgwfYE1C9p3Va8Wn4lXPZaz1fxe2rPC2ufa2JNsc6qTowU06EuoUVHTMadPqnirKqP2Rrb7oN0vbZPU7NfZ/7rjQ70q/79n+uZxDxtZeZINTFrkFxXWGzzN/979eaNtXZ6OM1E99rqvc/Nvz4RfXZ2i5OutZ4ZC/FacfIrz7cZfL/4UHNuKYXB+4Cfy6GpKD+gSuTV/1v5u/iNmG96npYy3s/XJcTcR4eWAf6Dfd8I6ALQ6fZ7XnIdvdvQJ/cFvu3ZZ7tzl+M9JMvg2g8KpWhqeH89IlfgMXDL4TYO+PBeT5vDhe9woAuITpd/k+b5/XIj8Oan8e7FsDHoz7tmfYHQC+dGPL/duc3qzT+urf9K/cxwCvzwEAgAIY2ACgD0ZvgvfL8Xst8oOgdvVv3KQKgKFxz/YMuwPAny5s+Qo2N3r7KqiYYjObiMl8JXb8ulc6VnP6zLvqiVj8eKbpoAAA4MEABzZ2Yv7ykk0zaj7mlBqAW6B2yfd9LfIDsPvJu2Ej2AKPyP3aM+wOgBAut+Wr2NxoIfbpVsQ8CLNZihm/7pWO7KU78Vakp/XdvREOAABuDfbYuDE8AIN1YLcD9Q800IXhAxmCroAu3RbUP7gm0LfHAvJ8XrAUBQAAAAAAAAAAAANFiP8/QYRYYWe/jUkAAAAASUVORK5CYII=)"]},{"cell_type":"markdown","metadata":{"id":"3Cw4-7hxxeTO"},"source":["# Implementing PCA"]},{"cell_type":"code","metadata":{"id":"DGMC467ExlYD"},"source":["from sklearn.decomposition import TruncatedSVD"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zw7HI2m4xy9H"},"source":["clf = TruncatedSVD(300)\n","#x_train_tfidf = clf.fit_transform(x_train_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbpMbyB97Mok","executionInfo":{"status":"ok","timestamp":1617790222511,"user_tz":-60,"elapsed":20613,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"c3a63582-0c95-4883-fecd-6aaa742f9451"},"source":["x_train_tfidf.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(49000, 300)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfmwQM8ZAIJ_","executionInfo":{"status":"ok","timestamp":1617743325928,"user_tz":-60,"elapsed":148832,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"54f44a41-2b7c-453b-ea7b-9ebcedbb0f92"},"source":["x_train_tfidf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2.20227856e-02, -5.15677812e-05, -2.60054259e-02, ...,\n","         1.73327051e-02, -2.00690388e-03, -6.52476915e-03],\n","       [ 2.11651967e-04, -7.58171475e-05, -1.54072167e-04, ...,\n","        -5.59536754e-04, -8.52818149e-04,  3.39897394e-04],\n","       [ 2.75658280e-02, -7.70696641e-03, -6.20610372e-02, ...,\n","         3.80152931e-04, -3.54484263e-04, -2.23806390e-04],\n","       ...,\n","       [ 7.41418266e-02, -5.79971508e-03, -6.02815992e-02, ...,\n","        -4.82838764e-03,  1.08221591e-02, -6.79233548e-03],\n","       [ 4.07157245e-02,  2.14566152e-03, -2.45256757e-02, ...,\n","        -3.62892844e-02,  8.41696669e-02, -2.88153204e-02],\n","       [ 9.09875237e-03,  7.63013494e-05, -8.38156859e-03, ...,\n","         3.94898494e-03,  2.13498322e-02, -2.93964695e-02]])"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"code","metadata":{"id":"c-mAeMc57XUe"},"source":["x_validation_tfidf = clf.fit_transform(x_validation_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFSOH6j67mGm","executionInfo":{"status":"ok","timestamp":1617823198717,"user_tz":-60,"elapsed":876,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"06d74a84-8c76-47d8-f655-948dd4cbf136"},"source":["x_validation_tfidf.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 300)"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIeiJZgpyZKi","executionInfo":{"status":"ok","timestamp":1617744415483,"user_tz":-60,"elapsed":96542,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"d9898c46-fe0f-4367-d975-6b2c60951d1e"},"source":["%%time\n","import keras\n","model_s = Sequential()\n","custom_adam = keras.optimizers.Adam(lr=0.001)\n","model_s.add(Dense(64, activation='relu', input_dim=300))\n","model_s.add(Dropout(0.5))\n","model_s.add(Dense(1, activation='sigmoid'))\n","model_s.compile(optimizer=custom_adam,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=30, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/30\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.6784 - accuracy: 0.5704 - val_loss: 0.7893 - val_accuracy: 0.5060\n","Epoch 2/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6418 - accuracy: 0.6245 - val_loss: 0.8336 - val_accuracy: 0.5300\n","Epoch 3/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6358 - accuracy: 0.6327 - val_loss: 0.8428 - val_accuracy: 0.5300\n","Epoch 4/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6361 - accuracy: 0.6307 - val_loss: 0.8709 - val_accuracy: 0.5280\n","Epoch 5/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6328 - accuracy: 0.6348 - val_loss: 0.8740 - val_accuracy: 0.5200\n","Epoch 6/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6333 - accuracy: 0.6354 - val_loss: 0.9016 - val_accuracy: 0.5160\n","Epoch 7/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6292 - accuracy: 0.6382 - val_loss: 0.9024 - val_accuracy: 0.5100\n","Epoch 8/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6268 - accuracy: 0.6440 - val_loss: 0.9269 - val_accuracy: 0.5160\n","Epoch 9/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6247 - accuracy: 0.6456 - val_loss: 0.9360 - val_accuracy: 0.5100\n","Epoch 10/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6230 - accuracy: 0.6473 - val_loss: 0.9359 - val_accuracy: 0.5160\n","Epoch 11/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6226 - accuracy: 0.6493 - val_loss: 0.9701 - val_accuracy: 0.5220\n","Epoch 12/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6210 - accuracy: 0.6492 - val_loss: 0.9742 - val_accuracy: 0.5180\n","Epoch 13/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6208 - accuracy: 0.6497 - val_loss: 0.9892 - val_accuracy: 0.5140\n","Epoch 14/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6188 - accuracy: 0.6548 - val_loss: 1.0196 - val_accuracy: 0.5060\n","Epoch 15/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6146 - accuracy: 0.6591 - val_loss: 1.0316 - val_accuracy: 0.5000\n","Epoch 16/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6164 - accuracy: 0.6529 - val_loss: 1.0433 - val_accuracy: 0.5040\n","Epoch 17/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6113 - accuracy: 0.6613 - val_loss: 1.0466 - val_accuracy: 0.5000\n","Epoch 18/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6126 - accuracy: 0.6591 - val_loss: 1.0780 - val_accuracy: 0.4980\n","Epoch 19/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6112 - accuracy: 0.6609 - val_loss: 1.0859 - val_accuracy: 0.4980\n","Epoch 20/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6078 - accuracy: 0.6625 - val_loss: 1.1051 - val_accuracy: 0.5000\n","Epoch 21/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6050 - accuracy: 0.6691 - val_loss: 1.1319 - val_accuracy: 0.5000\n","Epoch 22/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6042 - accuracy: 0.6677 - val_loss: 1.1553 - val_accuracy: 0.5000\n","Epoch 23/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6068 - accuracy: 0.6646 - val_loss: 1.1526 - val_accuracy: 0.4920\n","Epoch 24/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6044 - accuracy: 0.6694 - val_loss: 1.1693 - val_accuracy: 0.4920\n","Epoch 25/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.6003 - accuracy: 0.6671 - val_loss: 1.1762 - val_accuracy: 0.4940\n","Epoch 26/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.5996 - accuracy: 0.6729 - val_loss: 1.2012 - val_accuracy: 0.4880\n","Epoch 27/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.5975 - accuracy: 0.6742 - val_loss: 1.2328 - val_accuracy: 0.4980\n","Epoch 28/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.5988 - accuracy: 0.6752 - val_loss: 1.2548 - val_accuracy: 0.4960\n","Epoch 29/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.5987 - accuracy: 0.6690 - val_loss: 1.2529 - val_accuracy: 0.4960\n","Epoch 30/30\n","1531/1531 [==============================] - 3s 2ms/step - loss: 0.5945 - accuracy: 0.6759 - val_loss: 1.2746 - val_accuracy: 0.4940\n","CPU times: user 1min 59s, sys: 10.3 s, total: 2min 9s\n","Wall time: 1min 36s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXj4AthtEaLy","executionInfo":{"status":"ok","timestamp":1617790639992,"user_tz":-60,"elapsed":393060,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"61a01880-4946-48ad-c9a5-cf730e73985e"},"source":["%%time\n","import keras\n","model_s = Sequential()\n","custom_adam = keras.optimizers.Adam(lr=0.001)\n","model_s.add(Dense(150, activation='relu', input_dim=300))\n","model_s.add(Dropout(0.5))\n","model_s.add(Dense(64, activation='relu'))\n","model_s.add(Dropout(0.3))\n","model_s.add(Dense(1, activation='sigmoid'))\n","model_s.compile(optimizer=custom_adam,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=100, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/100\n","1531/1531 [==============================] - 5s 3ms/step - loss: 0.6717 - accuracy: 0.5732 - val_loss: 0.8207 - val_accuracy: 0.5300\n","Epoch 2/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.6391 - accuracy: 0.6275 - val_loss: 0.8365 - val_accuracy: 0.5300\n","Epoch 3/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.6337 - accuracy: 0.6359 - val_loss: 0.8614 - val_accuracy: 0.5260\n","Epoch 4/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.6287 - accuracy: 0.6412 - val_loss: 0.8560 - val_accuracy: 0.5220\n","Epoch 5/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.6227 - accuracy: 0.6477 - val_loss: 0.9172 - val_accuracy: 0.5120\n","Epoch 6/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.6151 - accuracy: 0.6538 - val_loss: 0.9921 - val_accuracy: 0.4980\n","Epoch 7/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.6095 - accuracy: 0.6635 - val_loss: 0.9465 - val_accuracy: 0.5060\n","Epoch 8/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.6052 - accuracy: 0.6629 - val_loss: 1.0425 - val_accuracy: 0.5220\n","Epoch 9/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5971 - accuracy: 0.6713 - val_loss: 1.0415 - val_accuracy: 0.5260\n","Epoch 10/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5904 - accuracy: 0.6833 - val_loss: 1.0676 - val_accuracy: 0.5200\n","Epoch 11/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5855 - accuracy: 0.6817 - val_loss: 1.0851 - val_accuracy: 0.5240\n","Epoch 12/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5785 - accuracy: 0.6874 - val_loss: 1.1163 - val_accuracy: 0.5380\n","Epoch 13/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5761 - accuracy: 0.6888 - val_loss: 1.1454 - val_accuracy: 0.5300\n","Epoch 14/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5708 - accuracy: 0.6944 - val_loss: 1.1798 - val_accuracy: 0.5180\n","Epoch 15/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5633 - accuracy: 0.7004 - val_loss: 1.2362 - val_accuracy: 0.5060\n","Epoch 16/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5593 - accuracy: 0.7043 - val_loss: 1.2660 - val_accuracy: 0.5000\n","Epoch 17/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5575 - accuracy: 0.7032 - val_loss: 1.3609 - val_accuracy: 0.5200\n","Epoch 18/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5535 - accuracy: 0.7066 - val_loss: 1.3850 - val_accuracy: 0.5120\n","Epoch 19/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5482 - accuracy: 0.7128 - val_loss: 1.3917 - val_accuracy: 0.5060\n","Epoch 20/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5441 - accuracy: 0.7130 - val_loss: 1.4717 - val_accuracy: 0.5080\n","Epoch 21/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5460 - accuracy: 0.7092 - val_loss: 1.5686 - val_accuracy: 0.5140\n","Epoch 22/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5412 - accuracy: 0.7162 - val_loss: 1.4877 - val_accuracy: 0.5080\n","Epoch 23/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5382 - accuracy: 0.7181 - val_loss: 1.5064 - val_accuracy: 0.5220\n","Epoch 24/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5363 - accuracy: 0.7181 - val_loss: 1.5741 - val_accuracy: 0.5160\n","Epoch 25/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5291 - accuracy: 0.7234 - val_loss: 1.6725 - val_accuracy: 0.5220\n","Epoch 26/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5257 - accuracy: 0.7261 - val_loss: 1.6721 - val_accuracy: 0.5060\n","Epoch 27/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5290 - accuracy: 0.7267 - val_loss: 1.6460 - val_accuracy: 0.5100\n","Epoch 28/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5274 - accuracy: 0.7248 - val_loss: 1.7502 - val_accuracy: 0.5020\n","Epoch 29/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5233 - accuracy: 0.7258 - val_loss: 1.6828 - val_accuracy: 0.5060\n","Epoch 30/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5207 - accuracy: 0.7323 - val_loss: 1.7594 - val_accuracy: 0.5140\n","Epoch 31/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5189 - accuracy: 0.7283 - val_loss: 1.7947 - val_accuracy: 0.5020\n","Epoch 32/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5205 - accuracy: 0.7288 - val_loss: 1.8657 - val_accuracy: 0.5000\n","Epoch 33/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5163 - accuracy: 0.7334 - val_loss: 1.8933 - val_accuracy: 0.5040\n","Epoch 34/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5098 - accuracy: 0.7368 - val_loss: 1.8658 - val_accuracy: 0.5000\n","Epoch 35/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5127 - accuracy: 0.7335 - val_loss: 1.9356 - val_accuracy: 0.4940\n","Epoch 36/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5130 - accuracy: 0.7319 - val_loss: 2.0353 - val_accuracy: 0.5060\n","Epoch 37/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5074 - accuracy: 0.7385 - val_loss: 2.0278 - val_accuracy: 0.4980\n","Epoch 38/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.5090 - accuracy: 0.7385 - val_loss: 1.9769 - val_accuracy: 0.5040\n","Epoch 39/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5036 - accuracy: 0.7375 - val_loss: 1.9220 - val_accuracy: 0.4960\n","Epoch 40/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5041 - accuracy: 0.7414 - val_loss: 2.1296 - val_accuracy: 0.5100\n","Epoch 41/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5024 - accuracy: 0.7407 - val_loss: 2.0778 - val_accuracy: 0.5080\n","Epoch 42/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5048 - accuracy: 0.7425 - val_loss: 2.0911 - val_accuracy: 0.5040\n","Epoch 43/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4962 - accuracy: 0.7439 - val_loss: 2.1659 - val_accuracy: 0.5100\n","Epoch 44/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.5005 - accuracy: 0.7421 - val_loss: 2.1959 - val_accuracy: 0.5200\n","Epoch 45/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4972 - accuracy: 0.7472 - val_loss: 2.2553 - val_accuracy: 0.5100\n","Epoch 46/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4969 - accuracy: 0.7443 - val_loss: 2.3096 - val_accuracy: 0.5100\n","Epoch 47/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4939 - accuracy: 0.7472 - val_loss: 2.2640 - val_accuracy: 0.5120\n","Epoch 48/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4938 - accuracy: 0.7487 - val_loss: 2.2692 - val_accuracy: 0.5160\n","Epoch 49/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4922 - accuracy: 0.7473 - val_loss: 2.3538 - val_accuracy: 0.5180\n","Epoch 50/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4861 - accuracy: 0.7535 - val_loss: 2.3220 - val_accuracy: 0.5080\n","Epoch 51/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4908 - accuracy: 0.7526 - val_loss: 2.3110 - val_accuracy: 0.5040\n","Epoch 52/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4915 - accuracy: 0.7506 - val_loss: 2.3108 - val_accuracy: 0.5100\n","Epoch 53/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4940 - accuracy: 0.7466 - val_loss: 2.5046 - val_accuracy: 0.5040\n","Epoch 54/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4878 - accuracy: 0.7535 - val_loss: 2.5371 - val_accuracy: 0.5060\n","Epoch 55/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4839 - accuracy: 0.7513 - val_loss: 2.3723 - val_accuracy: 0.5220\n","Epoch 56/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4832 - accuracy: 0.7523 - val_loss: 2.4685 - val_accuracy: 0.5140\n","Epoch 57/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4809 - accuracy: 0.7576 - val_loss: 2.5344 - val_accuracy: 0.5220\n","Epoch 58/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4799 - accuracy: 0.7571 - val_loss: 2.6178 - val_accuracy: 0.5160\n","Epoch 59/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4832 - accuracy: 0.7541 - val_loss: 2.7464 - val_accuracy: 0.5000\n","Epoch 60/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4800 - accuracy: 0.7543 - val_loss: 2.5538 - val_accuracy: 0.5160\n","Epoch 61/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4788 - accuracy: 0.7536 - val_loss: 2.6112 - val_accuracy: 0.5100\n","Epoch 62/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4732 - accuracy: 0.7600 - val_loss: 2.7764 - val_accuracy: 0.5080\n","Epoch 63/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4829 - accuracy: 0.7544 - val_loss: 2.6667 - val_accuracy: 0.5040\n","Epoch 64/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4746 - accuracy: 0.7589 - val_loss: 2.7186 - val_accuracy: 0.5140\n","Epoch 65/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4717 - accuracy: 0.7628 - val_loss: 2.7273 - val_accuracy: 0.5140\n","Epoch 66/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4766 - accuracy: 0.7616 - val_loss: 2.8335 - val_accuracy: 0.4980\n","Epoch 67/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4726 - accuracy: 0.7623 - val_loss: 2.9547 - val_accuracy: 0.5100\n","Epoch 68/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4764 - accuracy: 0.7571 - val_loss: 2.9198 - val_accuracy: 0.5040\n","Epoch 69/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4750 - accuracy: 0.7575 - val_loss: 2.8823 - val_accuracy: 0.5020\n","Epoch 70/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4727 - accuracy: 0.7604 - val_loss: 2.8605 - val_accuracy: 0.5120\n","Epoch 71/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4706 - accuracy: 0.7646 - val_loss: 3.0160 - val_accuracy: 0.5140\n","Epoch 72/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4716 - accuracy: 0.7604 - val_loss: 2.9195 - val_accuracy: 0.5100\n","Epoch 73/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4743 - accuracy: 0.7588 - val_loss: 3.0794 - val_accuracy: 0.5160\n","Epoch 74/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4755 - accuracy: 0.7577 - val_loss: 2.9360 - val_accuracy: 0.5140\n","Epoch 75/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4691 - accuracy: 0.7597 - val_loss: 3.0360 - val_accuracy: 0.5060\n","Epoch 76/100\n","1531/1531 [==============================] - 4s 2ms/step - loss: 0.4689 - accuracy: 0.7658 - val_loss: 2.9292 - val_accuracy: 0.5040\n","Epoch 77/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4671 - accuracy: 0.7641 - val_loss: 3.0806 - val_accuracy: 0.5000\n","Epoch 78/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4710 - accuracy: 0.7608 - val_loss: 3.1462 - val_accuracy: 0.4960\n","Epoch 79/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4724 - accuracy: 0.7601 - val_loss: 3.1542 - val_accuracy: 0.5200\n","Epoch 80/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4618 - accuracy: 0.7682 - val_loss: 3.0718 - val_accuracy: 0.5140\n","Epoch 81/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4669 - accuracy: 0.7699 - val_loss: 3.1027 - val_accuracy: 0.5160\n","Epoch 82/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4624 - accuracy: 0.7669 - val_loss: 3.1893 - val_accuracy: 0.5260\n","Epoch 83/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4606 - accuracy: 0.7681 - val_loss: 3.2492 - val_accuracy: 0.5140\n","Epoch 84/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4629 - accuracy: 0.7704 - val_loss: 3.2525 - val_accuracy: 0.5080\n","Epoch 85/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4628 - accuracy: 0.7667 - val_loss: 3.2804 - val_accuracy: 0.5160\n","Epoch 86/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4559 - accuracy: 0.7695 - val_loss: 3.3197 - val_accuracy: 0.5180\n","Epoch 87/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4646 - accuracy: 0.7638 - val_loss: 3.3640 - val_accuracy: 0.5160\n","Epoch 88/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4616 - accuracy: 0.7658 - val_loss: 3.3586 - val_accuracy: 0.5060\n","Epoch 89/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4644 - accuracy: 0.7664 - val_loss: 3.6191 - val_accuracy: 0.4980\n","Epoch 90/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4628 - accuracy: 0.7704 - val_loss: 3.5154 - val_accuracy: 0.5020\n","Epoch 91/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4568 - accuracy: 0.7735 - val_loss: 3.2414 - val_accuracy: 0.4940\n","Epoch 92/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4548 - accuracy: 0.7715 - val_loss: 3.3204 - val_accuracy: 0.5140\n","Epoch 93/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4531 - accuracy: 0.7763 - val_loss: 3.3343 - val_accuracy: 0.5060\n","Epoch 94/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4569 - accuracy: 0.7716 - val_loss: 3.5039 - val_accuracy: 0.4960\n","Epoch 95/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4503 - accuracy: 0.7761 - val_loss: 3.4099 - val_accuracy: 0.4980\n","Epoch 96/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4569 - accuracy: 0.7700 - val_loss: 3.4449 - val_accuracy: 0.5000\n","Epoch 97/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4543 - accuracy: 0.7735 - val_loss: 3.3767 - val_accuracy: 0.5120\n","Epoch 98/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4533 - accuracy: 0.7716 - val_loss: 3.4604 - val_accuracy: 0.5140\n","Epoch 99/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4526 - accuracy: 0.7756 - val_loss: 3.5087 - val_accuracy: 0.5120\n","Epoch 100/100\n","1531/1531 [==============================] - 4s 3ms/step - loss: 0.4601 - accuracy: 0.7714 - val_loss: 3.4821 - val_accuracy: 0.5060\n","CPU times: user 8min 44s, sys: 30.4 s, total: 9min 14s\n","Wall time: 6min 32s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5auYbTULHz4f"},"source":["# Trying new dataset to check for results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"ygmRuQx9rDki","executionInfo":{"status":"ok","timestamp":1617821858363,"user_tz":-60,"elapsed":909,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"9581bb93-3766-44ce-88b3-9e85385718c1"},"source":["train_data = train_data[train_data['text'].str.len() <=200]\n","train_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>know west teams play west teams east teams rig...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>underdogs earlier today , since Gronk ' announ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>meme ' funny none \" new york nigga \" ones Blac...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>could use one tools MaddenUltimateTeam cush2push</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>' pay attention , long ' legal ' kick bed took...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1010820</th>\n","      <td>1</td>\n","      <td>' sure Iran N Korea technology create pig/bird...</td>\n","    </tr>\n","    <tr>\n","      <th>1010821</th>\n","      <td>1</td>\n","      <td>whatever , ' vote green ! climate BCHarvey</td>\n","    </tr>\n","    <tr>\n","      <th>1010822</th>\n","      <td>1</td>\n","      <td>Perhaps atheist conspiracy make Christians loo...</td>\n","    </tr>\n","    <tr>\n","      <th>1010823</th>\n","      <td>1</td>\n","      <td>Slavs got country called Kosovo worldnews catsi</td>\n","    </tr>\n","    <tr>\n","      <th>1010824</th>\n","      <td>1</td>\n","      <td>values , capitalism good money imprisoning peo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1007056 rows × 2 columns</p>\n","</div>"],"text/plain":["         target                                               text\n","0             0  know west teams play west teams east teams rig...\n","1             0  underdogs earlier today , since Gronk ' announ...\n","2             0  meme ' funny none \" new york nigga \" ones Blac...\n","3             0   could use one tools MaddenUltimateTeam cush2push\n","4             0  ' pay attention , long ' legal ' kick bed took...\n","...         ...                                                ...\n","1010820       1  ' sure Iran N Korea technology create pig/bird...\n","1010821       1         whatever , ' vote green ! climate BCHarvey\n","1010822       1  Perhaps atheist conspiracy make Christians loo...\n","1010823       1    Slavs got country called Kosovo worldnews catsi\n","1010824       1  values , capitalism good money imprisoning peo...\n","\n","[1007056 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"WFDibCoiN5kC"},"source":["sar_train_data = train_data[train_data['target'] == 1]\n","neu_train_data = train_data[train_data['target'] == 0]\n","sar_train_data = sar_train_data.tail(50000)\n","neu_train_data = neu_train_data.tail(50000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5HvZqfvPV8q"},"source":["frames = [sar_train_data, neu_train_data]\n","new_train_data = pd.concat(frames, sort=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"EQ8ISJRvIuOl","executionInfo":{"status":"ok","timestamp":1617821865578,"user_tz":-60,"elapsed":432,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"3ecfd664-9b0f-4ee0-bd58-962daa01ea09"},"source":["new_train_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>920196</th>\n","      <td>1</td>\n","      <td>nop lun wrost pone hole show , 1 mylittlepony ...</td>\n","    </tr>\n","    <tr>\n","      <th>920197</th>\n","      <td>1</td>\n","      <td>steering wheel wrong side pics celluj34</td>\n","    </tr>\n","    <tr>\n","      <th>920198</th>\n","      <td>1</td>\n","      <td>love letting game run overnight get rent villa...</td>\n","    </tr>\n","    <tr>\n","      <th>920199</th>\n","      <td>1</td>\n","      <td>Sure , upvotes/downvotes agree/disagree button...</td>\n","    </tr>\n","    <tr>\n","      <th>920200</th>\n","      <td>1</td>\n","      <td>cardinals set record biggest shut loss ? nfl c...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        target                                               text\n","920196       1  nop lun wrost pone hole show , 1 mylittlepony ...\n","920197       1            steering wheel wrong side pics celluj34\n","920198       1  love letting game run overnight get rent villa...\n","920199       1  Sure , upvotes/downvotes agree/disagree button...\n","920200       1  cardinals set record biggest shut loss ? nfl c..."]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"aqadkZA6KiwY"},"source":["x = new_train_data.text\n","y = new_train_data.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEct50XVK0FA"},"source":["from sklearn.model_selection import train_test_split\n","SEED = 567\n","x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n","x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLATc7QgK7Qc","executionInfo":{"status":"ok","timestamp":1617821874594,"user_tz":-60,"elapsed":537,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"7f6b4978-21c3-4467-edc5-51ece4e0708a"},"source":["print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n","                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n","                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n","print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n","                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n","                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n","print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n","                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n","                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train set has total 98000 entries with 50.04% negative, 49.96% positive\n","Validation set has total 1000 entries with 48.40% negative, 51.60% positive\n","Test set has total 1000 entries with 48.00% negative, 52.00% positive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6rsPzybeLMAO"},"source":["tvec1.fit(x_train)\n","x_train_tfidf = tvec1.transform(x_train)\n","x_validation_tfidf = tvec1.transform(x_validation).toarray()\n","x_train_tfidf = clf.fit_transform(x_train_tfidf)\n","x_validation_tfidf = clf.fit_transform(x_validation_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bk3VEEb5LdlE","executionInfo":{"status":"ok","timestamp":1617822671386,"user_tz":-60,"elapsed":625148,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"857fbae8-f102-4579-eff5-39aca0b6acc0"},"source":["%%time\n","import keras\n","model_s = Sequential()\n","custom_adam = keras.optimizers.Adam(lr=0.0001)\n","model_s.add(Dense(150, activation='relu'))\n","model_s.add(Dropout(0.5))\n","model_s.add(Dense(64, activation='relu'))\n","model_s.add(Dropout(0.5))\n","model_s.add(Dense(1, activation='sigmoid'))\n","model_s.compile(optimizer=custom_adam,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n","                    epochs=100, validation_data=(x_validation_tfidf, y_validation),\n","                    steps_per_epoch=x_train_tfidf.shape[0]/32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/100\n","3062/3062 [==============================] - 7s 2ms/step - loss: 0.6894 - accuracy: 0.5327 - val_loss: 0.7064 - val_accuracy: 0.5350\n","Epoch 2/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6456 - accuracy: 0.6317 - val_loss: 0.7794 - val_accuracy: 0.5250\n","Epoch 3/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6288 - accuracy: 0.6471 - val_loss: 0.7967 - val_accuracy: 0.5170\n","Epoch 4/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6254 - accuracy: 0.6492 - val_loss: 0.8111 - val_accuracy: 0.5230\n","Epoch 5/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6217 - accuracy: 0.6545 - val_loss: 0.8133 - val_accuracy: 0.5290\n","Epoch 6/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6205 - accuracy: 0.6576 - val_loss: 0.8227 - val_accuracy: 0.5260\n","Epoch 7/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6202 - accuracy: 0.6569 - val_loss: 0.8280 - val_accuracy: 0.5260\n","Epoch 8/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6191 - accuracy: 0.6592 - val_loss: 0.8316 - val_accuracy: 0.5230\n","Epoch 9/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6173 - accuracy: 0.6590 - val_loss: 0.8407 - val_accuracy: 0.5230\n","Epoch 10/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6141 - accuracy: 0.6652 - val_loss: 0.8384 - val_accuracy: 0.5200\n","Epoch 11/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6141 - accuracy: 0.6659 - val_loss: 0.8515 - val_accuracy: 0.5170\n","Epoch 12/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6125 - accuracy: 0.6653 - val_loss: 0.8518 - val_accuracy: 0.5200\n","Epoch 13/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6108 - accuracy: 0.6663 - val_loss: 0.8554 - val_accuracy: 0.5180\n","Epoch 14/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6089 - accuracy: 0.6696 - val_loss: 0.8568 - val_accuracy: 0.5160\n","Epoch 15/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6071 - accuracy: 0.6706 - val_loss: 0.8672 - val_accuracy: 0.5190\n","Epoch 16/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6045 - accuracy: 0.6734 - val_loss: 0.8632 - val_accuracy: 0.5160\n","Epoch 17/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6051 - accuracy: 0.6754 - val_loss: 0.8722 - val_accuracy: 0.5180\n","Epoch 18/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6029 - accuracy: 0.6758 - val_loss: 0.8791 - val_accuracy: 0.5170\n","Epoch 19/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6008 - accuracy: 0.6768 - val_loss: 0.8806 - val_accuracy: 0.5160\n","Epoch 20/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.6023 - accuracy: 0.6755 - val_loss: 0.8894 - val_accuracy: 0.5090\n","Epoch 21/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5973 - accuracy: 0.6834 - val_loss: 0.8948 - val_accuracy: 0.5110\n","Epoch 22/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5979 - accuracy: 0.6816 - val_loss: 0.9016 - val_accuracy: 0.5100\n","Epoch 23/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5951 - accuracy: 0.6829 - val_loss: 0.9119 - val_accuracy: 0.5110\n","Epoch 24/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5956 - accuracy: 0.6823 - val_loss: 0.9120 - val_accuracy: 0.5130\n","Epoch 25/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5938 - accuracy: 0.6852 - val_loss: 0.9120 - val_accuracy: 0.5060\n","Epoch 26/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5934 - accuracy: 0.6848 - val_loss: 0.9219 - val_accuracy: 0.5080\n","Epoch 27/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5934 - accuracy: 0.6851 - val_loss: 0.9252 - val_accuracy: 0.5050\n","Epoch 28/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5888 - accuracy: 0.6892 - val_loss: 0.9295 - val_accuracy: 0.5080\n","Epoch 29/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5898 - accuracy: 0.6875 - val_loss: 0.9328 - val_accuracy: 0.5020\n","Epoch 30/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5875 - accuracy: 0.6887 - val_loss: 0.9495 - val_accuracy: 0.5040\n","Epoch 31/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5878 - accuracy: 0.6889 - val_loss: 0.9523 - val_accuracy: 0.5020\n","Epoch 32/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5854 - accuracy: 0.6903 - val_loss: 0.9531 - val_accuracy: 0.5050\n","Epoch 33/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5832 - accuracy: 0.6925 - val_loss: 0.9457 - val_accuracy: 0.5040\n","Epoch 34/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5850 - accuracy: 0.6934 - val_loss: 0.9563 - val_accuracy: 0.5030\n","Epoch 35/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5792 - accuracy: 0.6967 - val_loss: 0.9656 - val_accuracy: 0.4940\n","Epoch 36/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5829 - accuracy: 0.6936 - val_loss: 0.9753 - val_accuracy: 0.4980\n","Epoch 37/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5806 - accuracy: 0.6954 - val_loss: 0.9762 - val_accuracy: 0.4930\n","Epoch 38/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5810 - accuracy: 0.6951 - val_loss: 0.9811 - val_accuracy: 0.4940\n","Epoch 39/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5783 - accuracy: 0.6965 - val_loss: 0.9924 - val_accuracy: 0.4940\n","Epoch 40/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5784 - accuracy: 0.6980 - val_loss: 1.0010 - val_accuracy: 0.4890\n","Epoch 41/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5777 - accuracy: 0.6965 - val_loss: 1.0095 - val_accuracy: 0.4910\n","Epoch 42/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5750 - accuracy: 0.7006 - val_loss: 1.0060 - val_accuracy: 0.4940\n","Epoch 43/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5752 - accuracy: 0.6977 - val_loss: 1.0151 - val_accuracy: 0.4910\n","Epoch 44/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5759 - accuracy: 0.6996 - val_loss: 1.0237 - val_accuracy: 0.4920\n","Epoch 45/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5721 - accuracy: 0.7031 - val_loss: 1.0151 - val_accuracy: 0.4900\n","Epoch 46/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5692 - accuracy: 0.7043 - val_loss: 1.0214 - val_accuracy: 0.4910\n","Epoch 47/100\n","3062/3062 [==============================] - 7s 2ms/step - loss: 0.5695 - accuracy: 0.7041 - val_loss: 1.0339 - val_accuracy: 0.5000\n","Epoch 48/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5683 - accuracy: 0.7054 - val_loss: 1.0359 - val_accuracy: 0.4960\n","Epoch 49/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5700 - accuracy: 0.7042 - val_loss: 1.0359 - val_accuracy: 0.4960\n","Epoch 50/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5694 - accuracy: 0.7029 - val_loss: 1.0452 - val_accuracy: 0.4860\n","Epoch 51/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5690 - accuracy: 0.7042 - val_loss: 1.0580 - val_accuracy: 0.4910\n","Epoch 52/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5683 - accuracy: 0.7074 - val_loss: 1.0647 - val_accuracy: 0.4930\n","Epoch 53/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5639 - accuracy: 0.7102 - val_loss: 1.0596 - val_accuracy: 0.4880\n","Epoch 54/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5647 - accuracy: 0.7083 - val_loss: 1.0628 - val_accuracy: 0.4950\n","Epoch 55/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5635 - accuracy: 0.7090 - val_loss: 1.0795 - val_accuracy: 0.4950\n","Epoch 56/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5619 - accuracy: 0.7129 - val_loss: 1.0759 - val_accuracy: 0.4970\n","Epoch 57/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5624 - accuracy: 0.7107 - val_loss: 1.0874 - val_accuracy: 0.4940\n","Epoch 58/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5632 - accuracy: 0.7106 - val_loss: 1.0810 - val_accuracy: 0.4950\n","Epoch 59/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5615 - accuracy: 0.7120 - val_loss: 1.0822 - val_accuracy: 0.4890\n","Epoch 60/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5607 - accuracy: 0.7103 - val_loss: 1.0869 - val_accuracy: 0.4970\n","Epoch 61/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5617 - accuracy: 0.7092 - val_loss: 1.1011 - val_accuracy: 0.4960\n","Epoch 62/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5609 - accuracy: 0.7113 - val_loss: 1.1022 - val_accuracy: 0.4880\n","Epoch 63/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5565 - accuracy: 0.7139 - val_loss: 1.0989 - val_accuracy: 0.4920\n","Epoch 64/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5610 - accuracy: 0.7092 - val_loss: 1.1047 - val_accuracy: 0.4940\n","Epoch 65/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5591 - accuracy: 0.7123 - val_loss: 1.1047 - val_accuracy: 0.4950\n","Epoch 66/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5580 - accuracy: 0.7133 - val_loss: 1.1172 - val_accuracy: 0.5000\n","Epoch 67/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5556 - accuracy: 0.7154 - val_loss: 1.1219 - val_accuracy: 0.4940\n","Epoch 68/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5557 - accuracy: 0.7145 - val_loss: 1.1195 - val_accuracy: 0.4900\n","Epoch 69/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5554 - accuracy: 0.7141 - val_loss: 1.1194 - val_accuracy: 0.4980\n","Epoch 70/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5592 - accuracy: 0.7120 - val_loss: 1.1382 - val_accuracy: 0.5000\n","Epoch 71/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5556 - accuracy: 0.7139 - val_loss: 1.1321 - val_accuracy: 0.4950\n","Epoch 72/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5534 - accuracy: 0.7176 - val_loss: 1.1357 - val_accuracy: 0.4940\n","Epoch 73/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5550 - accuracy: 0.7126 - val_loss: 1.1539 - val_accuracy: 0.4910\n","Epoch 74/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5539 - accuracy: 0.7156 - val_loss: 1.1397 - val_accuracy: 0.4940\n","Epoch 75/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5516 - accuracy: 0.7196 - val_loss: 1.1495 - val_accuracy: 0.4960\n","Epoch 76/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5521 - accuracy: 0.7161 - val_loss: 1.1541 - val_accuracy: 0.4930\n","Epoch 77/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5512 - accuracy: 0.7173 - val_loss: 1.1578 - val_accuracy: 0.4930\n","Epoch 78/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5525 - accuracy: 0.7155 - val_loss: 1.1629 - val_accuracy: 0.4910\n","Epoch 79/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5498 - accuracy: 0.7172 - val_loss: 1.1789 - val_accuracy: 0.4920\n","Epoch 80/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5503 - accuracy: 0.7176 - val_loss: 1.1911 - val_accuracy: 0.4950\n","Epoch 81/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5521 - accuracy: 0.7170 - val_loss: 1.1726 - val_accuracy: 0.4920\n","Epoch 82/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5497 - accuracy: 0.7185 - val_loss: 1.1818 - val_accuracy: 0.4920\n","Epoch 83/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5488 - accuracy: 0.7196 - val_loss: 1.1782 - val_accuracy: 0.4950\n","Epoch 84/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5446 - accuracy: 0.7221 - val_loss: 1.1874 - val_accuracy: 0.4930\n","Epoch 85/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5485 - accuracy: 0.7178 - val_loss: 1.1806 - val_accuracy: 0.4900\n","Epoch 86/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5484 - accuracy: 0.7196 - val_loss: 1.1939 - val_accuracy: 0.4950\n","Epoch 87/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5461 - accuracy: 0.7230 - val_loss: 1.2027 - val_accuracy: 0.4920\n","Epoch 88/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5494 - accuracy: 0.7175 - val_loss: 1.2096 - val_accuracy: 0.4980\n","Epoch 89/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5424 - accuracy: 0.7235 - val_loss: 1.1940 - val_accuracy: 0.5030\n","Epoch 90/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5492 - accuracy: 0.7187 - val_loss: 1.2067 - val_accuracy: 0.4990\n","Epoch 91/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5463 - accuracy: 0.7222 - val_loss: 1.2141 - val_accuracy: 0.5020\n","Epoch 92/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5462 - accuracy: 0.7213 - val_loss: 1.2166 - val_accuracy: 0.5020\n","Epoch 93/100\n","3062/3062 [==============================] - 7s 2ms/step - loss: 0.5438 - accuracy: 0.7222 - val_loss: 1.2123 - val_accuracy: 0.4980\n","Epoch 94/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5448 - accuracy: 0.7252 - val_loss: 1.2273 - val_accuracy: 0.4950\n","Epoch 95/100\n","3062/3062 [==============================] - 7s 2ms/step - loss: 0.5446 - accuracy: 0.7220 - val_loss: 1.2131 - val_accuracy: 0.4940\n","Epoch 96/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5423 - accuracy: 0.7251 - val_loss: 1.2359 - val_accuracy: 0.5000\n","Epoch 97/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5441 - accuracy: 0.7209 - val_loss: 1.2351 - val_accuracy: 0.5010\n","Epoch 98/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5396 - accuracy: 0.7254 - val_loss: 1.2367 - val_accuracy: 0.5000\n","Epoch 99/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5428 - accuracy: 0.7237 - val_loss: 1.2620 - val_accuracy: 0.4990\n","Epoch 100/100\n","3062/3062 [==============================] - 6s 2ms/step - loss: 0.5420 - accuracy: 0.7232 - val_loss: 1.2364 - val_accuracy: 0.5020\n","CPU times: user 14min 3s, sys: 58.8 s, total: 15min 1s\n","Wall time: 10min 24s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNrXjrsPuycR","executionInfo":{"status":"ok","timestamp":1617823186771,"user_tz":-60,"elapsed":31240,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"e4f51114-8e74-45ce-9e42-5e0d6a98c050"},"source":["x_test_tfidf = tvec1.transform(x_test).toarray()\n","x_test_tfidf = clf.fit_transform(x_test_tfidf)\n","x_test_tfidf.shape\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 300)"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"sDs3hKEexzE7"},"source":["test_pred = model_s.predict(x_test_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-QiuHUKcyg3I","executionInfo":{"status":"ok","timestamp":1617823582406,"user_tz":-60,"elapsed":614,"user":{"displayName":"Siddarth Shantinath Patil","photoUrl":"","userId":"12151914423541973374"}},"outputId":"7453a6a0-cadd-4b41-e871-fe86ca202c83"},"source":["score = model_s.evaluate(x_test_tfidf, y_test) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["32/32 [==============================] - 0s 1ms/step - loss: 1.1910 - accuracy: 0.5160\n"],"name":"stdout"}]}]}