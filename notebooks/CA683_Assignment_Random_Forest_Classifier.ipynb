{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mechanical-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For preprocessing\n",
    "import unicodedata   ## Removing Accented Characters\n",
    "import contractions #from contractions.py ##Expanding Contractions\n",
    "import re ## Removing Special Character\n",
    "import spacy  ## Lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import string ## Removing Punctuation\n",
    "import nltk ##Stemming\n",
    "from nltk.tokenize import ToktokTokenizer ## Removing Stopwords\n",
    "tokenizer = ToktokTokenizer() ## Removing Stopwords\n",
    "stopword_list = nltk.corpus.stopwords.words('english') ## Removing Stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer ##TfIdf\n",
    "from sklearn.model_selection import train_test_split ##Spliting the data\n",
    "from sklearn.model_selection import cross_val_score ##Cross Validation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designed-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data\n",
    "train_data = pd.read_csv(\"train-balanced.csv\", sep='\\t')\n",
    "keys = pd.read_csv(\"key.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "natural-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1010825 entries, 0 to 1010824\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   label           1010825 non-null  int64 \n",
      " 1   comment         1010772 non-null  object\n",
      " 2   author          1010825 non-null  object\n",
      " 3   subreddit       1010825 non-null  object\n",
      " 4   score           1010825 non-null  int64 \n",
      " 5   ups             1010825 non-null  int64 \n",
      " 6   downs           1010825 non-null  int64 \n",
      " 7   date            1010825 non-null  object\n",
      " 8   created_utc     1010825 non-null  int64 \n",
      " 9   parent_comment  1010825 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 77.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#Adding column names to train and test dataframe\n",
    "train_data.columns = keys.columns\n",
    "train_data.info()  ##Some comments are missing. I dropped them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intensive-berlin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1010772 entries, 0 to 1010824\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   label           1010772 non-null  int64 \n",
      " 1   comment         1010772 non-null  object\n",
      " 2   author          1010772 non-null  object\n",
      " 3   subreddit       1010772 non-null  object\n",
      " 4   score           1010772 non-null  int64 \n",
      " 5   ups             1010772 non-null  int64 \n",
      " 6   downs           1010772 non-null  int64 \n",
      " 7   date            1010772 non-null  object\n",
      " 8   created_utc     1010772 non-null  int64 \n",
      " 9   parent_comment  1010772 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 84.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.dropna(subset=['comment'], inplace=True)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lucky-prospect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477959850</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>1474580737</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>1476824627</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>1483117213</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't pay attention to her, but as long as s...</td>\n",
       "      <td>only7inches</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>1472812508</td>\n",
       "      <td>do you find ariana grande sexy ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment       author  \\\n",
       "0      0  You do know west teams play against west teams...    Shbshb906   \n",
       "1      0  They were underdogs earlier today, but since G...     Creepeth   \n",
       "2      0  This meme isn't funny none of the \"new york ni...    icebrotha   \n",
       "3      0                    I could use one of those tools.    cush2push   \n",
       "4      0  I don't pay attention to her, but as long as s...  only7inches   \n",
       "\n",
       "            subreddit  score  ups  downs     date  created_utc  \\\n",
       "0                 nba     -4   -1     -1  2016-11   1477959850   \n",
       "1                 nfl      3    3      0  2016-09   1474580737   \n",
       "2  BlackPeopleTwitter     -8   -1     -1  2016-10   1476824627   \n",
       "3  MaddenUltimateTeam      6   -1     -1  2016-12   1483117213   \n",
       "4           AskReddit      0    0      0  2016-09   1472812508   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "1                            They're favored to win.  \n",
       "2                         deadass don't kill my buzz  \n",
       "3  Yep can confirm I saw the tool they use for th...  \n",
       "4                   do you find ariana grande sexy ?  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fantastic-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50000 entries, 32 to 42338\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   label           50000 non-null  int64 \n",
      " 1   comment         50000 non-null  object\n",
      " 2   author          50000 non-null  object\n",
      " 3   subreddit       50000 non-null  object\n",
      " 4   score           50000 non-null  int64 \n",
      " 5   ups             50000 non-null  int64 \n",
      " 6   downs           50000 non-null  int64 \n",
      " 7   date            50000 non-null  object\n",
      " 8   created_utc     50000 non-null  int64 \n",
      " 9   parent_comment  50000 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "sar_train_data = train_data[train_data['label'] == 1]\n",
    "neu_train_data = train_data[train_data['label'] == 0]\n",
    "sar_train_data = sar_train_data[:25000]\n",
    "neu_train_data = neu_train_data[:25000]\n",
    "\n",
    "frames = [sar_train_data, neu_train_data]\n",
    "data_train = pd.concat(frames, sort=False)\n",
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "departmental-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Accented Characters\n",
    "\n",
    "# Funtion Definition\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "# Function Call\n",
    "data_train['cleancomment']=data_train['comment'].map(lambda s:remove_accented_chars(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "industrial-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding Contractions\n",
    "\n",
    "# Funtion Definition\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Function Call \n",
    "data_train['cleancomment']=data_train['cleancomment'].map(lambda s:expand_contractions(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expensive-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Special Characters\n",
    "\n",
    "# Funtion Definition\n",
    "def remove_special_characters(text):\n",
    "    # define the pattern to keep\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pat, '', text)\n",
    " \n",
    "# Function Call \n",
    "data_train['cleancomment']=data_train['cleancomment'].map(lambda s:remove_special_characters(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "planned-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation\n",
    "\n",
    "# Funtion Definition\n",
    "def remove_punctuation(text):\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "# Function Call\n",
    "data_train['cleancomment']=data_train['cleancomment'].map(lambda s:remove_punctuation(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "european-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "# Funtion Definition\n",
    "def remove_stopwords(text):\n",
    "    # convert sentence into token of words\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    # check in lowercase \n",
    "    t = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    text = ' '.join(t)    \n",
    "    return text\n",
    "\n",
    "# Function Call\n",
    "data_train['cleancomment']=data_train['cleancomment'].map(lambda s:remove_stopwords(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hired-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extra whitespaces and tabs\n",
    "\n",
    "# Funtion Definition\n",
    "def remove_extra_whitespace_tabs(text):\n",
    "    pattern = r'^\\s*|\\s\\s*'\n",
    "    return re.sub(pattern, ' ', text).strip()\n",
    "\n",
    "# Function Call\n",
    "data_train['cleancomment']=data_train['cleancomment'].map(lambda s:remove_extra_whitespace_tabs(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "industrial-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "\n",
    "# Funtion Definition\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function Call\n",
    "data_train['cleancomment']=data_train['cleancomment'].map(lambda s:to_lowercase(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aging-sugar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>cleancomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>But they'll have all those reviews!</td>\n",
       "      <td>RoguishPoppet</td>\n",
       "      <td>ProductTesting</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477965899</td>\n",
       "      <td>The dumb thing is, they are risking their sell...</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>wow it is totally unreasonable to assume that ...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477968131</td>\n",
       "      <td>Clinton campaign accuses FBI of 'blatant doubl...</td>\n",
       "      <td>wow totally unreasonable assume agency covered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>Ho ho ho... But Melania said that there is no ...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>1476807653</td>\n",
       "      <td>Anyone else think that it was interesting the ...</td>\n",
       "      <td>ho ho ho melania said way could happened know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>I can't wait until @potus starts a twitter war...</td>\n",
       "      <td>kitduncan</td>\n",
       "      <td>politics</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477970553</td>\n",
       "      <td>Here's what happens when Obama gives up his Tw...</td>\n",
       "      <td>wait potus starts twitter war morning joe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>gotta love the teachers who give exams on the ...</td>\n",
       "      <td>DEP61</td>\n",
       "      <td>CFBOffTopic</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477971011</td>\n",
       "      <td>Monday night Drinking thread Brought to You by...</td>\n",
       "      <td>got love teachers give exams day halloween</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                            comment         author  \\\n",
       "32      1                But they'll have all those reviews!  RoguishPoppet   \n",
       "43      1  wow it is totally unreasonable to assume that ...       pb2crazy   \n",
       "44      1  Ho ho ho... But Melania said that there is no ...       pb2crazy   \n",
       "65      1  I can't wait until @potus starts a twitter war...      kitduncan   \n",
       "68      1  gotta love the teachers who give exams on the ...          DEP61   \n",
       "\n",
       "         subreddit  score  ups  downs     date  created_utc  \\\n",
       "32  ProductTesting      0   -1     -1  2016-11   1477965899   \n",
       "43        politics      2   -1     -1  2016-11   1477968131   \n",
       "44        politics      8   -1     -1  2016-10   1476807653   \n",
       "65        politics      3   -1     -1  2016-11   1477970553   \n",
       "68     CFBOffTopic      3   -1     -1  2016-11   1477971011   \n",
       "\n",
       "                                       parent_comment  \\\n",
       "32  The dumb thing is, they are risking their sell...   \n",
       "43  Clinton campaign accuses FBI of 'blatant doubl...   \n",
       "44  Anyone else think that it was interesting the ...   \n",
       "65  Here's what happens when Obama gives up his Tw...   \n",
       "68  Monday night Drinking thread Brought to You by...   \n",
       "\n",
       "                                         cleancomment  \n",
       "32                                            reviews  \n",
       "43  wow totally unreasonable assume agency covered...  \n",
       "44  ho ho ho melania said way could happened know ...  \n",
       "65          wait potus starts twitter war morning joe  \n",
       "68         got love teachers give exams day halloween  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "subsequent-contamination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "attended-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train['cleancomment']\n",
    "y = data_train['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 101\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=.1, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "subtle-student",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 45000 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 2500 entries with 50.16% negative, 49.84% positive\n",
      "Test set has total 2500 entries with 49.76% negative, 50.24% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "european-attachment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45000x33401 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 207827 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating CountVerctorization matrix\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "atmospheric-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv = cv.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "developing-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_cv = cv.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "special-cholesterol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Defining and training Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "breeding-voice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6252"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_validation_cv, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "statutory-healing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814222222222222"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-community",
   "metadata": {},
   "source": [
    "# 2nd model with \"author\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "presidential-michael",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19308</th>\n",
       "      <td>sleaze_bag_alert</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10994</th>\n",
       "      <td>TouchMeHerePls</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>EggCouncil</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>ShyBiDude89</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18217</th>\n",
       "      <td>pokemon_fetish</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>xVoltage360</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17168</th>\n",
       "      <td>mindlessrabble</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>Cynner</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13151</th>\n",
       "      <td>brainiac3397</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10941</th>\n",
       "      <td>TombstoneAintThatBad</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     author  Count\n",
       "19308      sleaze_bag_alert     31\n",
       "10994        TouchMeHerePls     25\n",
       "3220             EggCouncil     21\n",
       "9348            ShyBiDude89     20\n",
       "18217        pokemon_fetish     15\n",
       "20995           xVoltage360     14\n",
       "17168        mindlessrabble     13\n",
       "2392                 Cynner     13\n",
       "13151          brainiac3397     12\n",
       "10941  TombstoneAintThatBad     12"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 sarcastic authors\n",
    "sarcastic_author = data_train[data_train['label'] == 1]\n",
    "sarcastic_author = sarcastic_author[['label', 'author']].copy()\n",
    "sarcastic_author = sarcastic_author.groupby('author').agg(['count']).reset_index()\n",
    "sarcastic_author.columns = [\"author\", \"Count\"]\n",
    "sarcastic_author = sarcastic_author.sort_values(by=['Count'], ascending=False) \n",
    "sarcastic_author.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "selected-helen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11678</th>\n",
       "      <td>TouchMeHerePls</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12132</th>\n",
       "      <td>Vince5970</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22064</th>\n",
       "      <td>xVoltage360</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18191</th>\n",
       "      <td>mindlessrabble</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20369</th>\n",
       "      <td>sleaze_bag_alert</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>PianoRainMelody</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16350</th>\n",
       "      <td>hookyboysb</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>Makdranon</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552</th>\n",
       "      <td>ukulelej</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11624</th>\n",
       "      <td>TombstoneAintThatBad</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     author  Count\n",
       "11678        TouchMeHerePls     16\n",
       "12132             Vince5970     13\n",
       "22064           xVoltage360     13\n",
       "18191        mindlessrabble     12\n",
       "20369      sleaze_bag_alert     12\n",
       "8592        PianoRainMelody      9\n",
       "16350            hookyboysb      9\n",
       "6883              Makdranon      8\n",
       "21552              ukulelej      8\n",
       "11624  TombstoneAintThatBad      8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 neutral authors\n",
    "neutral_author = data_train[data_train['label'] == 0]\n",
    "neutral_author = neutral_author[['label', 'author']].copy()\n",
    "neutral_author = neutral_author.groupby('author').agg(['count']).reset_index()\n",
    "neutral_author.columns = [\"author\", \"Count\"]\n",
    "neutral_author = neutral_author.sort_values(by=['Count'], ascending=False) \n",
    "neutral_author.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "personalized-hometown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>politics</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>news</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>nfl</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>GlobalOffensive</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>pics</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit  Count\n",
       "2033         politics   2043\n",
       "94          AskReddit   1307\n",
       "1068       The_Donald    672\n",
       "2406        worldnews    624\n",
       "1997     pcmasterrace    486\n",
       "1795  leagueoflegends    434\n",
       "1913             news    411\n",
       "1915              nfl    399\n",
       "483   GlobalOffensive    270\n",
       "2014             pics    267"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 subreddits for sarcastic comments\n",
    "sarcastic_subs = data_train[data_train['label'] == 1]\n",
    "sarcastic_subs = sarcastic_subs[['label', 'subreddit']].copy()\n",
    "sarcastic_subs = sarcastic_subs.groupby('subreddit').agg(['count']).reset_index()\n",
    "sarcastic_subs.columns = [\"subreddit\", \"Count\"]\n",
    "sarcastic_subs = sarcastic_subs.sort_values(by=['Count'], ascending=False) \n",
    "sarcastic_subs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "actual-sculpture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>politics</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>nfl</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>funny</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>nba</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>news</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit  Count\n",
       "127         AskReddit   1852\n",
       "2661         politics   1205\n",
       "1408       The_Donald    676\n",
       "2360  leagueoflegends    425\n",
       "2619     pcmasterrace    372\n",
       "2531              nfl    364\n",
       "3147        worldnews    337\n",
       "2112            funny    268\n",
       "2508              nba    264\n",
       "2525             news    262"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 subreddits for normal comments\n",
    "normal_subs = data_train[data_train['label'] == 0]\n",
    "normal_subs = normal_subs[['label', 'subreddit']].copy()\n",
    "normal_subs = normal_subs.groupby('subreddit').agg(['count']).reset_index()\n",
    "normal_subs.columns = [\"subreddit\", \"Count\"]\n",
    "normal_subs = normal_subs.sort_values(by=['Count'], ascending=False) \n",
    "normal_subs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "effective-abraham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>cleancomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>But they'll have all those reviews!</td>\n",
       "      <td>RoguishPoppet</td>\n",
       "      <td>ProductTesting</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477965899</td>\n",
       "      <td>The dumb thing is, they are risking their sell...</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>wow it is totally unreasonable to assume that ...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477968131</td>\n",
       "      <td>Clinton campaign accuses FBI of 'blatant doubl...</td>\n",
       "      <td>wow totally unreasonable assume agency covered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>Ho ho ho... But Melania said that there is no ...</td>\n",
       "      <td>pb2crazy</td>\n",
       "      <td>politics</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>1476807653</td>\n",
       "      <td>Anyone else think that it was interesting the ...</td>\n",
       "      <td>ho ho ho melania said way could happened know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>I can't wait until @potus starts a twitter war...</td>\n",
       "      <td>kitduncan</td>\n",
       "      <td>politics</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477970553</td>\n",
       "      <td>Here's what happens when Obama gives up his Tw...</td>\n",
       "      <td>wait potus starts twitter war morning joe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>gotta love the teachers who give exams on the ...</td>\n",
       "      <td>DEP61</td>\n",
       "      <td>CFBOffTopic</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>1477971011</td>\n",
       "      <td>Monday night Drinking thread Brought to You by...</td>\n",
       "      <td>got love teachers give exams day halloween</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                            comment         author  \\\n",
       "32      1                But they'll have all those reviews!  RoguishPoppet   \n",
       "43      1  wow it is totally unreasonable to assume that ...       pb2crazy   \n",
       "44      1  Ho ho ho... But Melania said that there is no ...       pb2crazy   \n",
       "65      1  I can't wait until @potus starts a twitter war...      kitduncan   \n",
       "68      1  gotta love the teachers who give exams on the ...          DEP61   \n",
       "\n",
       "         subreddit  score  ups  downs     date  created_utc  \\\n",
       "32  ProductTesting      0   -1     -1  2016-11   1477965899   \n",
       "43        politics      2   -1     -1  2016-11   1477968131   \n",
       "44        politics      8   -1     -1  2016-10   1476807653   \n",
       "65        politics      3   -1     -1  2016-11   1477970553   \n",
       "68     CFBOffTopic      3   -1     -1  2016-11   1477971011   \n",
       "\n",
       "                                       parent_comment  \\\n",
       "32  The dumb thing is, they are risking their sell...   \n",
       "43  Clinton campaign accuses FBI of 'blatant doubl...   \n",
       "44  Anyone else think that it was interesting the ...   \n",
       "65  Here's what happens when Obama gives up his Tw...   \n",
       "68  Monday night Drinking thread Brought to You by...   \n",
       "\n",
       "                                         cleancomment  \n",
       "32                                            reviews  \n",
       "43  wow totally unreasonable assume agency covered...  \n",
       "44  ho ho ho melania said way could happened know ...  \n",
       "65          wait potus starts twitter war morning joe  \n",
       "68         got love teachers give exams day halloween  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "toxic-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 45000 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 2500 entries with 50.16% negative, 49.84% positive\n",
      "Test set has total 2500 entries with 49.76% negative, 50.24% positive\n"
     ]
    }
   ],
   "source": [
    "data_train['cc_aut'] = data_train.agg('{0[cleancomment]} {0[author]}'.format, axis=1)\n",
    "X = data_train['cc_aut']\n",
    "y = data_train['label']\n",
    "\n",
    "SEED = 101\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=.1, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "considered-bachelor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45000x67951 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 253759 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "auburn-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv = cv.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "prostate-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_cv = cv.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "interracial-halloween",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Defining and training Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "delayed-argentina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6396"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_validation_cv, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "empirical-canal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998444444444444"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-capture",
   "metadata": {},
   "source": [
    "# 3rd model with subreddit feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "infectious-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 45000 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 2500 entries with 50.16% negative, 49.84% positive\n",
      "Test set has total 2500 entries with 49.76% negative, 50.24% positive\n"
     ]
    }
   ],
   "source": [
    "data_train['cc_subr'] = data_train.agg('{0[cleancomment]} {0[subreddit]}'.format, axis=1)\n",
    "X = data_train['cc_subr']\n",
    "y = data_train['label']\n",
    "\n",
    "SEED = 101\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=.1, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "radio-mustang",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45000x36242 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 252379 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "casual-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv = cv.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "gross-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_cv = cv.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "altered-composite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Defining and training Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "variable-producer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6292"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_validation_cv, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "leading-symphony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9961333333333333"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-lemon",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sorted-promotion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 45000 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 2500 entries with 50.16% negative, 49.84% positive\n",
      "Test set has total 2500 entries with 49.76% negative, 50.24% positive\n"
     ]
    }
   ],
   "source": [
    "data_train['all'] = data_train.agg('{0[cleancomment]} {0[subreddit]} {0[author]}'.format, axis=1)\n",
    "X = data_train['all']\n",
    "y = data_train['label']\n",
    "\n",
    "SEED = 101\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=.1, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "discrete-queen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45000x70784 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 298303 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "higher-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv = cv.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "demanding-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_cv = cv.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "weird-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Defining and training Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "convinced-trinidad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6356"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_validation_cv, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "simplified-discrimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998888888888889"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-miami",
   "metadata": {},
   "source": [
    "# Saving all the models (Ignore this Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modle to file in the current working directory\n",
    "\n",
    "Pkl1_Filename = \"Clean_Comment.pkl\"  \n",
    "\n",
    "with open(Pkl1_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "    \n",
    "    \n",
    "\n",
    "Pkl2_Filename = \"CC_with_author.pkl\"  \n",
    "\n",
    "with open(Pkl2_Filename, 'wb') as file:  \n",
    "    pickle.dump(model1, file)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "Pkl3_Filename = \"CC_with_subreddit.pkl\"  \n",
    "\n",
    "with open(Pkl3_Filename, 'wb') as file:  \n",
    "    pickle.dump(model2, file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Pkl4_Filename = \"CC_with_both.pkl\"  \n",
    "\n",
    "with open(Pkl4_Filename, 'wb') as file:  \n",
    "    pickle.dump(model3, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-alias",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/prmohanty/python-how-to-save-and-load-ml-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-horror",
   "metadata": {},
   "source": [
    "# Trying fresh model with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "declared-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 45000 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 2500 entries with 50.16% negative, 49.84% positive\n",
      "Test set has total 2500 entries with 49.76% negative, 50.24% positive\n"
     ]
    }
   ],
   "source": [
    "X = data_train['all']\n",
    "y = data_train['label']\n",
    "\n",
    "SEED = 101\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=.1, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "clear-assurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec1 = TfidfVectorizer()\n",
    "tvec1.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "satellite-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tvec1.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "freelance-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_tfidf = tvec1.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "stunning-december",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Defining and training Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "seeing-adapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.668"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_validation_tfidf, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "quick-location",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999555555555556"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-steal",
   "metadata": {},
   "source": [
    "# Trying fresh model with tfidf with bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adequate-grocery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec1 = TfidfVectorizer(ngram_range=(1,2))\n",
    "tvec1.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ambient-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tvec1.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "marine-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_tfidf = tvec1.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "sustainable-juice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Defining and training Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "reasonable-animation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6544"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_validation_tfidf, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "judicial-campbell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999555555555556"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-crack",
   "metadata": {},
   "source": [
    "## Note: TfIdf using bigram proved to be less acrurate. Therefore, I will ignore this and not save the model using pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-sierra",
   "metadata": {},
   "source": [
    "Future work can be making use of Dimentionality reduction (PCA) or Word Embedding (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-audience",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
